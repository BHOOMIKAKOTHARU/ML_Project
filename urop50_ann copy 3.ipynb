{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 50 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 15:33:29.368224: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 15:33:32.050681: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 15:33:32.050788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 15:33:32.442359: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 15:33:33.286776: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 15:33:33.287378: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 15:33:37.006594: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081a3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.6.3 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.7.0)\n",
      "Requirement already satisfied: tqdm in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (4.64.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.7.0)\n",
      "Requirement already satisfied: decorator in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: packaging in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (22.0)\n",
      "Requirement already satisfied: jinja2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (3.8.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from jinja2->mne) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files4/S001R06.edf',\n",
       " 'files4/S002R06.edf',\n",
       " 'files4/S003R06.edf',\n",
       " 'files4/S004R06.edf',\n",
       " 'files4/S005R06.edf',\n",
       " 'files4/S006R06.edf',\n",
       " 'files4/S007R06.edf',\n",
       " 'files4/S008R06.edf',\n",
       " 'files4/S009R06.edf',\n",
       " 'files4/S010R06.edf',\n",
       " 'files4/S011R06.edf',\n",
       " 'files4/S012R06.edf',\n",
       " 'files4/S013R06.edf',\n",
       " 'files4/S014R06.edf',\n",
       " 'files4/S015R06.edf',\n",
       " 'files4/S016R06.edf',\n",
       " 'files4/S017R06.edf',\n",
       " 'files4/S018R06.edf',\n",
       " 'files4/S019R06.edf',\n",
       " 'files4/S020R06.edf',\n",
       " 'files4/S021R06.edf',\n",
       " 'files4/S022R06.edf',\n",
       " 'files4/S023R06.edf',\n",
       " 'files4/S024R06.edf',\n",
       " 'files4/S025R06.edf',\n",
       " 'files4/S026R06.edf',\n",
       " 'files4/S027R06.edf',\n",
       " 'files4/S028R06.edf',\n",
       " 'files4/S029R06.edf',\n",
       " 'files4/S030R06.edf',\n",
       " 'files4/S031R06.edf',\n",
       " 'files4/S032R06.edf',\n",
       " 'files4/S033R06.edf',\n",
       " 'files4/S034R06.edf',\n",
       " 'files4/S035R06.edf',\n",
       " 'files4/S036R06.edf',\n",
       " 'files4/S037R06.edf',\n",
       " 'files4/S038R06.edf',\n",
       " 'files4/S039R06.edf',\n",
       " 'files4/S040R06.edf',\n",
       " 'files4/S041R06.edf',\n",
       " 'files4/S042R06.edf',\n",
       " 'files4/S043R06.edf',\n",
       " 'files4/S044R06.edf',\n",
       " 'files4/S045R06.edf',\n",
       " 'files4/S046R06.edf',\n",
       " 'files4/S047R06.edf',\n",
       " 'files4/S048R06.edf',\n",
       " 'files4/S049R06.edf',\n",
       " 'files4/S050R06.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files4/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 74950, 600000, 74950)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.8e-05 -3.7e-05 -1.0e-06 ... -1.3e-05 -2.6e-05 -2.4e-05]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1        0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2        0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3        0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4        0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995  -0.000030  -0.000019  -0.000023  -0.000027  -0.000028  -0.000021   \n",
       "599996  -0.000034  -0.000034  -0.000037  -0.000042  -0.000047  -0.000039   \n",
       "599997  -0.000030  -0.000034  -0.000032  -0.000040  -0.000051  -0.000043   \n",
       "599998  -0.000032  -0.000024  -0.000030  -0.000035  -0.000034  -0.000031   \n",
       "599999  -0.000020  -0.000012  -0.000008  -0.000017  -0.000024  -0.000016   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1        0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2        0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3        0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4        0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995  -0.000026  -0.000016  -0.000025  -0.000004  ...   -0.000010   \n",
       "599996  -0.000044  -0.000031  -0.000033  -0.000015  ...   -0.000014   \n",
       "599997  -0.000046  -0.000025  -0.000029  -0.000012  ...   -0.000011   \n",
       "599998  -0.000032  -0.000013  -0.000027  -0.000005  ...    0.000000   \n",
       "599999  -0.000026  -0.000011  -0.000007   0.000018  ...    0.000004   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1         0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2         0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3         0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4         0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995   -0.000007   -0.000016   -0.000004    0.000000    0.000002   \n",
       "599996    0.000007   -0.000001    0.000003    0.000001   -0.000001   \n",
       "599997    0.000002   -0.000005    0.000005    0.000004    0.000002   \n",
       "599998    0.000002    0.000002    0.000011    0.000018    0.000014   \n",
       "599999    0.000013    0.000016    0.000035    0.000025    0.000017   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000029   -0.000004    0.000059   -0.000092  \n",
       "1         0.000032    0.000004    0.000069   -0.000093  \n",
       "2         0.000025   -0.000003    0.000065   -0.000102  \n",
       "3         0.000013   -0.000016    0.000050   -0.000107  \n",
       "4         0.000015   -0.000013    0.000044   -0.000094  \n",
       "...            ...         ...         ...         ...  \n",
       "599995   -0.000007   -0.000014   -0.000006   -0.000016  \n",
       "599996    0.000008   -0.000001   -0.000003   -0.000007  \n",
       "599997    0.000011    0.000000    0.000004   -0.000006  \n",
       "599998    0.000012    0.000011    0.000018    0.000003  \n",
       "599999    0.000022    0.000020    0.000022    0.000006  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4465/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_4465/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_4465/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1        0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2        0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3        0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4        0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995  -0.000030  -0.000019  -0.000023  -0.000027  -0.000028  -0.000021   \n",
       "599996  -0.000034  -0.000034  -0.000037  -0.000042  -0.000047  -0.000039   \n",
       "599997  -0.000030  -0.000034  -0.000032  -0.000040  -0.000051  -0.000043   \n",
       "599998  -0.000032  -0.000024  -0.000030  -0.000035  -0.000034  -0.000031   \n",
       "599999  -0.000020  -0.000012  -0.000008  -0.000017  -0.000024  -0.000016   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1        0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2        0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3        0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4        0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995  -0.000026  -0.000016  -0.000025  -0.000004  ...   -0.000010   \n",
       "599996  -0.000044  -0.000031  -0.000033  -0.000015  ...   -0.000014   \n",
       "599997  -0.000046  -0.000025  -0.000029  -0.000012  ...   -0.000011   \n",
       "599998  -0.000032  -0.000013  -0.000027  -0.000005  ...    0.000000   \n",
       "599999  -0.000026  -0.000011  -0.000007   0.000018  ...    0.000004   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1         0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2         0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3         0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4         0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995   -0.000007   -0.000016   -0.000004    0.000000    0.000002   \n",
       "599996    0.000007   -0.000001    0.000003    0.000001   -0.000001   \n",
       "599997    0.000002   -0.000005    0.000005    0.000004    0.000002   \n",
       "599998    0.000002    0.000002    0.000011    0.000018    0.000014   \n",
       "599999    0.000013    0.000016    0.000035    0.000025    0.000017   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000029   -0.000004    0.000059   -0.000092  \n",
       "1         0.000032    0.000004    0.000069   -0.000093  \n",
       "2         0.000025   -0.000003    0.000065   -0.000102  \n",
       "3         0.000013   -0.000016    0.000050   -0.000107  \n",
       "4         0.000015   -0.000013    0.000044   -0.000094  \n",
       "...            ...         ...         ...         ...  \n",
       "599995   -0.000007   -0.000014   -0.000006   -0.000016  \n",
       "599996    0.000008   -0.000001   -0.000003   -0.000007  \n",
       "599997    0.000011    0.000000    0.000004   -0.000006  \n",
       "599998    0.000012    0.000011    0.000018    0.000003  \n",
       "599999    0.000022    0.000020    0.000022    0.000006  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.66209\n",
      "[1]\tvalidation_0-mlogloss:3.53682\n",
      "[2]\tvalidation_0-mlogloss:3.45711\n",
      "[3]\tvalidation_0-mlogloss:3.39559\n",
      "[4]\tvalidation_0-mlogloss:3.34792\n",
      "[5]\tvalidation_0-mlogloss:3.30644\n",
      "[6]\tvalidation_0-mlogloss:3.27538\n",
      "[7]\tvalidation_0-mlogloss:3.24662\n",
      "[8]\tvalidation_0-mlogloss:3.22118\n",
      "[9]\tvalidation_0-mlogloss:3.19438\n",
      "[10]\tvalidation_0-mlogloss:3.17717\n",
      "[11]\tvalidation_0-mlogloss:3.15846\n",
      "[12]\tvalidation_0-mlogloss:3.14407\n",
      "[13]\tvalidation_0-mlogloss:3.12944\n",
      "[14]\tvalidation_0-mlogloss:3.11773\n",
      "[15]\tvalidation_0-mlogloss:3.10499\n",
      "[16]\tvalidation_0-mlogloss:3.09529\n",
      "[17]\tvalidation_0-mlogloss:3.08399\n",
      "[18]\tvalidation_0-mlogloss:3.07484\n",
      "[19]\tvalidation_0-mlogloss:3.06546\n",
      "[20]\tvalidation_0-mlogloss:3.05607\n",
      "[21]\tvalidation_0-mlogloss:3.04749\n",
      "[22]\tvalidation_0-mlogloss:3.04007\n",
      "[23]\tvalidation_0-mlogloss:3.03347\n",
      "[24]\tvalidation_0-mlogloss:3.02555\n",
      "[25]\tvalidation_0-mlogloss:3.01801\n",
      "[26]\tvalidation_0-mlogloss:3.00974\n",
      "[27]\tvalidation_0-mlogloss:3.00424\n",
      "[28]\tvalidation_0-mlogloss:2.99745\n",
      "[29]\tvalidation_0-mlogloss:2.98932\n",
      "[30]\tvalidation_0-mlogloss:2.98251\n",
      "[31]\tvalidation_0-mlogloss:2.97673\n",
      "[32]\tvalidation_0-mlogloss:2.97095\n",
      "[33]\tvalidation_0-mlogloss:2.96512\n",
      "[34]\tvalidation_0-mlogloss:2.96072\n",
      "[35]\tvalidation_0-mlogloss:2.95522\n",
      "[36]\tvalidation_0-mlogloss:2.94837\n",
      "[37]\tvalidation_0-mlogloss:2.94288\n",
      "[38]\tvalidation_0-mlogloss:2.93700\n",
      "[39]\tvalidation_0-mlogloss:2.93122\n",
      "[40]\tvalidation_0-mlogloss:2.92670\n",
      "[41]\tvalidation_0-mlogloss:2.92155\n",
      "[42]\tvalidation_0-mlogloss:2.91713\n",
      "[43]\tvalidation_0-mlogloss:2.91218\n",
      "[44]\tvalidation_0-mlogloss:2.90608\n",
      "[45]\tvalidation_0-mlogloss:2.90160\n",
      "[46]\tvalidation_0-mlogloss:2.89528\n",
      "[47]\tvalidation_0-mlogloss:2.89003\n",
      "[48]\tvalidation_0-mlogloss:2.88894\n",
      "[49]\tvalidation_0-mlogloss:2.88349\n",
      "[50]\tvalidation_0-mlogloss:2.87768\n",
      "[51]\tvalidation_0-mlogloss:2.87586\n",
      "[52]\tvalidation_0-mlogloss:2.87077\n",
      "[53]\tvalidation_0-mlogloss:2.86517\n",
      "[54]\tvalidation_0-mlogloss:2.86081\n",
      "[55]\tvalidation_0-mlogloss:2.85662\n",
      "[56]\tvalidation_0-mlogloss:2.85297\n",
      "[57]\tvalidation_0-mlogloss:2.84738\n",
      "[58]\tvalidation_0-mlogloss:2.84302\n",
      "[59]\tvalidation_0-mlogloss:2.83900\n",
      "[60]\tvalidation_0-mlogloss:2.83589\n",
      "[61]\tvalidation_0-mlogloss:2.83276\n",
      "[62]\tvalidation_0-mlogloss:2.82958\n",
      "[63]\tvalidation_0-mlogloss:2.82661\n",
      "[64]\tvalidation_0-mlogloss:2.82494\n",
      "[65]\tvalidation_0-mlogloss:2.82085\n",
      "[66]\tvalidation_0-mlogloss:2.81867\n",
      "[67]\tvalidation_0-mlogloss:2.81673\n",
      "[68]\tvalidation_0-mlogloss:2.81396\n",
      "[69]\tvalidation_0-mlogloss:2.81129\n",
      "[70]\tvalidation_0-mlogloss:2.80898\n",
      "[71]\tvalidation_0-mlogloss:2.80726\n",
      "[72]\tvalidation_0-mlogloss:2.80383\n",
      "[73]\tvalidation_0-mlogloss:2.80223\n",
      "[74]\tvalidation_0-mlogloss:2.79872\n",
      "[75]\tvalidation_0-mlogloss:2.79703\n",
      "[76]\tvalidation_0-mlogloss:2.79304\n",
      "[77]\tvalidation_0-mlogloss:2.79172\n",
      "[78]\tvalidation_0-mlogloss:2.79094\n",
      "[79]\tvalidation_0-mlogloss:2.78910\n",
      "[80]\tvalidation_0-mlogloss:2.78645\n",
      "[81]\tvalidation_0-mlogloss:2.78377\n",
      "[82]\tvalidation_0-mlogloss:2.78211\n",
      "[83]\tvalidation_0-mlogloss:2.78016\n",
      "[84]\tvalidation_0-mlogloss:2.77843\n",
      "[85]\tvalidation_0-mlogloss:2.77473\n",
      "[86]\tvalidation_0-mlogloss:2.77232\n",
      "[87]\tvalidation_0-mlogloss:2.76987\n",
      "[88]\tvalidation_0-mlogloss:2.76779\n",
      "[89]\tvalidation_0-mlogloss:2.76537\n",
      "[90]\tvalidation_0-mlogloss:2.76369\n",
      "[91]\tvalidation_0-mlogloss:2.76221\n",
      "[92]\tvalidation_0-mlogloss:2.75959\n",
      "[93]\tvalidation_0-mlogloss:2.75769\n",
      "[94]\tvalidation_0-mlogloss:2.75663\n",
      "[95]\tvalidation_0-mlogloss:2.75434\n",
      "[96]\tvalidation_0-mlogloss:2.75095\n",
      "[97]\tvalidation_0-mlogloss:2.74880\n",
      "[98]\tvalidation_0-mlogloss:2.74821\n",
      "[99]\tvalidation_0-mlogloss:2.74765\n",
      "[100]\tvalidation_0-mlogloss:2.74602\n",
      "[101]\tvalidation_0-mlogloss:2.74380\n",
      "[102]\tvalidation_0-mlogloss:2.74249\n",
      "[103]\tvalidation_0-mlogloss:2.74064\n",
      "[104]\tvalidation_0-mlogloss:2.73830\n",
      "[105]\tvalidation_0-mlogloss:2.73732\n",
      "[106]\tvalidation_0-mlogloss:2.73480\n",
      "[107]\tvalidation_0-mlogloss:2.73315\n",
      "[108]\tvalidation_0-mlogloss:2.73093\n",
      "[109]\tvalidation_0-mlogloss:2.73010\n",
      "[110]\tvalidation_0-mlogloss:2.72801\n",
      "[111]\tvalidation_0-mlogloss:2.72684\n",
      "[112]\tvalidation_0-mlogloss:2.72465\n",
      "[113]\tvalidation_0-mlogloss:2.72334\n",
      "[114]\tvalidation_0-mlogloss:2.72210\n",
      "[115]\tvalidation_0-mlogloss:2.72067\n",
      "[116]\tvalidation_0-mlogloss:2.71881\n",
      "[117]\tvalidation_0-mlogloss:2.71653\n",
      "[118]\tvalidation_0-mlogloss:2.71457\n",
      "[119]\tvalidation_0-mlogloss:2.71325\n",
      "[120]\tvalidation_0-mlogloss:2.71278\n",
      "[121]\tvalidation_0-mlogloss:2.71143\n",
      "[122]\tvalidation_0-mlogloss:2.70930\n",
      "[123]\tvalidation_0-mlogloss:2.70714\n",
      "[124]\tvalidation_0-mlogloss:2.70615\n",
      "[125]\tvalidation_0-mlogloss:2.70515\n",
      "[126]\tvalidation_0-mlogloss:2.70389\n",
      "[127]\tvalidation_0-mlogloss:2.70351\n",
      "[128]\tvalidation_0-mlogloss:2.70166\n",
      "[129]\tvalidation_0-mlogloss:2.69994\n",
      "[130]\tvalidation_0-mlogloss:2.69885\n",
      "[131]\tvalidation_0-mlogloss:2.69809\n",
      "[132]\tvalidation_0-mlogloss:2.69562\n",
      "[133]\tvalidation_0-mlogloss:2.69470\n",
      "[134]\tvalidation_0-mlogloss:2.69250\n",
      "[135]\tvalidation_0-mlogloss:2.69224\n",
      "[136]\tvalidation_0-mlogloss:2.69176\n",
      "[137]\tvalidation_0-mlogloss:2.69038\n",
      "[138]\tvalidation_0-mlogloss:2.68900\n",
      "[139]\tvalidation_0-mlogloss:2.68845\n",
      "[140]\tvalidation_0-mlogloss:2.68756\n",
      "[141]\tvalidation_0-mlogloss:2.68637\n",
      "[142]\tvalidation_0-mlogloss:2.68574\n",
      "[143]\tvalidation_0-mlogloss:2.68511\n",
      "[144]\tvalidation_0-mlogloss:2.68454\n",
      "[145]\tvalidation_0-mlogloss:2.68348\n",
      "[146]\tvalidation_0-mlogloss:2.68223\n",
      "[147]\tvalidation_0-mlogloss:2.68241\n",
      "[148]\tvalidation_0-mlogloss:2.68244\n",
      "[149]\tvalidation_0-mlogloss:2.68123\n",
      "[150]\tvalidation_0-mlogloss:2.67937\n",
      "[151]\tvalidation_0-mlogloss:2.67930\n",
      "[152]\tvalidation_0-mlogloss:2.67825\n",
      "[153]\tvalidation_0-mlogloss:2.67812\n",
      "[154]\tvalidation_0-mlogloss:2.67837\n",
      "[155]\tvalidation_0-mlogloss:2.67751\n",
      "[156]\tvalidation_0-mlogloss:2.67734\n",
      "[157]\tvalidation_0-mlogloss:2.67688\n",
      "[158]\tvalidation_0-mlogloss:2.67696\n",
      "[159]\tvalidation_0-mlogloss:2.67550\n",
      "[160]\tvalidation_0-mlogloss:2.67464\n",
      "[161]\tvalidation_0-mlogloss:2.67365\n",
      "[162]\tvalidation_0-mlogloss:2.67251\n",
      "[163]\tvalidation_0-mlogloss:2.67099\n",
      "[164]\tvalidation_0-mlogloss:2.67047\n",
      "[165]\tvalidation_0-mlogloss:2.66970\n",
      "[166]\tvalidation_0-mlogloss:2.66929\n",
      "[167]\tvalidation_0-mlogloss:2.66697\n",
      "[168]\tvalidation_0-mlogloss:2.66657\n",
      "[169]\tvalidation_0-mlogloss:2.66533\n",
      "[170]\tvalidation_0-mlogloss:2.66441\n",
      "[171]\tvalidation_0-mlogloss:2.66408\n",
      "[172]\tvalidation_0-mlogloss:2.66360\n",
      "[173]\tvalidation_0-mlogloss:2.66327\n",
      "[174]\tvalidation_0-mlogloss:2.66323\n",
      "[175]\tvalidation_0-mlogloss:2.66257\n",
      "[176]\tvalidation_0-mlogloss:2.66233\n",
      "[177]\tvalidation_0-mlogloss:2.66178\n",
      "[178]\tvalidation_0-mlogloss:2.66163\n",
      "[179]\tvalidation_0-mlogloss:2.66183\n",
      "[180]\tvalidation_0-mlogloss:2.66159\n",
      "[181]\tvalidation_0-mlogloss:2.66178\n",
      "[182]\tvalidation_0-mlogloss:2.66146\n",
      "[183]\tvalidation_0-mlogloss:2.66187\n",
      "[184]\tvalidation_0-mlogloss:2.66147\n",
      "[185]\tvalidation_0-mlogloss:2.66279\n",
      "[186]\tvalidation_0-mlogloss:2.66345\n",
      "[187]\tvalidation_0-mlogloss:2.66354\n",
      "[188]\tvalidation_0-mlogloss:2.66404\n",
      "[189]\tvalidation_0-mlogloss:2.66491\n",
      "[190]\tvalidation_0-mlogloss:2.66407\n",
      "[191]\tvalidation_0-mlogloss:2.66387\n",
      "[192]\tvalidation_0-mlogloss:2.66323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.10      0.12      1499\n",
      "           1       0.09      0.09      0.09      1499\n",
      "           2       0.27      0.21      0.23      1499\n",
      "           3       0.56      0.50      0.53      1499\n",
      "           4       0.77      0.70      0.74      1499\n",
      "           5       0.46      0.45      0.45      1499\n",
      "           6       0.33      0.48      0.39      1499\n",
      "           7       0.43      0.44      0.44      1499\n",
      "           8       0.54      0.64      0.58      1499\n",
      "           9       0.58      0.32      0.41      1499\n",
      "          10       0.32      0.25      0.28      1499\n",
      "          11       0.32      0.25      0.28      1499\n",
      "          12       0.19      0.11      0.14      1499\n",
      "          13       0.10      0.07      0.09      1499\n",
      "          14       0.13      0.07      0.09      1499\n",
      "          15       0.40      0.37      0.38      1499\n",
      "          16       0.30      0.15      0.20      1499\n",
      "          17       0.32      0.71      0.44      1499\n",
      "          18       0.16      0.08      0.10      1499\n",
      "          19       0.19      0.21      0.20      1499\n",
      "          20       0.74      0.63      0.68      1499\n",
      "          21       0.40      0.32      0.36      1499\n",
      "          22       0.27      0.17      0.21      1499\n",
      "          23       0.69      0.73      0.71      1499\n",
      "          24       0.31      0.40      0.35      1499\n",
      "          25       0.14      0.12      0.13      1499\n",
      "          26       0.16      0.19      0.18      1499\n",
      "          27       0.30      0.32      0.31      1499\n",
      "          28       0.39      0.48      0.43      1499\n",
      "          29       0.28      0.41      0.33      1499\n",
      "          30       0.21      0.19      0.20      1499\n",
      "          31       0.21      0.24      0.23      1499\n",
      "          32       0.10      0.05      0.07      1499\n",
      "          33       0.13      0.14      0.13      1499\n",
      "          34       0.52      0.63      0.57      1499\n",
      "          35       0.39      0.66      0.49      1499\n",
      "          36       0.20      0.35      0.25      1499\n",
      "          37       0.29      0.19      0.23      1499\n",
      "          38       0.48      0.27      0.34      1499\n",
      "          39       0.37      0.33      0.35      1499\n",
      "          40       0.24      0.41      0.31      1499\n",
      "          41       0.30      0.49      0.37      1499\n",
      "          42       0.22      0.16      0.19      1499\n",
      "          43       0.35      0.29      0.31      1499\n",
      "          44       0.48      0.58      0.52      1499\n",
      "          45       0.08      0.05      0.06      1499\n",
      "          46       0.15      0.24      0.18      1499\n",
      "          47       0.38      0.28      0.32      1499\n",
      "          48       0.54      0.72      0.62      1499\n",
      "          49       0.11      0.07      0.09      1499\n",
      "\n",
      "    accuracy                           0.33     74950\n",
      "   macro avg       0.32      0.33      0.31     74950\n",
      "weighted avg       0.32      0.33      0.31     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b271d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 16s 838us/step - loss: 2.4486 - val_loss: 2.8820\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 15s 825us/step - loss: 1.9359 - val_loss: 2.7118\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 36s 2ms/step - loss: 1.6807 - val_loss: 2.7449\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 19s 1ms/step - loss: 1.6167 - val_loss: 2.6841\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 17s 912us/step - loss: 1.5794 - val_loss: 2.7076\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 16s 828us/step - loss: 1.5546 - val_loss: 2.7850\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 16s 869us/step - loss: 1.5358 - val_loss: 2.7789\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 17s 929us/step - loss: 1.5199 - val_loss: 2.7111\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 705s 38ms/step - loss: 1.5082 - val_loss: 2.9171\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 533s 28ms/step - loss: 1.4984 - val_loss: 2.7248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28fdc2dd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 1s 335us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.14      0.16      1499\n",
      "           1       0.12      0.08      0.10      1499\n",
      "           2       0.32      0.26      0.29      1499\n",
      "           3       0.74      0.67      0.70      1499\n",
      "           4       0.77      0.72      0.75      1499\n",
      "           5       0.47      0.25      0.32      1499\n",
      "           6       0.48      0.66      0.55      1499\n",
      "           7       0.54      0.60      0.57      1499\n",
      "           8       0.64      0.63      0.63      1499\n",
      "           9       0.48      0.42      0.45      1499\n",
      "          10       0.36      0.20      0.26      1499\n",
      "          11       0.22      0.45      0.30      1499\n",
      "          12       0.28      0.12      0.17      1499\n",
      "          13       0.12      0.09      0.10      1499\n",
      "          14       0.17      0.08      0.11      1499\n",
      "          15       0.40      0.40      0.40      1499\n",
      "          16       0.31      0.13      0.18      1499\n",
      "          17       0.50      0.87      0.64      1499\n",
      "          18       0.18      0.16      0.17      1499\n",
      "          19       0.18      0.11      0.14      1499\n",
      "          20       0.82      0.45      0.58      1499\n",
      "          21       0.34      0.52      0.41      1499\n",
      "          22       0.30      0.17      0.22      1499\n",
      "          23       0.61      0.79      0.69      1499\n",
      "          24       0.39      0.38      0.39      1499\n",
      "          25       0.33      0.16      0.22      1499\n",
      "          26       0.20      0.21      0.21      1499\n",
      "          27       0.33      0.32      0.33      1499\n",
      "          28       0.45      0.67      0.54      1499\n",
      "          29       0.40      0.57      0.47      1499\n",
      "          30       0.29      0.26      0.27      1499\n",
      "          31       0.30      0.22      0.25      1499\n",
      "          32       0.07      0.06      0.07      1499\n",
      "          33       0.20      0.15      0.17      1499\n",
      "          34       0.86      0.77      0.82      1499\n",
      "          35       0.58      0.74      0.65      1499\n",
      "          36       0.22      0.44      0.30      1499\n",
      "          37       0.23      0.33      0.27      1499\n",
      "          38       0.45      0.30      0.36      1499\n",
      "          39       0.36      0.42      0.39      1499\n",
      "          40       0.33      0.34      0.33      1499\n",
      "          41       0.30      0.25      0.27      1499\n",
      "          42       0.19      0.19      0.19      1499\n",
      "          43       0.42      0.41      0.42      1499\n",
      "          44       0.53      0.68      0.60      1499\n",
      "          45       0.10      0.07      0.08      1499\n",
      "          46       0.26      0.41      0.32      1499\n",
      "          47       0.28      0.21      0.24      1499\n",
      "          48       0.56      0.88      0.68      1499\n",
      "          49       0.11      0.14      0.12      1499\n",
      "\n",
      "    accuracy                           0.37     74950\n",
      "   macro avg       0.37      0.37      0.36     74950\n",
      "weighted avg       0.37      0.37      0.36     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4465/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_4465/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_4465/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1        0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2        0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3        0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4        0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995  -0.000030  -0.000019  -0.000023  -0.000027  -0.000028  -0.000021   \n",
       "599996  -0.000034  -0.000034  -0.000037  -0.000042  -0.000047  -0.000039   \n",
       "599997  -0.000030  -0.000034  -0.000032  -0.000040  -0.000051  -0.000043   \n",
       "599998  -0.000032  -0.000024  -0.000030  -0.000035  -0.000034  -0.000031   \n",
       "599999  -0.000020  -0.000012  -0.000008  -0.000017  -0.000024  -0.000016   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1        0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2        0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3        0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4        0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995  -0.000026  -0.000016  -0.000025  -0.000004  ...   -0.000010   \n",
       "599996  -0.000044  -0.000031  -0.000033  -0.000015  ...   -0.000014   \n",
       "599997  -0.000046  -0.000025  -0.000029  -0.000012  ...   -0.000011   \n",
       "599998  -0.000032  -0.000013  -0.000027  -0.000005  ...    0.000000   \n",
       "599999  -0.000026  -0.000011  -0.000007   0.000018  ...    0.000004   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1         0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2         0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3         0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4         0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995   -0.000007   -0.000016   -0.000004    0.000000    0.000002   \n",
       "599996    0.000007   -0.000001    0.000003    0.000001   -0.000001   \n",
       "599997    0.000002   -0.000005    0.000005    0.000004    0.000002   \n",
       "599998    0.000002    0.000002    0.000011    0.000018    0.000014   \n",
       "599999    0.000013    0.000016    0.000035    0.000025    0.000017   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000029   -0.000004    0.000059   -0.000092  \n",
       "1         0.000032    0.000004    0.000069   -0.000093  \n",
       "2         0.000025   -0.000003    0.000065   -0.000102  \n",
       "3         0.000013   -0.000016    0.000050   -0.000107  \n",
       "4         0.000015   -0.000013    0.000044   -0.000094  \n",
       "...            ...         ...         ...         ...  \n",
       "599995   -0.000007   -0.000014   -0.000006   -0.000016  \n",
       "599996    0.000008   -0.000001   -0.000003   -0.000007  \n",
       "599997    0.000011    0.000000    0.000004   -0.000006  \n",
       "599998    0.000012    0.000011    0.000018    0.000003  \n",
       "599999    0.000022    0.000020    0.000022    0.000006  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.57532\n",
      "[1]\tvalidation_0-mlogloss:3.42308\n",
      "[2]\tvalidation_0-mlogloss:3.32399\n",
      "[3]\tvalidation_0-mlogloss:3.23828\n",
      "[4]\tvalidation_0-mlogloss:3.16996\n",
      "[5]\tvalidation_0-mlogloss:3.11928\n",
      "[6]\tvalidation_0-mlogloss:3.07446\n",
      "[7]\tvalidation_0-mlogloss:3.03235\n",
      "[8]\tvalidation_0-mlogloss:2.99824\n",
      "[9]\tvalidation_0-mlogloss:2.96821\n",
      "[10]\tvalidation_0-mlogloss:2.93868\n",
      "[11]\tvalidation_0-mlogloss:2.91499\n",
      "[12]\tvalidation_0-mlogloss:2.89261\n",
      "[13]\tvalidation_0-mlogloss:2.87120\n",
      "[14]\tvalidation_0-mlogloss:2.85247\n",
      "[15]\tvalidation_0-mlogloss:2.83567\n",
      "[16]\tvalidation_0-mlogloss:2.82070\n",
      "[17]\tvalidation_0-mlogloss:2.80697\n",
      "[18]\tvalidation_0-mlogloss:2.78955\n",
      "[19]\tvalidation_0-mlogloss:2.77647\n",
      "[20]\tvalidation_0-mlogloss:2.76333\n",
      "[21]\tvalidation_0-mlogloss:2.74869\n",
      "[22]\tvalidation_0-mlogloss:2.73260\n",
      "[23]\tvalidation_0-mlogloss:2.72029\n",
      "[24]\tvalidation_0-mlogloss:2.70962\n",
      "[25]\tvalidation_0-mlogloss:2.69838\n",
      "[26]\tvalidation_0-mlogloss:2.68812\n",
      "[27]\tvalidation_0-mlogloss:2.67651\n",
      "[28]\tvalidation_0-mlogloss:2.66703\n",
      "[29]\tvalidation_0-mlogloss:2.65537\n",
      "[30]\tvalidation_0-mlogloss:2.64420\n",
      "[31]\tvalidation_0-mlogloss:2.63472\n",
      "[32]\tvalidation_0-mlogloss:2.62406\n",
      "[33]\tvalidation_0-mlogloss:2.61201\n",
      "[34]\tvalidation_0-mlogloss:2.60180\n",
      "[35]\tvalidation_0-mlogloss:2.59261\n",
      "[36]\tvalidation_0-mlogloss:2.58190\n",
      "[37]\tvalidation_0-mlogloss:2.57286\n",
      "[38]\tvalidation_0-mlogloss:2.56342\n",
      "[39]\tvalidation_0-mlogloss:2.55312\n",
      "[40]\tvalidation_0-mlogloss:2.54350\n",
      "[41]\tvalidation_0-mlogloss:2.53714\n",
      "[42]\tvalidation_0-mlogloss:2.53000\n",
      "[43]\tvalidation_0-mlogloss:2.52169\n",
      "[44]\tvalidation_0-mlogloss:2.51353\n",
      "[45]\tvalidation_0-mlogloss:2.50622\n",
      "[46]\tvalidation_0-mlogloss:2.49891\n",
      "[47]\tvalidation_0-mlogloss:2.49044\n",
      "[48]\tvalidation_0-mlogloss:2.48315\n",
      "[49]\tvalidation_0-mlogloss:2.47590\n",
      "[50]\tvalidation_0-mlogloss:2.46901\n",
      "[51]\tvalidation_0-mlogloss:2.46354\n",
      "[52]\tvalidation_0-mlogloss:2.45878\n",
      "[53]\tvalidation_0-mlogloss:2.45392\n",
      "[54]\tvalidation_0-mlogloss:2.44754\n",
      "[55]\tvalidation_0-mlogloss:2.44464\n",
      "[56]\tvalidation_0-mlogloss:2.43990\n",
      "[57]\tvalidation_0-mlogloss:2.43317\n",
      "[58]\tvalidation_0-mlogloss:2.42698\n",
      "[59]\tvalidation_0-mlogloss:2.42082\n",
      "[60]\tvalidation_0-mlogloss:2.41485\n",
      "[61]\tvalidation_0-mlogloss:2.40875\n",
      "[62]\tvalidation_0-mlogloss:2.40037\n",
      "[63]\tvalidation_0-mlogloss:2.39566\n",
      "[64]\tvalidation_0-mlogloss:2.39137\n",
      "[65]\tvalidation_0-mlogloss:2.38521\n",
      "[66]\tvalidation_0-mlogloss:2.38170\n",
      "[67]\tvalidation_0-mlogloss:2.37600\n",
      "[68]\tvalidation_0-mlogloss:2.37147\n",
      "[69]\tvalidation_0-mlogloss:2.36698\n",
      "[70]\tvalidation_0-mlogloss:2.36181\n",
      "[71]\tvalidation_0-mlogloss:2.35898\n",
      "[72]\tvalidation_0-mlogloss:2.35402\n",
      "[73]\tvalidation_0-mlogloss:2.35074\n",
      "[74]\tvalidation_0-mlogloss:2.34467\n",
      "[75]\tvalidation_0-mlogloss:2.34049\n",
      "[76]\tvalidation_0-mlogloss:2.33676\n",
      "[77]\tvalidation_0-mlogloss:2.33469\n",
      "[78]\tvalidation_0-mlogloss:2.33093\n",
      "[79]\tvalidation_0-mlogloss:2.32707\n",
      "[80]\tvalidation_0-mlogloss:2.32423\n",
      "[81]\tvalidation_0-mlogloss:2.32094\n",
      "[82]\tvalidation_0-mlogloss:2.31751\n",
      "[83]\tvalidation_0-mlogloss:2.31129\n",
      "[84]\tvalidation_0-mlogloss:2.30733\n",
      "[85]\tvalidation_0-mlogloss:2.30356\n",
      "[86]\tvalidation_0-mlogloss:2.30151\n",
      "[87]\tvalidation_0-mlogloss:2.29728\n",
      "[88]\tvalidation_0-mlogloss:2.29439\n",
      "[89]\tvalidation_0-mlogloss:2.29077\n",
      "[90]\tvalidation_0-mlogloss:2.28901\n",
      "[91]\tvalidation_0-mlogloss:2.28728\n",
      "[92]\tvalidation_0-mlogloss:2.28387\n",
      "[93]\tvalidation_0-mlogloss:2.28104\n",
      "[94]\tvalidation_0-mlogloss:2.27679\n",
      "[95]\tvalidation_0-mlogloss:2.27390\n",
      "[96]\tvalidation_0-mlogloss:2.27140\n",
      "[97]\tvalidation_0-mlogloss:2.26964\n",
      "[98]\tvalidation_0-mlogloss:2.26798\n",
      "[99]\tvalidation_0-mlogloss:2.26578\n",
      "[100]\tvalidation_0-mlogloss:2.26340\n",
      "[101]\tvalidation_0-mlogloss:2.26017\n",
      "[102]\tvalidation_0-mlogloss:2.25543\n",
      "[103]\tvalidation_0-mlogloss:2.25344\n",
      "[104]\tvalidation_0-mlogloss:2.25000\n",
      "[105]\tvalidation_0-mlogloss:2.24733\n",
      "[106]\tvalidation_0-mlogloss:2.24779\n",
      "[107]\tvalidation_0-mlogloss:2.24486\n",
      "[108]\tvalidation_0-mlogloss:2.24285\n",
      "[109]\tvalidation_0-mlogloss:2.24188\n",
      "[110]\tvalidation_0-mlogloss:2.23975\n",
      "[111]\tvalidation_0-mlogloss:2.23821\n",
      "[112]\tvalidation_0-mlogloss:2.23723\n",
      "[113]\tvalidation_0-mlogloss:2.23580\n",
      "[114]\tvalidation_0-mlogloss:2.23433\n",
      "[115]\tvalidation_0-mlogloss:2.23096\n",
      "[116]\tvalidation_0-mlogloss:2.22823\n",
      "[117]\tvalidation_0-mlogloss:2.22591\n",
      "[118]\tvalidation_0-mlogloss:2.22493\n",
      "[119]\tvalidation_0-mlogloss:2.22299\n",
      "[120]\tvalidation_0-mlogloss:2.22026\n",
      "[121]\tvalidation_0-mlogloss:2.21838\n",
      "[122]\tvalidation_0-mlogloss:2.21615\n",
      "[123]\tvalidation_0-mlogloss:2.21497\n",
      "[124]\tvalidation_0-mlogloss:2.21394\n",
      "[125]\tvalidation_0-mlogloss:2.21243\n",
      "[126]\tvalidation_0-mlogloss:2.21023\n",
      "[127]\tvalidation_0-mlogloss:2.20880\n",
      "[128]\tvalidation_0-mlogloss:2.20783\n",
      "[129]\tvalidation_0-mlogloss:2.20495\n",
      "[130]\tvalidation_0-mlogloss:2.20283\n",
      "[131]\tvalidation_0-mlogloss:2.20078\n",
      "[132]\tvalidation_0-mlogloss:2.19917\n",
      "[133]\tvalidation_0-mlogloss:2.19805\n",
      "[134]\tvalidation_0-mlogloss:2.19701\n",
      "[135]\tvalidation_0-mlogloss:2.19482\n",
      "[136]\tvalidation_0-mlogloss:2.19322\n",
      "[137]\tvalidation_0-mlogloss:2.19196\n",
      "[138]\tvalidation_0-mlogloss:2.18977\n",
      "[139]\tvalidation_0-mlogloss:2.18732\n",
      "[140]\tvalidation_0-mlogloss:2.18646\n",
      "[141]\tvalidation_0-mlogloss:2.18496\n",
      "[142]\tvalidation_0-mlogloss:2.18325\n",
      "[143]\tvalidation_0-mlogloss:2.18245\n",
      "[144]\tvalidation_0-mlogloss:2.18111\n",
      "[145]\tvalidation_0-mlogloss:2.17967\n",
      "[146]\tvalidation_0-mlogloss:2.17733\n",
      "[147]\tvalidation_0-mlogloss:2.17588\n",
      "[148]\tvalidation_0-mlogloss:2.17512\n",
      "[149]\tvalidation_0-mlogloss:2.17294\n",
      "[150]\tvalidation_0-mlogloss:2.17131\n",
      "[151]\tvalidation_0-mlogloss:2.17135\n",
      "[152]\tvalidation_0-mlogloss:2.17052\n",
      "[153]\tvalidation_0-mlogloss:2.16926\n",
      "[154]\tvalidation_0-mlogloss:2.16866\n",
      "[155]\tvalidation_0-mlogloss:2.16645\n",
      "[156]\tvalidation_0-mlogloss:2.16456\n",
      "[157]\tvalidation_0-mlogloss:2.16289\n",
      "[158]\tvalidation_0-mlogloss:2.16104\n",
      "[159]\tvalidation_0-mlogloss:2.15989\n",
      "[160]\tvalidation_0-mlogloss:2.15898\n",
      "[161]\tvalidation_0-mlogloss:2.15818\n",
      "[162]\tvalidation_0-mlogloss:2.15774\n",
      "[163]\tvalidation_0-mlogloss:2.15732\n",
      "[164]\tvalidation_0-mlogloss:2.15627\n",
      "[165]\tvalidation_0-mlogloss:2.15545\n",
      "[166]\tvalidation_0-mlogloss:2.15496\n",
      "[167]\tvalidation_0-mlogloss:2.15413\n",
      "[168]\tvalidation_0-mlogloss:2.15380\n",
      "[169]\tvalidation_0-mlogloss:2.15157\n",
      "[170]\tvalidation_0-mlogloss:2.15048\n",
      "[171]\tvalidation_0-mlogloss:2.14981\n",
      "[172]\tvalidation_0-mlogloss:2.14925\n",
      "[173]\tvalidation_0-mlogloss:2.14863\n",
      "[174]\tvalidation_0-mlogloss:2.14753\n",
      "[175]\tvalidation_0-mlogloss:2.14580\n",
      "[176]\tvalidation_0-mlogloss:2.14606\n",
      "[177]\tvalidation_0-mlogloss:2.14563\n",
      "[178]\tvalidation_0-mlogloss:2.14378\n",
      "[179]\tvalidation_0-mlogloss:2.14380\n",
      "[180]\tvalidation_0-mlogloss:2.14389\n",
      "[181]\tvalidation_0-mlogloss:2.14405\n",
      "[182]\tvalidation_0-mlogloss:2.14362\n",
      "[183]\tvalidation_0-mlogloss:2.14326\n",
      "[184]\tvalidation_0-mlogloss:2.14307\n",
      "[185]\tvalidation_0-mlogloss:2.14223\n",
      "[186]\tvalidation_0-mlogloss:2.14146\n",
      "[187]\tvalidation_0-mlogloss:2.14154\n",
      "[188]\tvalidation_0-mlogloss:2.14084\n",
      "[189]\tvalidation_0-mlogloss:2.13952\n",
      "[190]\tvalidation_0-mlogloss:2.13885\n",
      "[191]\tvalidation_0-mlogloss:2.13710\n",
      "[192]\tvalidation_0-mlogloss:2.13547\n",
      "[193]\tvalidation_0-mlogloss:2.13573\n",
      "[194]\tvalidation_0-mlogloss:2.13392\n",
      "[195]\tvalidation_0-mlogloss:2.13463\n",
      "[196]\tvalidation_0-mlogloss:2.13381\n",
      "[197]\tvalidation_0-mlogloss:2.13240\n",
      "[198]\tvalidation_0-mlogloss:2.13151\n",
      "[199]\tvalidation_0-mlogloss:2.13069\n",
      "[200]\tvalidation_0-mlogloss:2.13037\n",
      "[201]\tvalidation_0-mlogloss:2.13067\n",
      "[202]\tvalidation_0-mlogloss:2.12936\n",
      "[203]\tvalidation_0-mlogloss:2.12907\n",
      "[204]\tvalidation_0-mlogloss:2.12927\n",
      "[205]\tvalidation_0-mlogloss:2.12855\n",
      "[206]\tvalidation_0-mlogloss:2.12774\n",
      "[207]\tvalidation_0-mlogloss:2.12692\n",
      "[208]\tvalidation_0-mlogloss:2.12656\n",
      "[209]\tvalidation_0-mlogloss:2.12644\n",
      "[210]\tvalidation_0-mlogloss:2.12579\n",
      "[211]\tvalidation_0-mlogloss:2.12535\n",
      "[212]\tvalidation_0-mlogloss:2.12453\n",
      "[213]\tvalidation_0-mlogloss:2.12621\n",
      "[214]\tvalidation_0-mlogloss:2.12658\n",
      "[215]\tvalidation_0-mlogloss:2.12645\n",
      "[216]\tvalidation_0-mlogloss:2.12657\n",
      "[217]\tvalidation_0-mlogloss:2.12608\n",
      "[218]\tvalidation_0-mlogloss:2.12532\n",
      "[219]\tvalidation_0-mlogloss:2.12472\n",
      "[220]\tvalidation_0-mlogloss:2.12452\n",
      "[221]\tvalidation_0-mlogloss:2.12465\n",
      "[222]\tvalidation_0-mlogloss:2.12494\n",
      "[223]\tvalidation_0-mlogloss:2.12459\n",
      "[224]\tvalidation_0-mlogloss:2.12397\n",
      "[225]\tvalidation_0-mlogloss:2.12406\n",
      "[226]\tvalidation_0-mlogloss:2.12316\n",
      "[227]\tvalidation_0-mlogloss:2.12345\n",
      "[228]\tvalidation_0-mlogloss:2.12276\n",
      "[229]\tvalidation_0-mlogloss:2.12273\n",
      "[230]\tvalidation_0-mlogloss:2.12257\n",
      "[231]\tvalidation_0-mlogloss:2.12195\n",
      "[232]\tvalidation_0-mlogloss:2.12178\n",
      "[233]\tvalidation_0-mlogloss:2.12193\n",
      "[234]\tvalidation_0-mlogloss:2.12088\n",
      "[235]\tvalidation_0-mlogloss:2.12050\n",
      "[236]\tvalidation_0-mlogloss:2.12026\n",
      "[237]\tvalidation_0-mlogloss:2.12006\n",
      "[238]\tvalidation_0-mlogloss:2.11922\n",
      "[239]\tvalidation_0-mlogloss:2.11863\n",
      "[240]\tvalidation_0-mlogloss:2.11847\n",
      "[241]\tvalidation_0-mlogloss:2.11780\n",
      "[242]\tvalidation_0-mlogloss:2.11760\n",
      "[243]\tvalidation_0-mlogloss:2.11753\n",
      "[244]\tvalidation_0-mlogloss:2.11816\n",
      "[245]\tvalidation_0-mlogloss:2.11877\n",
      "[246]\tvalidation_0-mlogloss:2.11806\n",
      "[247]\tvalidation_0-mlogloss:2.11737\n",
      "[248]\tvalidation_0-mlogloss:2.11648\n",
      "[249]\tvalidation_0-mlogloss:2.11594\n",
      "[250]\tvalidation_0-mlogloss:2.11577\n",
      "[251]\tvalidation_0-mlogloss:2.11490\n",
      "[252]\tvalidation_0-mlogloss:2.11412\n",
      "[253]\tvalidation_0-mlogloss:2.11407\n",
      "[254]\tvalidation_0-mlogloss:2.11436\n",
      "[255]\tvalidation_0-mlogloss:2.11421\n",
      "[256]\tvalidation_0-mlogloss:2.11437\n",
      "[257]\tvalidation_0-mlogloss:2.11514\n",
      "[258]\tvalidation_0-mlogloss:2.11487\n",
      "[259]\tvalidation_0-mlogloss:2.11510\n",
      "[260]\tvalidation_0-mlogloss:2.11524\n",
      "[261]\tvalidation_0-mlogloss:2.11500\n",
      "[262]\tvalidation_0-mlogloss:2.11467\n",
      "[263]\tvalidation_0-mlogloss:2.11453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.26      0.28      1499\n",
      "           1       0.12      0.16      0.14      1499\n",
      "           2       0.40      0.27      0.32      1499\n",
      "           3       0.59      0.43      0.50      1499\n",
      "           4       0.88      0.83      0.86      1499\n",
      "           5       0.57      0.58      0.57      1499\n",
      "           6       0.65      0.74      0.69      1499\n",
      "           7       0.55      0.43      0.48      1499\n",
      "           8       0.62      0.69      0.65      1499\n",
      "           9       0.91      0.64      0.75      1499\n",
      "          10       0.31      0.21      0.25      1499\n",
      "          11       0.51      0.56      0.54      1499\n",
      "          12       0.34      0.52      0.41      1499\n",
      "          13       0.19      0.22      0.20      1499\n",
      "          14       0.22      0.16      0.19      1499\n",
      "          15       0.64      0.42      0.51      1499\n",
      "          16       0.39      0.10      0.16      1499\n",
      "          17       0.47      0.59      0.52      1499\n",
      "          18       0.31      0.48      0.38      1499\n",
      "          19       0.30      0.39      0.34      1499\n",
      "          20       0.80      0.91      0.85      1499\n",
      "          21       0.44      0.35      0.39      1499\n",
      "          22       0.35      0.31      0.33      1499\n",
      "          23       0.83      0.85      0.84      1499\n",
      "          24       0.56      0.47      0.51      1499\n",
      "          25       0.50      0.33      0.40      1499\n",
      "          26       0.36      0.38      0.37      1499\n",
      "          27       0.33      0.30      0.31      1499\n",
      "          28       0.49      0.54      0.51      1499\n",
      "          29       0.39      0.63      0.48      1499\n",
      "          30       0.36      0.30      0.33      1499\n",
      "          31       0.30      0.44      0.36      1499\n",
      "          32       0.11      0.07      0.09      1499\n",
      "          33       0.37      0.36      0.37      1499\n",
      "          34       0.76      0.76      0.76      1499\n",
      "          35       0.54      0.74      0.63      1499\n",
      "          36       0.45      0.42      0.44      1499\n",
      "          37       0.41      0.59      0.48      1499\n",
      "          38       0.71      0.56      0.63      1499\n",
      "          39       0.71      0.55      0.62      1499\n",
      "          40       0.67      0.42      0.51      1499\n",
      "          41       0.60      0.65      0.62      1499\n",
      "          42       0.29      0.17      0.21      1499\n",
      "          43       0.40      0.37      0.38      1499\n",
      "          44       0.63      0.77      0.69      1499\n",
      "          45       0.27      0.25      0.26      1499\n",
      "          46       0.25      0.41      0.32      1499\n",
      "          47       0.74      0.57      0.64      1499\n",
      "          48       0.56      0.76      0.64      1499\n",
      "          49       0.38      0.40      0.39      1499\n",
      "\n",
      "    accuracy                           0.47     74950\n",
      "   macro avg       0.48      0.47      0.46     74950\n",
      "weighted avg       0.48      0.47      0.46     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7844baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7980b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 19s 984us/step - loss: 1.6897 - val_loss: 2.6328\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 15s 814us/step - loss: 1.0103 - val_loss: 2.7506\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 16s 850us/step - loss: 0.8799 - val_loss: 2.8169\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 15s 811us/step - loss: 0.8145 - val_loss: 2.8397\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 15s 812us/step - loss: 0.7716 - val_loss: 3.0011\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 15s 816us/step - loss: 0.7403 - val_loss: 3.0550\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 15s 820us/step - loss: 0.7113 - val_loss: 2.9857\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 15s 818us/step - loss: 0.6854 - val_loss: 2.9270\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 16s 845us/step - loss: 0.6628 - val_loss: 2.9154\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 16s 837us/step - loss: 0.6464 - val_loss: 2.9681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2f7b456f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 1s 349us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.42      0.41      1499\n",
      "           1       0.14      0.15      0.15      1499\n",
      "           2       0.45      0.30      0.36      1499\n",
      "           3       0.70      0.50      0.59      1499\n",
      "           4       0.91      0.80      0.85      1499\n",
      "           5       0.63      0.28      0.39      1499\n",
      "           6       0.57      0.85      0.68      1499\n",
      "           7       0.84      0.59      0.69      1499\n",
      "           8       0.59      0.64      0.62      1499\n",
      "           9       0.77      0.50      0.61      1499\n",
      "          10       0.54      0.40      0.46      1499\n",
      "          11       0.56      0.43      0.49      1499\n",
      "          12       0.43      0.51      0.47      1499\n",
      "          13       0.26      0.30      0.28      1499\n",
      "          14       0.25      0.24      0.24      1499\n",
      "          15       0.66      0.49      0.56      1499\n",
      "          16       0.09      0.05      0.07      1499\n",
      "          17       0.82      0.30      0.44      1499\n",
      "          18       0.27      0.69      0.39      1499\n",
      "          19       0.41      0.30      0.35      1499\n",
      "          20       0.79      0.83      0.81      1499\n",
      "          21       0.29      0.51      0.37      1499\n",
      "          22       0.37      0.39      0.38      1499\n",
      "          23       0.78      0.89      0.83      1499\n",
      "          24       0.65      0.40      0.49      1499\n",
      "          25       0.66      0.28      0.40      1499\n",
      "          26       0.29      0.25      0.27      1499\n",
      "          27       0.29      0.28      0.28      1499\n",
      "          28       0.52      0.79      0.63      1499\n",
      "          29       0.51      0.66      0.57      1499\n",
      "          30       0.39      0.58      0.46      1499\n",
      "          31       0.36      0.32      0.34      1499\n",
      "          32       0.06      0.06      0.06      1499\n",
      "          33       0.47      0.21      0.29      1499\n",
      "          34       0.96      0.60      0.73      1499\n",
      "          35       0.60      0.83      0.70      1499\n",
      "          36       0.33      0.10      0.15      1499\n",
      "          37       0.40      0.58      0.47      1499\n",
      "          38       0.53      0.40      0.45      1499\n",
      "          39       0.53      0.50      0.51      1499\n",
      "          40       0.62      0.27      0.38      1499\n",
      "          41       0.81      0.26      0.40      1499\n",
      "          42       0.25      0.26      0.25      1499\n",
      "          43       0.36      0.65      0.46      1499\n",
      "          44       0.57      0.83      0.68      1499\n",
      "          45       0.31      0.16      0.21      1499\n",
      "          46       0.21      0.40      0.28      1499\n",
      "          47       0.66      0.65      0.65      1499\n",
      "          48       0.70      0.81      0.75      1499\n",
      "          49       0.34      0.65      0.44      1499\n",
      "\n",
      "    accuracy                           0.46     74950\n",
      "   macro avg       0.50      0.46      0.46     74950\n",
      "weighted avg       0.50      0.46      0.46     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4465/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_4465/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_4465/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.43197\n",
      "[1]\tvalidation_0-mlogloss:3.21146\n",
      "[2]\tvalidation_0-mlogloss:3.07842\n",
      "[3]\tvalidation_0-mlogloss:2.97043\n",
      "[4]\tvalidation_0-mlogloss:2.89345\n",
      "[5]\tvalidation_0-mlogloss:2.82348\n",
      "[6]\tvalidation_0-mlogloss:2.75691\n",
      "[7]\tvalidation_0-mlogloss:2.70334\n",
      "[8]\tvalidation_0-mlogloss:2.65758\n",
      "[9]\tvalidation_0-mlogloss:2.61606\n",
      "[10]\tvalidation_0-mlogloss:2.57477\n",
      "[11]\tvalidation_0-mlogloss:2.53175\n",
      "[12]\tvalidation_0-mlogloss:2.49910\n",
      "[13]\tvalidation_0-mlogloss:2.46778\n",
      "[14]\tvalidation_0-mlogloss:2.43974\n",
      "[15]\tvalidation_0-mlogloss:2.41224\n",
      "[16]\tvalidation_0-mlogloss:2.38896\n",
      "[17]\tvalidation_0-mlogloss:2.36210\n",
      "[18]\tvalidation_0-mlogloss:2.34049\n",
      "[19]\tvalidation_0-mlogloss:2.31448\n",
      "[20]\tvalidation_0-mlogloss:2.29445\n",
      "[21]\tvalidation_0-mlogloss:2.27922\n",
      "[22]\tvalidation_0-mlogloss:2.26131\n",
      "[23]\tvalidation_0-mlogloss:2.24316\n",
      "[24]\tvalidation_0-mlogloss:2.22098\n",
      "[25]\tvalidation_0-mlogloss:2.20459\n",
      "[26]\tvalidation_0-mlogloss:2.19083\n",
      "[27]\tvalidation_0-mlogloss:2.17829\n",
      "[28]\tvalidation_0-mlogloss:2.16082\n",
      "[29]\tvalidation_0-mlogloss:2.14734\n",
      "[30]\tvalidation_0-mlogloss:2.13251\n",
      "[31]\tvalidation_0-mlogloss:2.11816\n",
      "[32]\tvalidation_0-mlogloss:2.10626\n",
      "[33]\tvalidation_0-mlogloss:2.09140\n",
      "[34]\tvalidation_0-mlogloss:2.07884\n",
      "[35]\tvalidation_0-mlogloss:2.06666\n",
      "[36]\tvalidation_0-mlogloss:2.05647\n",
      "[37]\tvalidation_0-mlogloss:2.04531\n",
      "[38]\tvalidation_0-mlogloss:2.03228\n",
      "[39]\tvalidation_0-mlogloss:2.02060\n",
      "[40]\tvalidation_0-mlogloss:2.00859\n",
      "[41]\tvalidation_0-mlogloss:2.00080\n",
      "[42]\tvalidation_0-mlogloss:1.99153\n",
      "[43]\tvalidation_0-mlogloss:1.97991\n",
      "[44]\tvalidation_0-mlogloss:1.96906\n",
      "[45]\tvalidation_0-mlogloss:1.95869\n",
      "[46]\tvalidation_0-mlogloss:1.94907\n",
      "[47]\tvalidation_0-mlogloss:1.93904\n",
      "[48]\tvalidation_0-mlogloss:1.92884\n",
      "[49]\tvalidation_0-mlogloss:1.91793\n",
      "[50]\tvalidation_0-mlogloss:1.90951\n",
      "[51]\tvalidation_0-mlogloss:1.90214\n",
      "[52]\tvalidation_0-mlogloss:1.89234\n",
      "[53]\tvalidation_0-mlogloss:1.88237\n",
      "[54]\tvalidation_0-mlogloss:1.87391\n",
      "[55]\tvalidation_0-mlogloss:1.86897\n",
      "[56]\tvalidation_0-mlogloss:1.86161\n",
      "[57]\tvalidation_0-mlogloss:1.85527\n",
      "[58]\tvalidation_0-mlogloss:1.84631\n",
      "[59]\tvalidation_0-mlogloss:1.83928\n",
      "[60]\tvalidation_0-mlogloss:1.83214\n",
      "[61]\tvalidation_0-mlogloss:1.82557\n",
      "[62]\tvalidation_0-mlogloss:1.81809\n",
      "[63]\tvalidation_0-mlogloss:1.81117\n",
      "[64]\tvalidation_0-mlogloss:1.80421\n",
      "[65]\tvalidation_0-mlogloss:1.80033\n",
      "[66]\tvalidation_0-mlogloss:1.79294\n",
      "[67]\tvalidation_0-mlogloss:1.78681\n",
      "[68]\tvalidation_0-mlogloss:1.77762\n",
      "[69]\tvalidation_0-mlogloss:1.77469\n",
      "[70]\tvalidation_0-mlogloss:1.76822\n",
      "[71]\tvalidation_0-mlogloss:1.76242\n",
      "[72]\tvalidation_0-mlogloss:1.75514\n",
      "[73]\tvalidation_0-mlogloss:1.75001\n",
      "[74]\tvalidation_0-mlogloss:1.74248\n",
      "[75]\tvalidation_0-mlogloss:1.74065\n",
      "[76]\tvalidation_0-mlogloss:1.73475\n",
      "[77]\tvalidation_0-mlogloss:1.72978\n",
      "[78]\tvalidation_0-mlogloss:1.72329\n",
      "[79]\tvalidation_0-mlogloss:1.71894\n",
      "[80]\tvalidation_0-mlogloss:1.71617\n",
      "[81]\tvalidation_0-mlogloss:1.71056\n",
      "[82]\tvalidation_0-mlogloss:1.70625\n",
      "[83]\tvalidation_0-mlogloss:1.69980\n",
      "[84]\tvalidation_0-mlogloss:1.69550\n",
      "[85]\tvalidation_0-mlogloss:1.69092\n",
      "[86]\tvalidation_0-mlogloss:1.68720\n",
      "[87]\tvalidation_0-mlogloss:1.68054\n",
      "[88]\tvalidation_0-mlogloss:1.67590\n",
      "[89]\tvalidation_0-mlogloss:1.67143\n",
      "[90]\tvalidation_0-mlogloss:1.66774\n",
      "[91]\tvalidation_0-mlogloss:1.66465\n",
      "[92]\tvalidation_0-mlogloss:1.66009\n",
      "[93]\tvalidation_0-mlogloss:1.65704\n",
      "[94]\tvalidation_0-mlogloss:1.65333\n",
      "[95]\tvalidation_0-mlogloss:1.64982\n",
      "[96]\tvalidation_0-mlogloss:1.64552\n",
      "[97]\tvalidation_0-mlogloss:1.64173\n",
      "[98]\tvalidation_0-mlogloss:1.63746\n",
      "[99]\tvalidation_0-mlogloss:1.63369\n",
      "[100]\tvalidation_0-mlogloss:1.62827\n",
      "[101]\tvalidation_0-mlogloss:1.62417\n",
      "[102]\tvalidation_0-mlogloss:1.61942\n",
      "[103]\tvalidation_0-mlogloss:1.61651\n",
      "[104]\tvalidation_0-mlogloss:1.61301\n",
      "[105]\tvalidation_0-mlogloss:1.60772\n",
      "[106]\tvalidation_0-mlogloss:1.60472\n",
      "[107]\tvalidation_0-mlogloss:1.60050\n",
      "[108]\tvalidation_0-mlogloss:1.59873\n",
      "[109]\tvalidation_0-mlogloss:1.59593\n",
      "[110]\tvalidation_0-mlogloss:1.59124\n",
      "[111]\tvalidation_0-mlogloss:1.58708\n",
      "[112]\tvalidation_0-mlogloss:1.58494\n",
      "[113]\tvalidation_0-mlogloss:1.58224\n",
      "[114]\tvalidation_0-mlogloss:1.57796\n",
      "[115]\tvalidation_0-mlogloss:1.57687\n",
      "[116]\tvalidation_0-mlogloss:1.57678\n",
      "[117]\tvalidation_0-mlogloss:1.57330\n",
      "[118]\tvalidation_0-mlogloss:1.57050\n",
      "[119]\tvalidation_0-mlogloss:1.56883\n",
      "[120]\tvalidation_0-mlogloss:1.56858\n",
      "[121]\tvalidation_0-mlogloss:1.56555\n",
      "[122]\tvalidation_0-mlogloss:1.56213\n",
      "[123]\tvalidation_0-mlogloss:1.55876\n",
      "[124]\tvalidation_0-mlogloss:1.55662\n",
      "[125]\tvalidation_0-mlogloss:1.55378\n",
      "[126]\tvalidation_0-mlogloss:1.55052\n",
      "[127]\tvalidation_0-mlogloss:1.54843\n",
      "[128]\tvalidation_0-mlogloss:1.54663\n",
      "[129]\tvalidation_0-mlogloss:1.54420\n",
      "[130]\tvalidation_0-mlogloss:1.54303\n",
      "[131]\tvalidation_0-mlogloss:1.54079\n",
      "[132]\tvalidation_0-mlogloss:1.53984\n",
      "[133]\tvalidation_0-mlogloss:1.53790\n",
      "[134]\tvalidation_0-mlogloss:1.53598\n",
      "[135]\tvalidation_0-mlogloss:1.53281\n",
      "[136]\tvalidation_0-mlogloss:1.53063\n",
      "[137]\tvalidation_0-mlogloss:1.52929\n",
      "[138]\tvalidation_0-mlogloss:1.52716\n",
      "[139]\tvalidation_0-mlogloss:1.52534\n",
      "[140]\tvalidation_0-mlogloss:1.52308\n",
      "[141]\tvalidation_0-mlogloss:1.52051\n",
      "[142]\tvalidation_0-mlogloss:1.51884\n",
      "[143]\tvalidation_0-mlogloss:1.51795\n",
      "[144]\tvalidation_0-mlogloss:1.51515\n",
      "[145]\tvalidation_0-mlogloss:1.51350\n",
      "[146]\tvalidation_0-mlogloss:1.51084\n",
      "[147]\tvalidation_0-mlogloss:1.50871\n",
      "[148]\tvalidation_0-mlogloss:1.50811\n",
      "[149]\tvalidation_0-mlogloss:1.50721\n",
      "[150]\tvalidation_0-mlogloss:1.50582\n",
      "[151]\tvalidation_0-mlogloss:1.50538\n",
      "[152]\tvalidation_0-mlogloss:1.50432\n",
      "[153]\tvalidation_0-mlogloss:1.50174\n",
      "[154]\tvalidation_0-mlogloss:1.49901\n",
      "[155]\tvalidation_0-mlogloss:1.49907\n",
      "[156]\tvalidation_0-mlogloss:1.49887\n",
      "[157]\tvalidation_0-mlogloss:1.49762\n",
      "[158]\tvalidation_0-mlogloss:1.49524\n",
      "[159]\tvalidation_0-mlogloss:1.49426\n",
      "[160]\tvalidation_0-mlogloss:1.49298\n",
      "[161]\tvalidation_0-mlogloss:1.49114\n",
      "[162]\tvalidation_0-mlogloss:1.48922\n",
      "[163]\tvalidation_0-mlogloss:1.48716\n",
      "[164]\tvalidation_0-mlogloss:1.48679\n",
      "[165]\tvalidation_0-mlogloss:1.48628\n",
      "[166]\tvalidation_0-mlogloss:1.48594\n",
      "[167]\tvalidation_0-mlogloss:1.48400\n",
      "[168]\tvalidation_0-mlogloss:1.48280\n",
      "[169]\tvalidation_0-mlogloss:1.48331\n",
      "[170]\tvalidation_0-mlogloss:1.48301\n",
      "[171]\tvalidation_0-mlogloss:1.48225\n",
      "[172]\tvalidation_0-mlogloss:1.48209\n",
      "[173]\tvalidation_0-mlogloss:1.48052\n",
      "[174]\tvalidation_0-mlogloss:1.47962\n",
      "[175]\tvalidation_0-mlogloss:1.47713\n",
      "[176]\tvalidation_0-mlogloss:1.47521\n",
      "[177]\tvalidation_0-mlogloss:1.47327\n",
      "[178]\tvalidation_0-mlogloss:1.47190\n",
      "[179]\tvalidation_0-mlogloss:1.46998\n",
      "[180]\tvalidation_0-mlogloss:1.46918\n",
      "[181]\tvalidation_0-mlogloss:1.46777\n",
      "[182]\tvalidation_0-mlogloss:1.46635\n",
      "[183]\tvalidation_0-mlogloss:1.46377\n",
      "[184]\tvalidation_0-mlogloss:1.46286\n",
      "[185]\tvalidation_0-mlogloss:1.46287\n",
      "[186]\tvalidation_0-mlogloss:1.46249\n",
      "[187]\tvalidation_0-mlogloss:1.46206\n",
      "[188]\tvalidation_0-mlogloss:1.46202\n",
      "[189]\tvalidation_0-mlogloss:1.46082\n",
      "[190]\tvalidation_0-mlogloss:1.45971\n",
      "[191]\tvalidation_0-mlogloss:1.45724\n",
      "[192]\tvalidation_0-mlogloss:1.45620\n",
      "[193]\tvalidation_0-mlogloss:1.45415\n",
      "[194]\tvalidation_0-mlogloss:1.45348\n",
      "[195]\tvalidation_0-mlogloss:1.45333\n",
      "[196]\tvalidation_0-mlogloss:1.45126\n",
      "[197]\tvalidation_0-mlogloss:1.45060\n",
      "[198]\tvalidation_0-mlogloss:1.45016\n",
      "[199]\tvalidation_0-mlogloss:1.44834\n",
      "[200]\tvalidation_0-mlogloss:1.44738\n",
      "[201]\tvalidation_0-mlogloss:1.44654\n",
      "[202]\tvalidation_0-mlogloss:1.44516\n",
      "[203]\tvalidation_0-mlogloss:1.44572\n",
      "[204]\tvalidation_0-mlogloss:1.44394\n",
      "[205]\tvalidation_0-mlogloss:1.44205\n",
      "[206]\tvalidation_0-mlogloss:1.44127\n",
      "[207]\tvalidation_0-mlogloss:1.44062\n",
      "[208]\tvalidation_0-mlogloss:1.43945\n",
      "[209]\tvalidation_0-mlogloss:1.43917\n",
      "[210]\tvalidation_0-mlogloss:1.44051\n",
      "[211]\tvalidation_0-mlogloss:1.44160\n",
      "[212]\tvalidation_0-mlogloss:1.44007\n",
      "[213]\tvalidation_0-mlogloss:1.44006\n",
      "[214]\tvalidation_0-mlogloss:1.43864\n",
      "[215]\tvalidation_0-mlogloss:1.43761\n",
      "[216]\tvalidation_0-mlogloss:1.43718\n",
      "[217]\tvalidation_0-mlogloss:1.43579\n",
      "[218]\tvalidation_0-mlogloss:1.43509\n",
      "[219]\tvalidation_0-mlogloss:1.43413\n",
      "[220]\tvalidation_0-mlogloss:1.43459\n",
      "[221]\tvalidation_0-mlogloss:1.43506\n",
      "[222]\tvalidation_0-mlogloss:1.43444\n",
      "[223]\tvalidation_0-mlogloss:1.43332\n",
      "[224]\tvalidation_0-mlogloss:1.43302\n",
      "[225]\tvalidation_0-mlogloss:1.43315\n",
      "[226]\tvalidation_0-mlogloss:1.43224\n",
      "[227]\tvalidation_0-mlogloss:1.43180\n",
      "[228]\tvalidation_0-mlogloss:1.43161\n",
      "[229]\tvalidation_0-mlogloss:1.43098\n",
      "[230]\tvalidation_0-mlogloss:1.42985\n",
      "[231]\tvalidation_0-mlogloss:1.43005\n",
      "[232]\tvalidation_0-mlogloss:1.42892\n",
      "[233]\tvalidation_0-mlogloss:1.42810\n",
      "[234]\tvalidation_0-mlogloss:1.42723\n",
      "[235]\tvalidation_0-mlogloss:1.42788\n",
      "[236]\tvalidation_0-mlogloss:1.42855\n",
      "[237]\tvalidation_0-mlogloss:1.42853\n",
      "[238]\tvalidation_0-mlogloss:1.42798\n",
      "[239]\tvalidation_0-mlogloss:1.42818\n",
      "[240]\tvalidation_0-mlogloss:1.42708\n",
      "[241]\tvalidation_0-mlogloss:1.42684\n",
      "[242]\tvalidation_0-mlogloss:1.42547\n",
      "[243]\tvalidation_0-mlogloss:1.42448\n",
      "[244]\tvalidation_0-mlogloss:1.42441\n",
      "[245]\tvalidation_0-mlogloss:1.42448\n",
      "[246]\tvalidation_0-mlogloss:1.42357\n",
      "[247]\tvalidation_0-mlogloss:1.42270\n",
      "[248]\tvalidation_0-mlogloss:1.42236\n",
      "[249]\tvalidation_0-mlogloss:1.42104\n",
      "[250]\tvalidation_0-mlogloss:1.42042\n",
      "[251]\tvalidation_0-mlogloss:1.42021\n",
      "[252]\tvalidation_0-mlogloss:1.42214\n",
      "[253]\tvalidation_0-mlogloss:1.42261\n",
      "[254]\tvalidation_0-mlogloss:1.42267\n",
      "[255]\tvalidation_0-mlogloss:1.42187\n",
      "[256]\tvalidation_0-mlogloss:1.42173\n",
      "[257]\tvalidation_0-mlogloss:1.42210\n",
      "[258]\tvalidation_0-mlogloss:1.42117\n",
      "[259]\tvalidation_0-mlogloss:1.42076\n",
      "[260]\tvalidation_0-mlogloss:1.41949\n",
      "[261]\tvalidation_0-mlogloss:1.41852\n",
      "[262]\tvalidation_0-mlogloss:1.41823\n",
      "[263]\tvalidation_0-mlogloss:1.41802\n",
      "[264]\tvalidation_0-mlogloss:1.41750\n",
      "[265]\tvalidation_0-mlogloss:1.41650\n",
      "[266]\tvalidation_0-mlogloss:1.41549\n",
      "[267]\tvalidation_0-mlogloss:1.41419\n",
      "[268]\tvalidation_0-mlogloss:1.41483\n",
      "[269]\tvalidation_0-mlogloss:1.41381\n",
      "[270]\tvalidation_0-mlogloss:1.41365\n",
      "[271]\tvalidation_0-mlogloss:1.41516\n",
      "[272]\tvalidation_0-mlogloss:1.41460\n",
      "[273]\tvalidation_0-mlogloss:1.41489\n",
      "[274]\tvalidation_0-mlogloss:1.41484\n",
      "[275]\tvalidation_0-mlogloss:1.41515\n",
      "[276]\tvalidation_0-mlogloss:1.41529\n",
      "[277]\tvalidation_0-mlogloss:1.41462\n",
      "[278]\tvalidation_0-mlogloss:1.41474\n",
      "[279]\tvalidation_0-mlogloss:1.41422\n",
      "[280]\tvalidation_0-mlogloss:1.41346\n",
      "[281]\tvalidation_0-mlogloss:1.41381\n",
      "[282]\tvalidation_0-mlogloss:1.41347\n",
      "[283]\tvalidation_0-mlogloss:1.41360\n",
      "[284]\tvalidation_0-mlogloss:1.41354\n",
      "[285]\tvalidation_0-mlogloss:1.41359\n",
      "[286]\tvalidation_0-mlogloss:1.41344\n",
      "[287]\tvalidation_0-mlogloss:1.41352\n",
      "[288]\tvalidation_0-mlogloss:1.41370\n",
      "[289]\tvalidation_0-mlogloss:1.41370\n",
      "[290]\tvalidation_0-mlogloss:1.41325\n",
      "[291]\tvalidation_0-mlogloss:1.41295\n",
      "[292]\tvalidation_0-mlogloss:1.41316\n",
      "[293]\tvalidation_0-mlogloss:1.41288\n",
      "[294]\tvalidation_0-mlogloss:1.41271\n",
      "[295]\tvalidation_0-mlogloss:1.41251\n",
      "[296]\tvalidation_0-mlogloss:1.41292\n",
      "[297]\tvalidation_0-mlogloss:1.41346\n",
      "[298]\tvalidation_0-mlogloss:1.41330\n",
      "[299]\tvalidation_0-mlogloss:1.41274\n",
      "[300]\tvalidation_0-mlogloss:1.41250\n",
      "[301]\tvalidation_0-mlogloss:1.41263\n",
      "[302]\tvalidation_0-mlogloss:1.41349\n",
      "[303]\tvalidation_0-mlogloss:1.41306\n",
      "[304]\tvalidation_0-mlogloss:1.41271\n",
      "[305]\tvalidation_0-mlogloss:1.41233\n",
      "[306]\tvalidation_0-mlogloss:1.41302\n",
      "[307]\tvalidation_0-mlogloss:1.41334\n",
      "[308]\tvalidation_0-mlogloss:1.41276\n",
      "[309]\tvalidation_0-mlogloss:1.41226\n",
      "[310]\tvalidation_0-mlogloss:1.41199\n",
      "[311]\tvalidation_0-mlogloss:1.41106\n",
      "[312]\tvalidation_0-mlogloss:1.41123\n",
      "[313]\tvalidation_0-mlogloss:1.41108\n",
      "[314]\tvalidation_0-mlogloss:1.41023\n",
      "[315]\tvalidation_0-mlogloss:1.40996\n",
      "[316]\tvalidation_0-mlogloss:1.41009\n",
      "[317]\tvalidation_0-mlogloss:1.41062\n",
      "[318]\tvalidation_0-mlogloss:1.41086\n",
      "[319]\tvalidation_0-mlogloss:1.41024\n",
      "[320]\tvalidation_0-mlogloss:1.41058\n",
      "[321]\tvalidation_0-mlogloss:1.41066\n",
      "[322]\tvalidation_0-mlogloss:1.41050\n",
      "[323]\tvalidation_0-mlogloss:1.41140\n",
      "[324]\tvalidation_0-mlogloss:1.41130\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.64      0.59      1499\n",
      "           1       0.32      0.22      0.26      1499\n",
      "           2       0.73      0.83      0.77      1499\n",
      "           3       0.66      0.34      0.45      1499\n",
      "           4       0.90      0.93      0.92      1499\n",
      "           5       0.88      0.88      0.88      1499\n",
      "           6       0.78      0.81      0.80      1499\n",
      "           7       0.65      0.39      0.48      1499\n",
      "           8       0.76      0.85      0.80      1499\n",
      "           9       0.88      0.68      0.77      1499\n",
      "          10       0.81      0.62      0.70      1499\n",
      "          11       0.94      0.89      0.92      1499\n",
      "          12       0.70      0.83      0.76      1499\n",
      "          13       0.54      0.54      0.54      1499\n",
      "          14       0.48      0.62      0.54      1499\n",
      "          15       0.68      0.69      0.69      1499\n",
      "          16       0.61      0.41      0.49      1499\n",
      "          17       0.69      0.82      0.75      1499\n",
      "          18       0.73      0.88      0.80      1499\n",
      "          19       0.41      0.45      0.43      1499\n",
      "          20       0.95      0.96      0.95      1499\n",
      "          21       0.69      0.73      0.71      1499\n",
      "          22       0.59      0.74      0.65      1499\n",
      "          23       0.94      0.95      0.94      1499\n",
      "          24       0.76      0.59      0.67      1499\n",
      "          25       0.77      0.49      0.60      1499\n",
      "          26       0.61      0.32      0.42      1499\n",
      "          27       0.45      0.28      0.34      1499\n",
      "          28       0.68      0.73      0.70      1499\n",
      "          29       0.75      0.85      0.79      1499\n",
      "          30       0.51      0.66      0.58      1499\n",
      "          31       0.38      0.69      0.49      1499\n",
      "          32       0.17      0.14      0.16      1499\n",
      "          33       0.64      0.74      0.68      1499\n",
      "          34       0.96      0.92      0.94      1499\n",
      "          35       0.66      0.91      0.77      1499\n",
      "          36       0.81      0.75      0.78      1499\n",
      "          37       0.61      0.88      0.72      1499\n",
      "          38       0.76      0.61      0.68      1499\n",
      "          39       0.75      0.64      0.69      1499\n",
      "          40       0.93      0.65      0.76      1499\n",
      "          41       0.69      0.78      0.73      1499\n",
      "          42       0.54      0.34      0.41      1499\n",
      "          43       0.72      0.69      0.70      1499\n",
      "          44       0.79      0.89      0.84      1499\n",
      "          45       0.52      0.48      0.50      1499\n",
      "          46       0.29      0.38      0.33      1499\n",
      "          47       0.78      0.47      0.59      1499\n",
      "          48       0.54      0.84      0.65      1499\n",
      "          49       0.57      0.59      0.58      1499\n",
      "\n",
      "    accuracy                           0.66     74950\n",
      "   macro avg       0.67      0.66      0.65     74950\n",
      "weighted avg       0.67      0.66      0.65     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8f654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "300aa492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 18s 932us/step - loss: 1.1595 - val_loss: 2.6192\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 17s 932us/step - loss: 0.4525 - val_loss: 2.8171\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 17s 904us/step - loss: 0.3301 - val_loss: 2.7147\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 17s 925us/step - loss: 0.2749 - val_loss: 2.8720\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 19s 1ms/step - loss: 0.2407 - val_loss: 2.7542\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 17s 909us/step - loss: 0.2180 - val_loss: 2.4409\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 17s 924us/step - loss: 0.2019 - val_loss: 2.5652\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 17s 930us/step - loss: 0.1867 - val_loss: 2.5334\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 17s 930us/step - loss: 0.1753 - val_loss: 2.5839\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 18s 934us/step - loss: 0.1662 - val_loss: 2.4996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x30a65e680>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 1s 360us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71      1499\n",
      "           1       0.30      0.20      0.24      1499\n",
      "           2       0.65      0.86      0.74      1499\n",
      "           3       0.66      0.34      0.45      1499\n",
      "           4       0.91      0.95      0.93      1499\n",
      "           5       0.96      0.77      0.85      1499\n",
      "           6       0.77      0.94      0.85      1499\n",
      "           7       0.77      0.38      0.51      1499\n",
      "           8       0.57      0.94      0.71      1499\n",
      "           9       0.76      0.67      0.71      1499\n",
      "          10       0.57      0.90      0.70      1499\n",
      "          11       0.97      0.80      0.88      1499\n",
      "          12       0.73      0.85      0.78      1499\n",
      "          13       0.56      0.82      0.67      1499\n",
      "          14       0.59      0.54      0.56      1499\n",
      "          15       0.94      0.72      0.81      1499\n",
      "          16       0.41      0.21      0.27      1499\n",
      "          17       0.56      0.97      0.71      1499\n",
      "          18       0.82      0.87      0.85      1499\n",
      "          19       0.71      0.32      0.44      1499\n",
      "          20       0.95      0.98      0.97      1499\n",
      "          21       0.57      0.78      0.66      1499\n",
      "          22       0.76      0.71      0.73      1499\n",
      "          23       0.95      0.98      0.97      1499\n",
      "          24       0.73      0.67      0.70      1499\n",
      "          25       0.73      0.67      0.70      1499\n",
      "          26       0.35      0.30      0.32      1499\n",
      "          27       0.60      0.51      0.55      1499\n",
      "          28       0.87      0.77      0.81      1499\n",
      "          29       0.89      0.40      0.56      1499\n",
      "          30       0.79      0.77      0.78      1499\n",
      "          31       0.61      0.72      0.66      1499\n",
      "          32       0.32      0.15      0.21      1499\n",
      "          33       0.71      0.76      0.74      1499\n",
      "          34       0.99      0.81      0.89      1499\n",
      "          35       0.85      0.86      0.85      1499\n",
      "          36       0.91      0.58      0.71      1499\n",
      "          37       0.60      0.90      0.72      1499\n",
      "          38       0.54      0.65      0.59      1499\n",
      "          39       0.50      0.67      0.57      1499\n",
      "          40       0.66      0.92      0.77      1499\n",
      "          41       0.66      0.69      0.67      1499\n",
      "          42       0.47      0.53      0.50      1499\n",
      "          43       0.62      0.85      0.72      1499\n",
      "          44       0.71      0.96      0.81      1499\n",
      "          45       0.60      0.44      0.51      1499\n",
      "          46       0.39      0.33      0.36      1499\n",
      "          47       0.87      0.52      0.65      1499\n",
      "          48       0.57      0.72      0.64      1499\n",
      "          49       0.67      0.68      0.68      1499\n",
      "\n",
      "    accuracy                           0.68     74950\n",
      "   macro avg       0.69      0.68      0.67     74950\n",
      "weighted avg       0.69      0.68      0.67     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.17302\n",
      "[1]\tvalidation_0-mlogloss:2.88248\n",
      "[2]\tvalidation_0-mlogloss:2.70533\n",
      "[3]\tvalidation_0-mlogloss:2.57009\n",
      "[4]\tvalidation_0-mlogloss:2.45687\n",
      "[5]\tvalidation_0-mlogloss:2.36547\n",
      "[6]\tvalidation_0-mlogloss:2.28165\n",
      "[7]\tvalidation_0-mlogloss:2.20928\n",
      "[8]\tvalidation_0-mlogloss:2.15253\n",
      "[9]\tvalidation_0-mlogloss:2.09348\n",
      "[10]\tvalidation_0-mlogloss:2.04743\n",
      "[11]\tvalidation_0-mlogloss:2.00298\n",
      "[12]\tvalidation_0-mlogloss:1.96501\n",
      "[13]\tvalidation_0-mlogloss:1.92658\n",
      "[14]\tvalidation_0-mlogloss:1.88770\n",
      "[15]\tvalidation_0-mlogloss:1.85190\n",
      "[16]\tvalidation_0-mlogloss:1.81301\n",
      "[17]\tvalidation_0-mlogloss:1.78710\n",
      "[18]\tvalidation_0-mlogloss:1.76009\n",
      "[19]\tvalidation_0-mlogloss:1.73157\n",
      "[20]\tvalidation_0-mlogloss:1.70950\n",
      "[21]\tvalidation_0-mlogloss:1.69024\n",
      "[22]\tvalidation_0-mlogloss:1.66858\n",
      "[23]\tvalidation_0-mlogloss:1.64575\n",
      "[24]\tvalidation_0-mlogloss:1.62617\n",
      "[25]\tvalidation_0-mlogloss:1.60643\n",
      "[26]\tvalidation_0-mlogloss:1.58944\n",
      "[27]\tvalidation_0-mlogloss:1.57376\n",
      "[28]\tvalidation_0-mlogloss:1.55502\n",
      "[29]\tvalidation_0-mlogloss:1.53969\n",
      "[30]\tvalidation_0-mlogloss:1.52372\n",
      "[31]\tvalidation_0-mlogloss:1.50733\n",
      "[32]\tvalidation_0-mlogloss:1.49251\n",
      "[33]\tvalidation_0-mlogloss:1.47773\n",
      "[34]\tvalidation_0-mlogloss:1.46510\n",
      "[35]\tvalidation_0-mlogloss:1.45164\n",
      "[36]\tvalidation_0-mlogloss:1.43999\n",
      "[37]\tvalidation_0-mlogloss:1.42827\n",
      "[38]\tvalidation_0-mlogloss:1.41235\n",
      "[39]\tvalidation_0-mlogloss:1.40369\n",
      "[40]\tvalidation_0-mlogloss:1.39195\n",
      "[41]\tvalidation_0-mlogloss:1.38009\n",
      "[42]\tvalidation_0-mlogloss:1.36819\n",
      "[43]\tvalidation_0-mlogloss:1.35769\n",
      "[44]\tvalidation_0-mlogloss:1.34681\n",
      "[45]\tvalidation_0-mlogloss:1.33849\n",
      "[46]\tvalidation_0-mlogloss:1.32919\n",
      "[47]\tvalidation_0-mlogloss:1.31928\n",
      "[48]\tvalidation_0-mlogloss:1.30878\n",
      "[49]\tvalidation_0-mlogloss:1.29921\n",
      "[50]\tvalidation_0-mlogloss:1.29036\n",
      "[51]\tvalidation_0-mlogloss:1.28116\n",
      "[52]\tvalidation_0-mlogloss:1.27248\n",
      "[53]\tvalidation_0-mlogloss:1.26277\n",
      "[54]\tvalidation_0-mlogloss:1.25593\n",
      "[55]\tvalidation_0-mlogloss:1.24859\n",
      "[56]\tvalidation_0-mlogloss:1.24067\n",
      "[57]\tvalidation_0-mlogloss:1.23335\n",
      "[58]\tvalidation_0-mlogloss:1.22627\n",
      "[59]\tvalidation_0-mlogloss:1.22077\n",
      "[60]\tvalidation_0-mlogloss:1.21191\n",
      "[61]\tvalidation_0-mlogloss:1.20539\n",
      "[62]\tvalidation_0-mlogloss:1.19956\n",
      "[63]\tvalidation_0-mlogloss:1.19292\n",
      "[64]\tvalidation_0-mlogloss:1.18693\n",
      "[65]\tvalidation_0-mlogloss:1.17881\n",
      "[66]\tvalidation_0-mlogloss:1.17282\n",
      "[67]\tvalidation_0-mlogloss:1.16664\n",
      "[68]\tvalidation_0-mlogloss:1.16194\n",
      "[69]\tvalidation_0-mlogloss:1.15764\n",
      "[70]\tvalidation_0-mlogloss:1.15164\n",
      "[71]\tvalidation_0-mlogloss:1.14531\n",
      "[72]\tvalidation_0-mlogloss:1.13963\n",
      "[73]\tvalidation_0-mlogloss:1.13569\n",
      "[74]\tvalidation_0-mlogloss:1.13076\n",
      "[75]\tvalidation_0-mlogloss:1.12639\n",
      "[76]\tvalidation_0-mlogloss:1.12030\n",
      "[77]\tvalidation_0-mlogloss:1.11581\n",
      "[78]\tvalidation_0-mlogloss:1.11112\n",
      "[79]\tvalidation_0-mlogloss:1.10505\n",
      "[80]\tvalidation_0-mlogloss:1.10108\n",
      "[81]\tvalidation_0-mlogloss:1.09694\n",
      "[82]\tvalidation_0-mlogloss:1.09355\n",
      "[83]\tvalidation_0-mlogloss:1.08800\n",
      "[84]\tvalidation_0-mlogloss:1.08447\n",
      "[85]\tvalidation_0-mlogloss:1.07878\n",
      "[86]\tvalidation_0-mlogloss:1.07424\n",
      "[87]\tvalidation_0-mlogloss:1.07015\n",
      "[88]\tvalidation_0-mlogloss:1.06511\n",
      "[89]\tvalidation_0-mlogloss:1.06175\n",
      "[90]\tvalidation_0-mlogloss:1.05817\n",
      "[91]\tvalidation_0-mlogloss:1.05498\n",
      "[92]\tvalidation_0-mlogloss:1.05117\n",
      "[93]\tvalidation_0-mlogloss:1.04770\n",
      "[94]\tvalidation_0-mlogloss:1.04229\n",
      "[95]\tvalidation_0-mlogloss:1.03864\n",
      "[96]\tvalidation_0-mlogloss:1.03486\n",
      "[97]\tvalidation_0-mlogloss:1.03109\n",
      "[98]\tvalidation_0-mlogloss:1.02644\n",
      "[99]\tvalidation_0-mlogloss:1.02245\n",
      "[100]\tvalidation_0-mlogloss:1.01864\n",
      "[101]\tvalidation_0-mlogloss:1.01566\n",
      "[102]\tvalidation_0-mlogloss:1.01123\n",
      "[103]\tvalidation_0-mlogloss:1.00842\n",
      "[104]\tvalidation_0-mlogloss:1.00516\n",
      "[105]\tvalidation_0-mlogloss:1.00230\n",
      "[106]\tvalidation_0-mlogloss:0.99980\n",
      "[107]\tvalidation_0-mlogloss:0.99707\n",
      "[108]\tvalidation_0-mlogloss:0.99372\n",
      "[109]\tvalidation_0-mlogloss:0.98932\n",
      "[110]\tvalidation_0-mlogloss:0.98717\n",
      "[111]\tvalidation_0-mlogloss:0.98464\n",
      "[112]\tvalidation_0-mlogloss:0.98144\n",
      "[113]\tvalidation_0-mlogloss:0.97930\n",
      "[114]\tvalidation_0-mlogloss:0.97669\n",
      "[115]\tvalidation_0-mlogloss:0.97557\n",
      "[116]\tvalidation_0-mlogloss:0.97282\n",
      "[117]\tvalidation_0-mlogloss:0.96944\n",
      "[118]\tvalidation_0-mlogloss:0.96715\n",
      "[119]\tvalidation_0-mlogloss:0.96343\n",
      "[120]\tvalidation_0-mlogloss:0.96133\n",
      "[121]\tvalidation_0-mlogloss:0.95882\n",
      "[122]\tvalidation_0-mlogloss:0.95584\n",
      "[123]\tvalidation_0-mlogloss:0.95375\n",
      "[124]\tvalidation_0-mlogloss:0.95065\n",
      "[125]\tvalidation_0-mlogloss:0.94865\n",
      "[126]\tvalidation_0-mlogloss:0.94747\n",
      "[127]\tvalidation_0-mlogloss:0.94545\n",
      "[128]\tvalidation_0-mlogloss:0.94340\n",
      "[129]\tvalidation_0-mlogloss:0.94240\n",
      "[130]\tvalidation_0-mlogloss:0.94057\n",
      "[131]\tvalidation_0-mlogloss:0.93799\n",
      "[132]\tvalidation_0-mlogloss:0.93798\n",
      "[133]\tvalidation_0-mlogloss:0.93649\n",
      "[134]\tvalidation_0-mlogloss:0.93403\n",
      "[135]\tvalidation_0-mlogloss:0.93059\n",
      "[136]\tvalidation_0-mlogloss:0.92853\n",
      "[137]\tvalidation_0-mlogloss:0.92837\n",
      "[138]\tvalidation_0-mlogloss:0.92754\n",
      "[139]\tvalidation_0-mlogloss:0.92629\n",
      "[140]\tvalidation_0-mlogloss:0.92543\n",
      "[141]\tvalidation_0-mlogloss:0.92452\n",
      "[142]\tvalidation_0-mlogloss:0.92292\n",
      "[143]\tvalidation_0-mlogloss:0.92083\n",
      "[144]\tvalidation_0-mlogloss:0.91904\n",
      "[145]\tvalidation_0-mlogloss:0.91676\n",
      "[146]\tvalidation_0-mlogloss:0.91554\n",
      "[147]\tvalidation_0-mlogloss:0.91316\n",
      "[148]\tvalidation_0-mlogloss:0.91219\n",
      "[149]\tvalidation_0-mlogloss:0.91107\n",
      "[150]\tvalidation_0-mlogloss:0.91039\n",
      "[151]\tvalidation_0-mlogloss:0.90969\n",
      "[152]\tvalidation_0-mlogloss:0.90688\n",
      "[153]\tvalidation_0-mlogloss:0.90516\n",
      "[154]\tvalidation_0-mlogloss:0.90326\n",
      "[155]\tvalidation_0-mlogloss:0.90246\n",
      "[156]\tvalidation_0-mlogloss:0.90113\n",
      "[157]\tvalidation_0-mlogloss:0.89994\n",
      "[158]\tvalidation_0-mlogloss:0.89871\n",
      "[159]\tvalidation_0-mlogloss:0.89800\n",
      "[160]\tvalidation_0-mlogloss:0.89568\n",
      "[161]\tvalidation_0-mlogloss:0.89372\n",
      "[162]\tvalidation_0-mlogloss:0.89367\n",
      "[163]\tvalidation_0-mlogloss:0.89256\n",
      "[164]\tvalidation_0-mlogloss:0.89071\n",
      "[165]\tvalidation_0-mlogloss:0.89109\n",
      "[166]\tvalidation_0-mlogloss:0.89011\n",
      "[167]\tvalidation_0-mlogloss:0.88835\n",
      "[168]\tvalidation_0-mlogloss:0.88753\n",
      "[169]\tvalidation_0-mlogloss:0.88649\n",
      "[170]\tvalidation_0-mlogloss:0.88511\n",
      "[171]\tvalidation_0-mlogloss:0.88416\n",
      "[172]\tvalidation_0-mlogloss:0.88286\n",
      "[173]\tvalidation_0-mlogloss:0.88210\n",
      "[174]\tvalidation_0-mlogloss:0.88066\n",
      "[175]\tvalidation_0-mlogloss:0.87869\n",
      "[176]\tvalidation_0-mlogloss:0.87734\n",
      "[177]\tvalidation_0-mlogloss:0.87514\n",
      "[178]\tvalidation_0-mlogloss:0.87334\n",
      "[179]\tvalidation_0-mlogloss:0.87210\n",
      "[180]\tvalidation_0-mlogloss:0.87124\n",
      "[181]\tvalidation_0-mlogloss:0.86968\n",
      "[182]\tvalidation_0-mlogloss:0.86945\n",
      "[183]\tvalidation_0-mlogloss:0.86840\n",
      "[184]\tvalidation_0-mlogloss:0.86739\n",
      "[185]\tvalidation_0-mlogloss:0.86640\n",
      "[186]\tvalidation_0-mlogloss:0.86597\n",
      "[187]\tvalidation_0-mlogloss:0.86531\n",
      "[188]\tvalidation_0-mlogloss:0.86467\n",
      "[189]\tvalidation_0-mlogloss:0.86420\n",
      "[190]\tvalidation_0-mlogloss:0.86389\n",
      "[191]\tvalidation_0-mlogloss:0.86280\n",
      "[192]\tvalidation_0-mlogloss:0.86138\n",
      "[193]\tvalidation_0-mlogloss:0.86066\n",
      "[194]\tvalidation_0-mlogloss:0.86006\n",
      "[195]\tvalidation_0-mlogloss:0.85928\n",
      "[196]\tvalidation_0-mlogloss:0.85841\n",
      "[197]\tvalidation_0-mlogloss:0.85783\n",
      "[198]\tvalidation_0-mlogloss:0.85712\n",
      "[199]\tvalidation_0-mlogloss:0.85580\n",
      "[200]\tvalidation_0-mlogloss:0.85621\n",
      "[201]\tvalidation_0-mlogloss:0.85476\n",
      "[202]\tvalidation_0-mlogloss:0.85418\n",
      "[203]\tvalidation_0-mlogloss:0.85428\n",
      "[204]\tvalidation_0-mlogloss:0.85288\n",
      "[205]\tvalidation_0-mlogloss:0.85233\n",
      "[206]\tvalidation_0-mlogloss:0.85203\n",
      "[207]\tvalidation_0-mlogloss:0.85117\n",
      "[208]\tvalidation_0-mlogloss:0.84941\n",
      "[209]\tvalidation_0-mlogloss:0.84810\n",
      "[210]\tvalidation_0-mlogloss:0.84745\n",
      "[211]\tvalidation_0-mlogloss:0.84669\n",
      "[212]\tvalidation_0-mlogloss:0.84583\n",
      "[213]\tvalidation_0-mlogloss:0.84504\n",
      "[214]\tvalidation_0-mlogloss:0.84496\n",
      "[215]\tvalidation_0-mlogloss:0.84301\n",
      "[216]\tvalidation_0-mlogloss:0.84202\n",
      "[217]\tvalidation_0-mlogloss:0.84126\n",
      "[218]\tvalidation_0-mlogloss:0.84027\n",
      "[219]\tvalidation_0-mlogloss:0.83911\n",
      "[220]\tvalidation_0-mlogloss:0.83796\n",
      "[221]\tvalidation_0-mlogloss:0.83886\n",
      "[222]\tvalidation_0-mlogloss:0.83846\n",
      "[223]\tvalidation_0-mlogloss:0.83752\n",
      "[224]\tvalidation_0-mlogloss:0.83676\n",
      "[225]\tvalidation_0-mlogloss:0.83707\n",
      "[226]\tvalidation_0-mlogloss:0.83734\n",
      "[227]\tvalidation_0-mlogloss:0.83629\n",
      "[228]\tvalidation_0-mlogloss:0.83488\n",
      "[229]\tvalidation_0-mlogloss:0.83369\n",
      "[230]\tvalidation_0-mlogloss:0.83307\n",
      "[231]\tvalidation_0-mlogloss:0.83230\n",
      "[232]\tvalidation_0-mlogloss:0.83111\n",
      "[233]\tvalidation_0-mlogloss:0.83013\n",
      "[234]\tvalidation_0-mlogloss:0.82879\n",
      "[235]\tvalidation_0-mlogloss:0.82851\n",
      "[236]\tvalidation_0-mlogloss:0.82862\n",
      "[237]\tvalidation_0-mlogloss:0.82836\n",
      "[238]\tvalidation_0-mlogloss:0.82808\n",
      "[239]\tvalidation_0-mlogloss:0.82707\n",
      "[240]\tvalidation_0-mlogloss:0.82681\n",
      "[241]\tvalidation_0-mlogloss:0.82650\n",
      "[242]\tvalidation_0-mlogloss:0.82577\n",
      "[243]\tvalidation_0-mlogloss:0.82473\n",
      "[244]\tvalidation_0-mlogloss:0.82427\n",
      "[245]\tvalidation_0-mlogloss:0.82377\n",
      "[246]\tvalidation_0-mlogloss:0.82319\n",
      "[247]\tvalidation_0-mlogloss:0.82235\n",
      "[248]\tvalidation_0-mlogloss:0.82210\n",
      "[249]\tvalidation_0-mlogloss:0.82110\n",
      "[250]\tvalidation_0-mlogloss:0.82054\n",
      "[251]\tvalidation_0-mlogloss:0.81982\n",
      "[252]\tvalidation_0-mlogloss:0.81908\n",
      "[253]\tvalidation_0-mlogloss:0.81869\n",
      "[254]\tvalidation_0-mlogloss:0.81887\n",
      "[255]\tvalidation_0-mlogloss:0.81835\n",
      "[256]\tvalidation_0-mlogloss:0.81757\n",
      "[257]\tvalidation_0-mlogloss:0.81774\n",
      "[258]\tvalidation_0-mlogloss:0.81750\n",
      "[259]\tvalidation_0-mlogloss:0.81645\n",
      "[260]\tvalidation_0-mlogloss:0.81607\n",
      "[261]\tvalidation_0-mlogloss:0.81543\n",
      "[262]\tvalidation_0-mlogloss:0.81546\n",
      "[263]\tvalidation_0-mlogloss:0.81515\n",
      "[264]\tvalidation_0-mlogloss:0.81517\n",
      "[265]\tvalidation_0-mlogloss:0.81467\n",
      "[266]\tvalidation_0-mlogloss:0.81414\n",
      "[267]\tvalidation_0-mlogloss:0.81476\n",
      "[268]\tvalidation_0-mlogloss:0.81569\n",
      "[269]\tvalidation_0-mlogloss:0.81504\n",
      "[270]\tvalidation_0-mlogloss:0.81417\n",
      "[271]\tvalidation_0-mlogloss:0.81371\n",
      "[272]\tvalidation_0-mlogloss:0.81336\n",
      "[273]\tvalidation_0-mlogloss:0.81272\n",
      "[274]\tvalidation_0-mlogloss:0.81214\n",
      "[275]\tvalidation_0-mlogloss:0.81160\n",
      "[276]\tvalidation_0-mlogloss:0.81176\n",
      "[277]\tvalidation_0-mlogloss:0.81078\n",
      "[278]\tvalidation_0-mlogloss:0.81051\n",
      "[279]\tvalidation_0-mlogloss:0.80989\n",
      "[280]\tvalidation_0-mlogloss:0.81000\n",
      "[281]\tvalidation_0-mlogloss:0.80923\n",
      "[282]\tvalidation_0-mlogloss:0.80839\n",
      "[283]\tvalidation_0-mlogloss:0.80803\n",
      "[284]\tvalidation_0-mlogloss:0.80713\n",
      "[285]\tvalidation_0-mlogloss:0.80689\n",
      "[286]\tvalidation_0-mlogloss:0.80650\n",
      "[287]\tvalidation_0-mlogloss:0.80596\n",
      "[288]\tvalidation_0-mlogloss:0.80550\n",
      "[289]\tvalidation_0-mlogloss:0.80511\n",
      "[290]\tvalidation_0-mlogloss:0.80487\n",
      "[291]\tvalidation_0-mlogloss:0.80428\n",
      "[292]\tvalidation_0-mlogloss:0.80412\n",
      "[293]\tvalidation_0-mlogloss:0.80430\n",
      "[294]\tvalidation_0-mlogloss:0.80369\n",
      "[295]\tvalidation_0-mlogloss:0.80331\n",
      "[296]\tvalidation_0-mlogloss:0.80327\n",
      "[297]\tvalidation_0-mlogloss:0.80299\n",
      "[298]\tvalidation_0-mlogloss:0.80270\n",
      "[299]\tvalidation_0-mlogloss:0.80240\n",
      "[300]\tvalidation_0-mlogloss:0.80171\n",
      "[301]\tvalidation_0-mlogloss:0.80173\n",
      "[302]\tvalidation_0-mlogloss:0.80106\n",
      "[303]\tvalidation_0-mlogloss:0.80099\n",
      "[304]\tvalidation_0-mlogloss:0.80110\n",
      "[305]\tvalidation_0-mlogloss:0.80080\n",
      "[306]\tvalidation_0-mlogloss:0.80088\n",
      "[307]\tvalidation_0-mlogloss:0.80053\n",
      "[308]\tvalidation_0-mlogloss:0.79987\n",
      "[309]\tvalidation_0-mlogloss:0.79978\n",
      "[310]\tvalidation_0-mlogloss:0.79918\n",
      "[311]\tvalidation_0-mlogloss:0.79860\n",
      "[312]\tvalidation_0-mlogloss:0.79863\n",
      "[313]\tvalidation_0-mlogloss:0.79888\n",
      "[314]\tvalidation_0-mlogloss:0.79871\n",
      "[315]\tvalidation_0-mlogloss:0.79855\n",
      "[316]\tvalidation_0-mlogloss:0.79785\n",
      "[317]\tvalidation_0-mlogloss:0.79737\n",
      "[318]\tvalidation_0-mlogloss:0.79708\n",
      "[319]\tvalidation_0-mlogloss:0.79676\n",
      "[320]\tvalidation_0-mlogloss:0.79619\n",
      "[321]\tvalidation_0-mlogloss:0.79630\n",
      "[322]\tvalidation_0-mlogloss:0.79539\n",
      "[323]\tvalidation_0-mlogloss:0.79499\n",
      "[324]\tvalidation_0-mlogloss:0.79499\n",
      "[325]\tvalidation_0-mlogloss:0.79465\n",
      "[326]\tvalidation_0-mlogloss:0.79440\n",
      "[327]\tvalidation_0-mlogloss:0.79407\n",
      "[328]\tvalidation_0-mlogloss:0.79425\n",
      "[329]\tvalidation_0-mlogloss:0.79419\n",
      "[330]\tvalidation_0-mlogloss:0.79432\n",
      "[331]\tvalidation_0-mlogloss:0.79388\n",
      "[332]\tvalidation_0-mlogloss:0.79374\n",
      "[333]\tvalidation_0-mlogloss:0.79382\n",
      "[334]\tvalidation_0-mlogloss:0.79325\n",
      "[335]\tvalidation_0-mlogloss:0.79238\n",
      "[336]\tvalidation_0-mlogloss:0.79255\n",
      "[337]\tvalidation_0-mlogloss:0.79250\n",
      "[338]\tvalidation_0-mlogloss:0.79277\n",
      "[339]\tvalidation_0-mlogloss:0.79247\n",
      "[340]\tvalidation_0-mlogloss:0.79173\n",
      "[341]\tvalidation_0-mlogloss:0.79158\n",
      "[342]\tvalidation_0-mlogloss:0.79171\n",
      "[343]\tvalidation_0-mlogloss:0.79128\n",
      "[344]\tvalidation_0-mlogloss:0.79073\n",
      "[345]\tvalidation_0-mlogloss:0.79024\n",
      "[346]\tvalidation_0-mlogloss:0.79041\n",
      "[347]\tvalidation_0-mlogloss:0.79088\n",
      "[348]\tvalidation_0-mlogloss:0.79044\n",
      "[349]\tvalidation_0-mlogloss:0.79036\n",
      "[350]\tvalidation_0-mlogloss:0.79006\n",
      "[351]\tvalidation_0-mlogloss:0.78966\n",
      "[352]\tvalidation_0-mlogloss:0.78960\n",
      "[353]\tvalidation_0-mlogloss:0.78932\n",
      "[354]\tvalidation_0-mlogloss:0.78911\n",
      "[355]\tvalidation_0-mlogloss:0.78888\n",
      "[356]\tvalidation_0-mlogloss:0.78890\n",
      "[357]\tvalidation_0-mlogloss:0.78895\n",
      "[358]\tvalidation_0-mlogloss:0.78889\n",
      "[359]\tvalidation_0-mlogloss:0.78876\n",
      "[360]\tvalidation_0-mlogloss:0.78824\n",
      "[361]\tvalidation_0-mlogloss:0.78849\n",
      "[362]\tvalidation_0-mlogloss:0.78822\n",
      "[363]\tvalidation_0-mlogloss:0.78826\n",
      "[364]\tvalidation_0-mlogloss:0.78744\n",
      "[365]\tvalidation_0-mlogloss:0.78739\n",
      "[366]\tvalidation_0-mlogloss:0.78687\n",
      "[367]\tvalidation_0-mlogloss:0.78684\n",
      "[368]\tvalidation_0-mlogloss:0.78636\n",
      "[369]\tvalidation_0-mlogloss:0.78648\n",
      "[370]\tvalidation_0-mlogloss:0.78581\n",
      "[371]\tvalidation_0-mlogloss:0.78616\n",
      "[372]\tvalidation_0-mlogloss:0.78648\n",
      "[373]\tvalidation_0-mlogloss:0.78627\n",
      "[374]\tvalidation_0-mlogloss:0.78572\n",
      "[375]\tvalidation_0-mlogloss:0.78568\n",
      "[376]\tvalidation_0-mlogloss:0.78577\n",
      "[377]\tvalidation_0-mlogloss:0.78544\n",
      "[378]\tvalidation_0-mlogloss:0.78580\n",
      "[379]\tvalidation_0-mlogloss:0.78643\n",
      "[380]\tvalidation_0-mlogloss:0.78622\n",
      "[381]\tvalidation_0-mlogloss:0.78595\n",
      "[382]\tvalidation_0-mlogloss:0.78600\n",
      "[383]\tvalidation_0-mlogloss:0.78628\n",
      "[384]\tvalidation_0-mlogloss:0.78621\n",
      "[385]\tvalidation_0-mlogloss:0.78609\n",
      "[386]\tvalidation_0-mlogloss:0.78566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.79      0.70      1499\n",
      "           1       0.81      0.50      0.62      1499\n",
      "           2       0.81      0.92      0.86      1499\n",
      "           3       0.87      0.86      0.86      1499\n",
      "           4       0.97      0.99      0.98      1499\n",
      "           5       0.99      0.98      0.98      1499\n",
      "           6       0.92      0.84      0.88      1499\n",
      "           7       0.90      0.50      0.65      1499\n",
      "           8       0.76      0.94      0.84      1499\n",
      "           9       0.99      0.91      0.95      1499\n",
      "          10       0.83      0.62      0.71      1499\n",
      "          11       0.95      0.98      0.96      1499\n",
      "          12       0.84      0.91      0.87      1499\n",
      "          13       0.77      0.59      0.67      1499\n",
      "          14       0.80      0.90      0.85      1499\n",
      "          15       0.84      0.93      0.88      1499\n",
      "          16       0.84      0.92      0.88      1499\n",
      "          17       0.78      0.94      0.85      1499\n",
      "          18       0.93      0.92      0.92      1499\n",
      "          19       0.58      0.74      0.65      1499\n",
      "          20       0.98      1.00      0.99      1499\n",
      "          21       0.82      0.82      0.82      1499\n",
      "          22       0.76      0.89      0.82      1499\n",
      "          23       0.96      0.99      0.98      1499\n",
      "          24       0.91      0.85      0.87      1499\n",
      "          25       0.87      0.58      0.70      1499\n",
      "          26       0.79      0.27      0.40      1499\n",
      "          27       0.66      0.54      0.60      1499\n",
      "          28       0.79      0.93      0.86      1499\n",
      "          29       0.93      0.93      0.93      1499\n",
      "          30       0.74      0.96      0.83      1499\n",
      "          31       0.71      0.92      0.80      1499\n",
      "          32       0.50      0.53      0.51      1499\n",
      "          33       0.68      0.78      0.73      1499\n",
      "          34       0.99      0.96      0.97      1499\n",
      "          35       0.73      0.96      0.83      1499\n",
      "          36       0.84      0.89      0.87      1499\n",
      "          37       0.75      0.97      0.85      1499\n",
      "          38       0.91      0.42      0.58      1499\n",
      "          39       0.88      0.77      0.82      1499\n",
      "          40       0.95      0.78      0.86      1499\n",
      "          41       0.88      0.80      0.84      1499\n",
      "          42       0.91      0.76      0.83      1499\n",
      "          43       0.81      0.90      0.85      1499\n",
      "          44       0.90      0.97      0.93      1499\n",
      "          45       0.73      0.73      0.73      1499\n",
      "          46       0.54      0.56      0.55      1499\n",
      "          47       0.89      0.59      0.71      1499\n",
      "          48       0.62      0.89      0.73      1499\n",
      "          49       0.72      0.78      0.75      1499\n",
      "\n",
      "    accuracy                           0.81     74950\n",
      "   macro avg       0.82      0.81      0.80     74950\n",
      "weighted avg       0.82      0.81      0.80     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1dad4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3361defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 19s 984us/step - loss: 0.8086 - val_loss: 2.7155\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 293s 16ms/step - loss: 0.2965 - val_loss: 2.6128\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 19s 1ms/step - loss: 0.2116 - val_loss: 2.5895\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 17s 896us/step - loss: 0.1673 - val_loss: 2.4293\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 17s 906us/step - loss: 0.1415 - val_loss: 2.2409\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 17s 902us/step - loss: 0.1226 - val_loss: 2.4476\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 17s 904us/step - loss: 0.1101 - val_loss: 2.2728\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 16s 841us/step - loss: 0.1007 - val_loss: 2.2738\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 14s 769us/step - loss: 0.0926 - val_loss: 2.2116\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 14s 773us/step - loss: 0.0872 - val_loss: 2.0292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x312387100>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 1s 309us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79      1499\n",
      "           1       0.62      0.39      0.47      1499\n",
      "           2       0.70      0.87      0.78      1499\n",
      "           3       0.88      0.82      0.85      1499\n",
      "           4       0.88      1.00      0.93      1499\n",
      "           5       0.91      0.97      0.94      1499\n",
      "           6       0.91      0.83      0.87      1499\n",
      "           7       0.81      0.49      0.62      1499\n",
      "           8       0.73      0.96      0.83      1499\n",
      "           9       0.96      0.92      0.94      1499\n",
      "          10       0.62      0.53      0.57      1499\n",
      "          11       0.87      0.94      0.90      1499\n",
      "          12       0.91      0.78      0.84      1499\n",
      "          13       0.67      0.57      0.62      1499\n",
      "          14       0.79      0.73      0.76      1499\n",
      "          15       0.86      0.88      0.87      1499\n",
      "          16       0.87      0.90      0.88      1499\n",
      "          17       0.80      0.84      0.82      1499\n",
      "          18       0.85      0.91      0.88      1499\n",
      "          19       0.44      0.76      0.56      1499\n",
      "          20       0.98      0.97      0.98      1499\n",
      "          21       0.65      0.84      0.73      1499\n",
      "          22       0.75      0.81      0.78      1499\n",
      "          23       0.99      0.99      0.99      1499\n",
      "          24       0.81      0.60      0.69      1499\n",
      "          25       0.60      0.28      0.38      1499\n",
      "          26       0.61      0.35      0.44      1499\n",
      "          27       0.57      0.61      0.59      1499\n",
      "          28       0.71      0.90      0.80      1499\n",
      "          29       0.90      0.75      0.82      1499\n",
      "          30       0.79      0.91      0.85      1499\n",
      "          31       0.78      0.86      0.82      1499\n",
      "          32       0.36      0.30      0.33      1499\n",
      "          33       0.66      0.74      0.70      1499\n",
      "          34       0.99      0.98      0.99      1499\n",
      "          35       0.86      0.85      0.85      1499\n",
      "          36       0.64      0.84      0.72      1499\n",
      "          37       0.69      0.95      0.80      1499\n",
      "          38       0.96      0.37      0.54      1499\n",
      "          39       0.71      0.75      0.73      1499\n",
      "          40       0.81      0.66      0.73      1499\n",
      "          41       0.65      0.63      0.64      1499\n",
      "          42       0.66      0.71      0.68      1499\n",
      "          43       0.73      0.83      0.78      1499\n",
      "          44       0.86      0.96      0.91      1499\n",
      "          45       0.75      0.64      0.69      1499\n",
      "          46       0.34      0.35      0.35      1499\n",
      "          47       0.85      0.48      0.62      1499\n",
      "          48       0.47      0.82      0.60      1499\n",
      "          49       0.71      0.58      0.64      1499\n",
      "\n",
      "    accuracy                           0.74     74950\n",
      "   macro avg       0.75      0.74      0.74     74950\n",
      "weighted avg       0.75      0.74      0.74     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0aebd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e4457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e48a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

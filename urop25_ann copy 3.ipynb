{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 25 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 05:56:49.459133: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 05:56:49.493096: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 05:56:49.493140: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 05:56:49.494259: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 05:56:49.500107: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 05:56:49.501110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 05:56:50.545681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081a3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.6.3 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.7.0)\n",
      "Requirement already satisfied: tqdm in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (4.64.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.7.0)\n",
      "Requirement already satisfied: decorator in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: packaging in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (22.0)\n",
      "Requirement already satisfied: jinja2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (3.8.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from jinja2->mne) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files4/S001R06.edf',\n",
       " 'files4/S002R06.edf',\n",
       " 'files4/S003R06.edf',\n",
       " 'files4/S004R06.edf',\n",
       " 'files4/S005R06.edf',\n",
       " 'files4/S006R06.edf',\n",
       " 'files4/S007R06.edf',\n",
       " 'files4/S008R06.edf',\n",
       " 'files4/S009R06.edf',\n",
       " 'files4/S010R06.edf',\n",
       " 'files4/S011R06.edf',\n",
       " 'files4/S012R06.edf',\n",
       " 'files4/S013R06.edf',\n",
       " 'files4/S014R06.edf',\n",
       " 'files4/S015R06.edf',\n",
       " 'files4/S016R06.edf',\n",
       " 'files4/S017R06.edf',\n",
       " 'files4/S018R06.edf',\n",
       " 'files4/S019R06.edf',\n",
       " 'files4/S020R06.edf',\n",
       " 'files4/S021R06.edf',\n",
       " 'files4/S022R06.edf',\n",
       " 'files4/S023R06.edf',\n",
       " 'files4/S024R06.edf',\n",
       " 'files4/S025R06.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files4/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 37475, 300000, 37475)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.8e-05 -3.7e-05 -1.0e-06 ... -2.0e-06 -6.0e-06  1.0e-05]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1        0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2        0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3        0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4        0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "299995   0.000003  -0.000003   0.000012  -0.000006  -0.000007   0.000010   \n",
       "299996   0.000019   0.000000   0.000009  -0.000010  -0.000016  -0.000002   \n",
       "299997   0.000008  -0.000007   0.000001  -0.000019  -0.000026  -0.000012   \n",
       "299998   0.000000  -0.000005   0.000009  -0.000011  -0.000015  -0.000002   \n",
       "299999   0.000005  -0.000007   0.000014  -0.000003  -0.000008   0.000005   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1        0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2        0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3        0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4        0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "299995  -0.000013  -0.000012  -0.000010  -0.000003  ...   -0.000003   \n",
       "299996  -0.000020  -0.000016  -0.000014  -0.000010  ...   -0.000010   \n",
       "299997  -0.000029  -0.000014  -0.000017  -0.000012  ...   -0.000001   \n",
       "299998  -0.000019  -0.000013  -0.000013  -0.000002  ...   -0.000001   \n",
       "299999  -0.000017  -0.000022  -0.000018  -0.000001  ...   -0.000009   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1         0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2         0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3         0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4         0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "299995   -0.000005   -0.000009   -0.000011   -0.000017   -0.000011   \n",
       "299996    0.000001   -0.000007   -0.000012   -0.000019   -0.000012   \n",
       "299997   -0.000002   -0.000002    0.000000   -0.000006   -0.000001   \n",
       "299998   -0.000006   -0.000002    0.000005    0.000006    0.000018   \n",
       "299999   -0.000009   -0.000008    0.000000    0.000000    0.000006   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000029   -0.000004    0.000059   -0.000092  \n",
       "1         0.000032    0.000004    0.000069   -0.000093  \n",
       "2         0.000025   -0.000003    0.000065   -0.000102  \n",
       "3         0.000013   -0.000016    0.000050   -0.000107  \n",
       "4         0.000015   -0.000013    0.000044   -0.000094  \n",
       "...            ...         ...         ...         ...  \n",
       "299995   -0.000010   -0.000021    0.000000   -0.000011  \n",
       "299996   -0.000007   -0.000018   -0.000002   -0.000007  \n",
       "299997   -0.000002   -0.000012    0.000007   -0.000004  \n",
       "299998   -0.000004   -0.000007    0.000021   -0.000006  \n",
       "299999   -0.000007   -0.000009    0.000015   -0.000001  \n",
       "\n",
       "[300000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16064/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_16064/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_16064/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1        0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2        0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3        0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4        0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "299995   0.000003  -0.000003   0.000012  -0.000006  -0.000007   0.000010   \n",
       "299996   0.000019   0.000000   0.000009  -0.000010  -0.000016  -0.000002   \n",
       "299997   0.000008  -0.000007   0.000001  -0.000019  -0.000026  -0.000012   \n",
       "299998   0.000000  -0.000005   0.000009  -0.000011  -0.000015  -0.000002   \n",
       "299999   0.000005  -0.000007   0.000014  -0.000003  -0.000008   0.000005   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1        0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2        0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3        0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4        0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "299995  -0.000013  -0.000012  -0.000010  -0.000003  ...   -0.000003   \n",
       "299996  -0.000020  -0.000016  -0.000014  -0.000010  ...   -0.000010   \n",
       "299997  -0.000029  -0.000014  -0.000017  -0.000012  ...   -0.000001   \n",
       "299998  -0.000019  -0.000013  -0.000013  -0.000002  ...   -0.000001   \n",
       "299999  -0.000017  -0.000022  -0.000018  -0.000001  ...   -0.000009   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1         0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2         0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3         0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4         0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "299995   -0.000005   -0.000009   -0.000011   -0.000017   -0.000011   \n",
       "299996    0.000001   -0.000007   -0.000012   -0.000019   -0.000012   \n",
       "299997   -0.000002   -0.000002    0.000000   -0.000006   -0.000001   \n",
       "299998   -0.000006   -0.000002    0.000005    0.000006    0.000018   \n",
       "299999   -0.000009   -0.000008    0.000000    0.000000    0.000006   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000029   -0.000004    0.000059   -0.000092  \n",
       "1         0.000032    0.000004    0.000069   -0.000093  \n",
       "2         0.000025   -0.000003    0.000065   -0.000102  \n",
       "3         0.000013   -0.000016    0.000050   -0.000107  \n",
       "4         0.000015   -0.000013    0.000044   -0.000094  \n",
       "...            ...         ...         ...         ...  \n",
       "299995   -0.000010   -0.000021    0.000000   -0.000011  \n",
       "299996   -0.000007   -0.000018   -0.000002   -0.000007  \n",
       "299997   -0.000002   -0.000012    0.000007   -0.000004  \n",
       "299998   -0.000004   -0.000007    0.000021   -0.000006  \n",
       "299999   -0.000007   -0.000009    0.000015   -0.000001  \n",
       "\n",
       "[300000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.93748\n",
      "[1]\tvalidation_0-mlogloss:2.82408\n",
      "[2]\tvalidation_0-mlogloss:2.74997\n",
      "[3]\tvalidation_0-mlogloss:2.69232\n",
      "[4]\tvalidation_0-mlogloss:2.64341\n",
      "[5]\tvalidation_0-mlogloss:2.60324\n",
      "[6]\tvalidation_0-mlogloss:2.57157\n",
      "[7]\tvalidation_0-mlogloss:2.54279\n",
      "[8]\tvalidation_0-mlogloss:2.51731\n",
      "[9]\tvalidation_0-mlogloss:2.49742\n",
      "[10]\tvalidation_0-mlogloss:2.47643\n",
      "[11]\tvalidation_0-mlogloss:2.46454\n",
      "[12]\tvalidation_0-mlogloss:2.45105\n",
      "[13]\tvalidation_0-mlogloss:2.43652\n",
      "[14]\tvalidation_0-mlogloss:2.42776\n",
      "[15]\tvalidation_0-mlogloss:2.41585\n",
      "[16]\tvalidation_0-mlogloss:2.40826\n",
      "[17]\tvalidation_0-mlogloss:2.39969\n",
      "[18]\tvalidation_0-mlogloss:2.39324\n",
      "[19]\tvalidation_0-mlogloss:2.38785\n",
      "[20]\tvalidation_0-mlogloss:2.38183\n",
      "[21]\tvalidation_0-mlogloss:2.37495\n",
      "[22]\tvalidation_0-mlogloss:2.36783\n",
      "[23]\tvalidation_0-mlogloss:2.36093\n",
      "[24]\tvalidation_0-mlogloss:2.35553\n",
      "[25]\tvalidation_0-mlogloss:2.35118\n",
      "[26]\tvalidation_0-mlogloss:2.34412\n",
      "[27]\tvalidation_0-mlogloss:2.33641\n",
      "[28]\tvalidation_0-mlogloss:2.33081\n",
      "[29]\tvalidation_0-mlogloss:2.32526\n",
      "[30]\tvalidation_0-mlogloss:2.31635\n",
      "[31]\tvalidation_0-mlogloss:2.30997\n",
      "[32]\tvalidation_0-mlogloss:2.30411\n",
      "[33]\tvalidation_0-mlogloss:2.30301\n",
      "[34]\tvalidation_0-mlogloss:2.29738\n",
      "[35]\tvalidation_0-mlogloss:2.29196\n",
      "[36]\tvalidation_0-mlogloss:2.28805\n",
      "[37]\tvalidation_0-mlogloss:2.28445\n",
      "[38]\tvalidation_0-mlogloss:2.28168\n",
      "[39]\tvalidation_0-mlogloss:2.27950\n",
      "[40]\tvalidation_0-mlogloss:2.27577\n",
      "[41]\tvalidation_0-mlogloss:2.27508\n",
      "[42]\tvalidation_0-mlogloss:2.26950\n",
      "[43]\tvalidation_0-mlogloss:2.26453\n",
      "[44]\tvalidation_0-mlogloss:2.26242\n",
      "[45]\tvalidation_0-mlogloss:2.25926\n",
      "[46]\tvalidation_0-mlogloss:2.25516\n",
      "[47]\tvalidation_0-mlogloss:2.25187\n",
      "[48]\tvalidation_0-mlogloss:2.25040\n",
      "[49]\tvalidation_0-mlogloss:2.24771\n",
      "[50]\tvalidation_0-mlogloss:2.24371\n",
      "[51]\tvalidation_0-mlogloss:2.23894\n",
      "[52]\tvalidation_0-mlogloss:2.23756\n",
      "[53]\tvalidation_0-mlogloss:2.23754\n",
      "[54]\tvalidation_0-mlogloss:2.23443\n",
      "[55]\tvalidation_0-mlogloss:2.23160\n",
      "[56]\tvalidation_0-mlogloss:2.23080\n",
      "[57]\tvalidation_0-mlogloss:2.22813\n",
      "[58]\tvalidation_0-mlogloss:2.22704\n",
      "[59]\tvalidation_0-mlogloss:2.22471\n",
      "[60]\tvalidation_0-mlogloss:2.22269\n",
      "[61]\tvalidation_0-mlogloss:2.21850\n",
      "[62]\tvalidation_0-mlogloss:2.21706\n",
      "[63]\tvalidation_0-mlogloss:2.21770\n",
      "[64]\tvalidation_0-mlogloss:2.21560\n",
      "[65]\tvalidation_0-mlogloss:2.21406\n",
      "[66]\tvalidation_0-mlogloss:2.21168\n",
      "[67]\tvalidation_0-mlogloss:2.21100\n",
      "[68]\tvalidation_0-mlogloss:2.20870\n",
      "[69]\tvalidation_0-mlogloss:2.20803\n",
      "[70]\tvalidation_0-mlogloss:2.20856\n",
      "[71]\tvalidation_0-mlogloss:2.20706\n",
      "[72]\tvalidation_0-mlogloss:2.20557\n",
      "[73]\tvalidation_0-mlogloss:2.20528\n",
      "[74]\tvalidation_0-mlogloss:2.20429\n",
      "[75]\tvalidation_0-mlogloss:2.20124\n",
      "[76]\tvalidation_0-mlogloss:2.20059\n",
      "[77]\tvalidation_0-mlogloss:2.19787\n",
      "[78]\tvalidation_0-mlogloss:2.19589\n",
      "[79]\tvalidation_0-mlogloss:2.19586\n",
      "[80]\tvalidation_0-mlogloss:2.19528\n",
      "[81]\tvalidation_0-mlogloss:2.19295\n",
      "[82]\tvalidation_0-mlogloss:2.19044\n",
      "[83]\tvalidation_0-mlogloss:2.19024\n",
      "[84]\tvalidation_0-mlogloss:2.18914\n",
      "[85]\tvalidation_0-mlogloss:2.18714\n",
      "[86]\tvalidation_0-mlogloss:2.18571\n",
      "[87]\tvalidation_0-mlogloss:2.18260\n",
      "[88]\tvalidation_0-mlogloss:2.18202\n",
      "[89]\tvalidation_0-mlogloss:2.18005\n",
      "[90]\tvalidation_0-mlogloss:2.17964\n",
      "[91]\tvalidation_0-mlogloss:2.17919\n",
      "[92]\tvalidation_0-mlogloss:2.17861\n",
      "[93]\tvalidation_0-mlogloss:2.17715\n",
      "[94]\tvalidation_0-mlogloss:2.17752\n",
      "[95]\tvalidation_0-mlogloss:2.17792\n",
      "[96]\tvalidation_0-mlogloss:2.17850\n",
      "[97]\tvalidation_0-mlogloss:2.17891\n",
      "[98]\tvalidation_0-mlogloss:2.17788\n",
      "[99]\tvalidation_0-mlogloss:2.17879\n",
      "[100]\tvalidation_0-mlogloss:2.17938\n",
      "[101]\tvalidation_0-mlogloss:2.17906\n",
      "[102]\tvalidation_0-mlogloss:2.17682\n",
      "[103]\tvalidation_0-mlogloss:2.17779\n",
      "[104]\tvalidation_0-mlogloss:2.17724\n",
      "[105]\tvalidation_0-mlogloss:2.17897\n",
      "[106]\tvalidation_0-mlogloss:2.17784\n",
      "[107]\tvalidation_0-mlogloss:2.17864\n",
      "[108]\tvalidation_0-mlogloss:2.17918\n",
      "[109]\tvalidation_0-mlogloss:2.17901\n",
      "[110]\tvalidation_0-mlogloss:2.17893\n",
      "[111]\tvalidation_0-mlogloss:2.17807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.23      0.23      1499\n",
      "           1       0.11      0.16      0.13      1499\n",
      "           2       0.46      0.29      0.36      1499\n",
      "           3       0.62      0.51      0.56      1499\n",
      "           4       0.80      0.72      0.76      1499\n",
      "           5       0.51      0.60      0.55      1499\n",
      "           6       0.41      0.57      0.48      1499\n",
      "           7       0.48      0.40      0.44      1499\n",
      "           8       0.62      0.77      0.68      1499\n",
      "           9       0.71      0.46      0.56      1499\n",
      "          10       0.34      0.30      0.32      1499\n",
      "          11       0.38      0.24      0.30      1499\n",
      "          12       0.27      0.19      0.22      1499\n",
      "          13       0.15      0.15      0.15      1499\n",
      "          14       0.18      0.17      0.18      1499\n",
      "          15       0.46      0.43      0.44      1499\n",
      "          16       0.48      0.28      0.35      1499\n",
      "          17       0.45      0.88      0.60      1499\n",
      "          18       0.20      0.14      0.16      1499\n",
      "          19       0.26      0.38      0.31      1499\n",
      "          20       0.73      0.64      0.69      1499\n",
      "          21       0.54      0.51      0.52      1499\n",
      "          22       0.30      0.25      0.28      1499\n",
      "          23       0.66      0.77      0.71      1499\n",
      "          24       0.35      0.45      0.39      1499\n",
      "\n",
      "    accuracy                           0.42     37475\n",
      "   macro avg       0.43      0.42      0.41     37475\n",
      "weighted avg       0.43      0.42      0.41     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b271d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(25, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9375/9375 [==============================] - 6s 665us/step - loss: 1.7511 - val_loss: 2.6036\n",
      "Epoch 2/10\n",
      "9375/9375 [==============================] - 6s 661us/step - loss: 1.2737 - val_loss: 2.5230\n",
      "Epoch 3/10\n",
      "9375/9375 [==============================] - 6s 657us/step - loss: 1.1525 - val_loss: 2.3763\n",
      "Epoch 4/10\n",
      "9375/9375 [==============================] - 6s 673us/step - loss: 1.1001 - val_loss: 2.5393\n",
      "Epoch 5/10\n",
      "9375/9375 [==============================] - 6s 685us/step - loss: 1.0700 - val_loss: 2.5290\n",
      "Epoch 6/10\n",
      "9375/9375 [==============================] - 6s 675us/step - loss: 1.0468 - val_loss: 2.5569\n",
      "Epoch 7/10\n",
      "9375/9375 [==============================] - 6s 672us/step - loss: 1.0314 - val_loss: 2.6655\n",
      "Epoch 8/10\n",
      "9375/9375 [==============================] - 6s 679us/step - loss: 1.0170 - val_loss: 2.6334\n",
      "Epoch 9/10\n",
      "9375/9375 [==============================] - 6s 667us/step - loss: 1.0072 - val_loss: 2.8907\n",
      "Epoch 10/10\n",
      "9375/9375 [==============================] - 6s 684us/step - loss: 0.9962 - val_loss: 2.6263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2aa03e710>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172/1172 [==============================] - 0s 290us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.28      0.28      1499\n",
      "           1       0.15      0.19      0.17      1499\n",
      "           2       0.47      0.34      0.39      1499\n",
      "           3       0.67      0.72      0.70      1499\n",
      "           4       0.81      0.73      0.77      1499\n",
      "           5       0.70      0.67      0.68      1499\n",
      "           6       0.60      0.61      0.60      1499\n",
      "           7       0.55      0.72      0.62      1499\n",
      "           8       0.70      0.71      0.71      1499\n",
      "           9       0.61      0.38      0.47      1499\n",
      "          10       0.51      0.27      0.35      1499\n",
      "          11       0.34      0.47      0.40      1499\n",
      "          12       0.31      0.24      0.27      1499\n",
      "          13       0.21      0.22      0.21      1499\n",
      "          14       0.21      0.30      0.25      1499\n",
      "          15       0.51      0.54      0.52      1499\n",
      "          16       0.43      0.48      0.45      1499\n",
      "          17       0.70      0.96      0.81      1499\n",
      "          18       0.25      0.20      0.22      1499\n",
      "          19       0.30      0.33      0.31      1499\n",
      "          20       0.83      0.45      0.58      1499\n",
      "          21       0.54      0.67      0.60      1499\n",
      "          22       0.29      0.24      0.26      1499\n",
      "          23       0.64      0.76      0.69      1499\n",
      "          24       0.55      0.35      0.43      1499\n",
      "\n",
      "    accuracy                           0.47     37475\n",
      "   macro avg       0.49      0.47      0.47     37475\n",
      "weighted avg       0.49      0.47      0.47     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16064/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_16064/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_16064/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1        0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2        0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3        0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4        0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "299995   0.000003  -0.000003   0.000012  -0.000006  -0.000007   0.000010   \n",
       "299996   0.000019   0.000000   0.000009  -0.000010  -0.000016  -0.000002   \n",
       "299997   0.000008  -0.000007   0.000001  -0.000019  -0.000026  -0.000012   \n",
       "299998   0.000000  -0.000005   0.000009  -0.000011  -0.000015  -0.000002   \n",
       "299999   0.000005  -0.000007   0.000014  -0.000003  -0.000008   0.000005   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1        0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2        0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3        0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4        0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "299995  -0.000013  -0.000012  -0.000010  -0.000003  ...   -0.000003   \n",
       "299996  -0.000020  -0.000016  -0.000014  -0.000010  ...   -0.000010   \n",
       "299997  -0.000029  -0.000014  -0.000017  -0.000012  ...   -0.000001   \n",
       "299998  -0.000019  -0.000013  -0.000013  -0.000002  ...   -0.000001   \n",
       "299999  -0.000017  -0.000022  -0.000018  -0.000001  ...   -0.000009   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1         0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2         0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3         0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4         0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "299995   -0.000005   -0.000009   -0.000011   -0.000017   -0.000011   \n",
       "299996    0.000001   -0.000007   -0.000012   -0.000019   -0.000012   \n",
       "299997   -0.000002   -0.000002    0.000000   -0.000006   -0.000001   \n",
       "299998   -0.000006   -0.000002    0.000005    0.000006    0.000018   \n",
       "299999   -0.000009   -0.000008    0.000000    0.000000    0.000006   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000029   -0.000004    0.000059   -0.000092  \n",
       "1         0.000032    0.000004    0.000069   -0.000093  \n",
       "2         0.000025   -0.000003    0.000065   -0.000102  \n",
       "3         0.000013   -0.000016    0.000050   -0.000107  \n",
       "4         0.000015   -0.000013    0.000044   -0.000094  \n",
       "...            ...         ...         ...         ...  \n",
       "299995   -0.000010   -0.000021    0.000000   -0.000011  \n",
       "299996   -0.000007   -0.000018   -0.000002   -0.000007  \n",
       "299997   -0.000002   -0.000012    0.000007   -0.000004  \n",
       "299998   -0.000004   -0.000007    0.000021   -0.000006  \n",
       "299999   -0.000007   -0.000009    0.000015   -0.000001  \n",
       "\n",
       "[300000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.88066\n",
      "[1]\tvalidation_0-mlogloss:2.74734\n",
      "[2]\tvalidation_0-mlogloss:2.65738\n",
      "[3]\tvalidation_0-mlogloss:2.58701\n",
      "[4]\tvalidation_0-mlogloss:2.52411\n",
      "[5]\tvalidation_0-mlogloss:2.46530\n",
      "[6]\tvalidation_0-mlogloss:2.42389\n",
      "[7]\tvalidation_0-mlogloss:2.38614\n",
      "[8]\tvalidation_0-mlogloss:2.35516\n",
      "[9]\tvalidation_0-mlogloss:2.32180\n",
      "[10]\tvalidation_0-mlogloss:2.30115\n",
      "[11]\tvalidation_0-mlogloss:2.28013\n",
      "[12]\tvalidation_0-mlogloss:2.25919\n",
      "[13]\tvalidation_0-mlogloss:2.24082\n",
      "[14]\tvalidation_0-mlogloss:2.22172\n",
      "[15]\tvalidation_0-mlogloss:2.20852\n",
      "[16]\tvalidation_0-mlogloss:2.18925\n",
      "[17]\tvalidation_0-mlogloss:2.17435\n",
      "[18]\tvalidation_0-mlogloss:2.16095\n",
      "[19]\tvalidation_0-mlogloss:2.15082\n",
      "[20]\tvalidation_0-mlogloss:2.13862\n",
      "[21]\tvalidation_0-mlogloss:2.12629\n",
      "[22]\tvalidation_0-mlogloss:2.11142\n",
      "[23]\tvalidation_0-mlogloss:2.10274\n",
      "[24]\tvalidation_0-mlogloss:2.09004\n",
      "[25]\tvalidation_0-mlogloss:2.08108\n",
      "[26]\tvalidation_0-mlogloss:2.07538\n",
      "[27]\tvalidation_0-mlogloss:2.06707\n",
      "[28]\tvalidation_0-mlogloss:2.05980\n",
      "[29]\tvalidation_0-mlogloss:2.04658\n",
      "[30]\tvalidation_0-mlogloss:2.03596\n",
      "[31]\tvalidation_0-mlogloss:2.02884\n",
      "[32]\tvalidation_0-mlogloss:2.02141\n",
      "[33]\tvalidation_0-mlogloss:2.00954\n",
      "[34]\tvalidation_0-mlogloss:1.99957\n",
      "[35]\tvalidation_0-mlogloss:1.99060\n",
      "[36]\tvalidation_0-mlogloss:1.98160\n",
      "[37]\tvalidation_0-mlogloss:1.97537\n",
      "[38]\tvalidation_0-mlogloss:1.97100\n",
      "[39]\tvalidation_0-mlogloss:1.96228\n",
      "[40]\tvalidation_0-mlogloss:1.95725\n",
      "[41]\tvalidation_0-mlogloss:1.95003\n",
      "[42]\tvalidation_0-mlogloss:1.94196\n",
      "[43]\tvalidation_0-mlogloss:1.93431\n",
      "[44]\tvalidation_0-mlogloss:1.92833\n",
      "[45]\tvalidation_0-mlogloss:1.92235\n",
      "[46]\tvalidation_0-mlogloss:1.91636\n",
      "[47]\tvalidation_0-mlogloss:1.91327\n",
      "[48]\tvalidation_0-mlogloss:1.91039\n",
      "[49]\tvalidation_0-mlogloss:1.90486\n",
      "[50]\tvalidation_0-mlogloss:1.89922\n",
      "[51]\tvalidation_0-mlogloss:1.89417\n",
      "[52]\tvalidation_0-mlogloss:1.88963\n",
      "[53]\tvalidation_0-mlogloss:1.88558\n",
      "[54]\tvalidation_0-mlogloss:1.88199\n",
      "[55]\tvalidation_0-mlogloss:1.87773\n",
      "[56]\tvalidation_0-mlogloss:1.87704\n",
      "[57]\tvalidation_0-mlogloss:1.87186\n",
      "[58]\tvalidation_0-mlogloss:1.86879\n",
      "[59]\tvalidation_0-mlogloss:1.86433\n",
      "[60]\tvalidation_0-mlogloss:1.86185\n",
      "[61]\tvalidation_0-mlogloss:1.85704\n",
      "[62]\tvalidation_0-mlogloss:1.85260\n",
      "[63]\tvalidation_0-mlogloss:1.85085\n",
      "[64]\tvalidation_0-mlogloss:1.84801\n",
      "[65]\tvalidation_0-mlogloss:1.84636\n",
      "[66]\tvalidation_0-mlogloss:1.84000\n",
      "[67]\tvalidation_0-mlogloss:1.83667\n",
      "[68]\tvalidation_0-mlogloss:1.83236\n",
      "[69]\tvalidation_0-mlogloss:1.82899\n",
      "[70]\tvalidation_0-mlogloss:1.82596\n",
      "[71]\tvalidation_0-mlogloss:1.82339\n",
      "[72]\tvalidation_0-mlogloss:1.81924\n",
      "[73]\tvalidation_0-mlogloss:1.81711\n",
      "[74]\tvalidation_0-mlogloss:1.81590\n",
      "[75]\tvalidation_0-mlogloss:1.81285\n",
      "[76]\tvalidation_0-mlogloss:1.81145\n",
      "[77]\tvalidation_0-mlogloss:1.80749\n",
      "[78]\tvalidation_0-mlogloss:1.80633\n",
      "[79]\tvalidation_0-mlogloss:1.80357\n",
      "[80]\tvalidation_0-mlogloss:1.79939\n",
      "[81]\tvalidation_0-mlogloss:1.79647\n",
      "[82]\tvalidation_0-mlogloss:1.79351\n",
      "[83]\tvalidation_0-mlogloss:1.79087\n",
      "[84]\tvalidation_0-mlogloss:1.78765\n",
      "[85]\tvalidation_0-mlogloss:1.78586\n",
      "[86]\tvalidation_0-mlogloss:1.78224\n",
      "[87]\tvalidation_0-mlogloss:1.78042\n",
      "[88]\tvalidation_0-mlogloss:1.77858\n",
      "[89]\tvalidation_0-mlogloss:1.77605\n",
      "[90]\tvalidation_0-mlogloss:1.77379\n",
      "[91]\tvalidation_0-mlogloss:1.77389\n",
      "[92]\tvalidation_0-mlogloss:1.77087\n",
      "[93]\tvalidation_0-mlogloss:1.76815\n",
      "[94]\tvalidation_0-mlogloss:1.76685\n",
      "[95]\tvalidation_0-mlogloss:1.76419\n",
      "[96]\tvalidation_0-mlogloss:1.76172\n",
      "[97]\tvalidation_0-mlogloss:1.75899\n",
      "[98]\tvalidation_0-mlogloss:1.75766\n",
      "[99]\tvalidation_0-mlogloss:1.75630\n",
      "[100]\tvalidation_0-mlogloss:1.75504\n",
      "[101]\tvalidation_0-mlogloss:1.75370\n",
      "[102]\tvalidation_0-mlogloss:1.75282\n",
      "[103]\tvalidation_0-mlogloss:1.75211\n",
      "[104]\tvalidation_0-mlogloss:1.75041\n",
      "[105]\tvalidation_0-mlogloss:1.74945\n",
      "[106]\tvalidation_0-mlogloss:1.75063\n",
      "[107]\tvalidation_0-mlogloss:1.74908\n",
      "[108]\tvalidation_0-mlogloss:1.74694\n",
      "[109]\tvalidation_0-mlogloss:1.74521\n",
      "[110]\tvalidation_0-mlogloss:1.74362\n",
      "[111]\tvalidation_0-mlogloss:1.74270\n",
      "[112]\tvalidation_0-mlogloss:1.74142\n",
      "[113]\tvalidation_0-mlogloss:1.74127\n",
      "[114]\tvalidation_0-mlogloss:1.74211\n",
      "[115]\tvalidation_0-mlogloss:1.74141\n",
      "[116]\tvalidation_0-mlogloss:1.74156\n",
      "[117]\tvalidation_0-mlogloss:1.74282\n",
      "[118]\tvalidation_0-mlogloss:1.74301\n",
      "[119]\tvalidation_0-mlogloss:1.74416\n",
      "[120]\tvalidation_0-mlogloss:1.74419\n",
      "[121]\tvalidation_0-mlogloss:1.74327\n",
      "[122]\tvalidation_0-mlogloss:1.74247\n",
      "[123]\tvalidation_0-mlogloss:1.74190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.39      0.37      1499\n",
      "           1       0.13      0.19      0.15      1499\n",
      "           2       0.51      0.34      0.41      1499\n",
      "           3       0.63      0.43      0.51      1499\n",
      "           4       0.90      0.79      0.84      1499\n",
      "           5       0.58      0.65      0.61      1499\n",
      "           6       0.65      0.54      0.59      1499\n",
      "           7       0.46      0.49      0.48      1499\n",
      "           8       0.60      0.84      0.70      1499\n",
      "           9       0.94      0.70      0.80      1499\n",
      "          10       0.30      0.24      0.27      1499\n",
      "          11       0.48      0.46      0.47      1499\n",
      "          12       0.36      0.54      0.44      1499\n",
      "          13       0.21      0.24      0.22      1499\n",
      "          14       0.29      0.27      0.28      1499\n",
      "          15       0.61      0.40      0.49      1499\n",
      "          16       0.71      0.24      0.36      1499\n",
      "          17       0.65      0.75      0.70      1499\n",
      "          18       0.32      0.49      0.39      1499\n",
      "          19       0.41      0.62      0.49      1499\n",
      "          20       0.84      0.83      0.83      1499\n",
      "          21       0.56      0.45      0.49      1499\n",
      "          22       0.39      0.32      0.35      1499\n",
      "          23       0.80      0.82      0.81      1499\n",
      "          24       0.56      0.47      0.51      1499\n",
      "\n",
      "    accuracy                           0.50     37475\n",
      "   macro avg       0.53      0.50      0.50     37475\n",
      "weighted avg       0.53      0.50      0.50     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7844baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(25, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7980b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9375/9375 [==============================] - 7s 701us/step - loss: 1.2932 - val_loss: 2.2410\n",
      "Epoch 2/10\n",
      "9375/9375 [==============================] - 6s 677us/step - loss: 0.7348 - val_loss: 2.4089\n",
      "Epoch 3/10\n",
      "9375/9375 [==============================] - 6s 660us/step - loss: 0.5810 - val_loss: 2.6637\n",
      "Epoch 4/10\n",
      "9375/9375 [==============================] - 6s 651us/step - loss: 0.5159 - val_loss: 2.6524\n",
      "Epoch 5/10\n",
      "9375/9375 [==============================] - 6s 692us/step - loss: 0.4799 - val_loss: 2.7444\n",
      "Epoch 6/10\n",
      "9375/9375 [==============================] - 6s 691us/step - loss: 0.4568 - val_loss: 2.7384\n",
      "Epoch 7/10\n",
      "9375/9375 [==============================] - 6s 682us/step - loss: 0.4393 - val_loss: 3.0796\n",
      "Epoch 8/10\n",
      "9375/9375 [==============================] - 6s 693us/step - loss: 0.4246 - val_loss: 2.8056\n",
      "Epoch 9/10\n",
      "9375/9375 [==============================] - 6s 684us/step - loss: 0.4135 - val_loss: 2.8785\n",
      "Epoch 10/10\n",
      "9375/9375 [==============================] - 7s 697us/step - loss: 0.4017 - val_loss: 2.7590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x280f8e1a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172/1172 [==============================] - 0s 285us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.41      0.44      1499\n",
      "           1       0.16      0.23      0.19      1499\n",
      "           2       0.53      0.50      0.52      1499\n",
      "           3       0.73      0.45      0.56      1499\n",
      "           4       0.91      0.81      0.86      1499\n",
      "           5       0.78      0.73      0.75      1499\n",
      "           6       0.89      0.41      0.57      1499\n",
      "           7       0.68      0.59      0.63      1499\n",
      "           8       0.59      0.79      0.68      1499\n",
      "           9       0.79      0.62      0.69      1499\n",
      "          10       0.56      0.11      0.19      1499\n",
      "          11       0.70      0.55      0.62      1499\n",
      "          12       0.43      0.60      0.50      1499\n",
      "          13       0.24      0.42      0.31      1499\n",
      "          14       0.26      0.38      0.31      1499\n",
      "          15       0.76      0.39      0.51      1499\n",
      "          16       0.57      0.42      0.49      1499\n",
      "          17       0.91      0.76      0.83      1499\n",
      "          18       0.38      0.51      0.43      1499\n",
      "          19       0.49      0.50      0.49      1499\n",
      "          20       0.85      0.73      0.79      1499\n",
      "          21       0.54      0.62      0.58      1499\n",
      "          22       0.28      0.58      0.38      1499\n",
      "          23       0.79      0.85      0.82      1499\n",
      "          24       0.59      0.44      0.50      1499\n",
      "\n",
      "    accuracy                           0.54     37475\n",
      "   macro avg       0.60      0.54      0.54     37475\n",
      "weighted avg       0.60      0.54      0.54     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16064/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_16064/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_16064/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.66162\n",
      "[1]\tvalidation_0-mlogloss:2.46428\n",
      "[2]\tvalidation_0-mlogloss:2.34525\n",
      "[3]\tvalidation_0-mlogloss:2.24265\n",
      "[4]\tvalidation_0-mlogloss:2.15801\n",
      "[5]\tvalidation_0-mlogloss:2.08971\n",
      "[6]\tvalidation_0-mlogloss:2.03378\n",
      "[7]\tvalidation_0-mlogloss:1.98441\n",
      "[8]\tvalidation_0-mlogloss:1.93484\n",
      "[9]\tvalidation_0-mlogloss:1.89265\n",
      "[10]\tvalidation_0-mlogloss:1.85630\n",
      "[11]\tvalidation_0-mlogloss:1.82556\n",
      "[12]\tvalidation_0-mlogloss:1.79674\n",
      "[13]\tvalidation_0-mlogloss:1.76963\n",
      "[14]\tvalidation_0-mlogloss:1.74178\n",
      "[15]\tvalidation_0-mlogloss:1.71582\n",
      "[16]\tvalidation_0-mlogloss:1.69855\n",
      "[17]\tvalidation_0-mlogloss:1.67495\n",
      "[18]\tvalidation_0-mlogloss:1.64935\n",
      "[19]\tvalidation_0-mlogloss:1.63089\n",
      "[20]\tvalidation_0-mlogloss:1.61254\n",
      "[21]\tvalidation_0-mlogloss:1.59634\n",
      "[22]\tvalidation_0-mlogloss:1.58013\n",
      "[23]\tvalidation_0-mlogloss:1.56276\n",
      "[24]\tvalidation_0-mlogloss:1.54785\n",
      "[25]\tvalidation_0-mlogloss:1.52682\n",
      "[26]\tvalidation_0-mlogloss:1.51566\n",
      "[27]\tvalidation_0-mlogloss:1.50036\n",
      "[28]\tvalidation_0-mlogloss:1.48580\n",
      "[29]\tvalidation_0-mlogloss:1.47100\n",
      "[30]\tvalidation_0-mlogloss:1.45805\n",
      "[31]\tvalidation_0-mlogloss:1.44827\n",
      "[32]\tvalidation_0-mlogloss:1.43710\n",
      "[33]\tvalidation_0-mlogloss:1.42277\n",
      "[34]\tvalidation_0-mlogloss:1.41447\n",
      "[35]\tvalidation_0-mlogloss:1.40536\n",
      "[36]\tvalidation_0-mlogloss:1.39517\n",
      "[37]\tvalidation_0-mlogloss:1.38404\n",
      "[38]\tvalidation_0-mlogloss:1.37154\n",
      "[39]\tvalidation_0-mlogloss:1.36399\n",
      "[40]\tvalidation_0-mlogloss:1.35471\n",
      "[41]\tvalidation_0-mlogloss:1.34967\n",
      "[42]\tvalidation_0-mlogloss:1.34087\n",
      "[43]\tvalidation_0-mlogloss:1.33239\n",
      "[44]\tvalidation_0-mlogloss:1.32205\n",
      "[45]\tvalidation_0-mlogloss:1.31524\n",
      "[46]\tvalidation_0-mlogloss:1.30890\n",
      "[47]\tvalidation_0-mlogloss:1.30033\n",
      "[48]\tvalidation_0-mlogloss:1.29353\n",
      "[49]\tvalidation_0-mlogloss:1.28423\n",
      "[50]\tvalidation_0-mlogloss:1.27865\n",
      "[51]\tvalidation_0-mlogloss:1.27126\n",
      "[52]\tvalidation_0-mlogloss:1.26649\n",
      "[53]\tvalidation_0-mlogloss:1.26047\n",
      "[54]\tvalidation_0-mlogloss:1.25262\n",
      "[55]\tvalidation_0-mlogloss:1.24822\n",
      "[56]\tvalidation_0-mlogloss:1.24145\n",
      "[57]\tvalidation_0-mlogloss:1.23445\n",
      "[58]\tvalidation_0-mlogloss:1.22994\n",
      "[59]\tvalidation_0-mlogloss:1.22545\n",
      "[60]\tvalidation_0-mlogloss:1.22026\n",
      "[61]\tvalidation_0-mlogloss:1.21490\n",
      "[62]\tvalidation_0-mlogloss:1.20964\n",
      "[63]\tvalidation_0-mlogloss:1.20596\n",
      "[64]\tvalidation_0-mlogloss:1.20186\n",
      "[65]\tvalidation_0-mlogloss:1.19662\n",
      "[66]\tvalidation_0-mlogloss:1.19468\n",
      "[67]\tvalidation_0-mlogloss:1.19056\n",
      "[68]\tvalidation_0-mlogloss:1.18704\n",
      "[69]\tvalidation_0-mlogloss:1.18435\n",
      "[70]\tvalidation_0-mlogloss:1.18004\n",
      "[71]\tvalidation_0-mlogloss:1.17491\n",
      "[72]\tvalidation_0-mlogloss:1.17230\n",
      "[73]\tvalidation_0-mlogloss:1.17085\n",
      "[74]\tvalidation_0-mlogloss:1.16815\n",
      "[75]\tvalidation_0-mlogloss:1.16378\n",
      "[76]\tvalidation_0-mlogloss:1.16243\n",
      "[77]\tvalidation_0-mlogloss:1.16038\n",
      "[78]\tvalidation_0-mlogloss:1.15729\n",
      "[79]\tvalidation_0-mlogloss:1.15353\n",
      "[80]\tvalidation_0-mlogloss:1.15158\n",
      "[81]\tvalidation_0-mlogloss:1.14738\n",
      "[82]\tvalidation_0-mlogloss:1.14456\n",
      "[83]\tvalidation_0-mlogloss:1.14177\n",
      "[84]\tvalidation_0-mlogloss:1.14065\n",
      "[85]\tvalidation_0-mlogloss:1.13715\n",
      "[86]\tvalidation_0-mlogloss:1.13543\n",
      "[87]\tvalidation_0-mlogloss:1.13673\n",
      "[88]\tvalidation_0-mlogloss:1.13521\n",
      "[89]\tvalidation_0-mlogloss:1.13076\n",
      "[90]\tvalidation_0-mlogloss:1.12851\n",
      "[91]\tvalidation_0-mlogloss:1.12761\n",
      "[92]\tvalidation_0-mlogloss:1.12644\n",
      "[93]\tvalidation_0-mlogloss:1.12777\n",
      "[94]\tvalidation_0-mlogloss:1.12535\n",
      "[95]\tvalidation_0-mlogloss:1.12542\n",
      "[96]\tvalidation_0-mlogloss:1.12286\n",
      "[97]\tvalidation_0-mlogloss:1.12133\n",
      "[98]\tvalidation_0-mlogloss:1.12014\n",
      "[99]\tvalidation_0-mlogloss:1.11794\n",
      "[100]\tvalidation_0-mlogloss:1.11615\n",
      "[101]\tvalidation_0-mlogloss:1.11309\n",
      "[102]\tvalidation_0-mlogloss:1.10953\n",
      "[103]\tvalidation_0-mlogloss:1.11044\n",
      "[104]\tvalidation_0-mlogloss:1.10742\n",
      "[105]\tvalidation_0-mlogloss:1.10781\n",
      "[106]\tvalidation_0-mlogloss:1.10529\n",
      "[107]\tvalidation_0-mlogloss:1.10483\n",
      "[108]\tvalidation_0-mlogloss:1.10246\n",
      "[109]\tvalidation_0-mlogloss:1.10044\n",
      "[110]\tvalidation_0-mlogloss:1.09867\n",
      "[111]\tvalidation_0-mlogloss:1.09914\n",
      "[112]\tvalidation_0-mlogloss:1.09592\n",
      "[113]\tvalidation_0-mlogloss:1.09563\n",
      "[114]\tvalidation_0-mlogloss:1.09419\n",
      "[115]\tvalidation_0-mlogloss:1.09164\n",
      "[116]\tvalidation_0-mlogloss:1.09006\n",
      "[117]\tvalidation_0-mlogloss:1.08752\n",
      "[118]\tvalidation_0-mlogloss:1.08633\n",
      "[119]\tvalidation_0-mlogloss:1.08418\n",
      "[120]\tvalidation_0-mlogloss:1.08413\n",
      "[121]\tvalidation_0-mlogloss:1.08364\n",
      "[122]\tvalidation_0-mlogloss:1.08399\n",
      "[123]\tvalidation_0-mlogloss:1.08491\n",
      "[124]\tvalidation_0-mlogloss:1.08428\n",
      "[125]\tvalidation_0-mlogloss:1.08362\n",
      "[126]\tvalidation_0-mlogloss:1.08226\n",
      "[127]\tvalidation_0-mlogloss:1.08214\n",
      "[128]\tvalidation_0-mlogloss:1.08082\n",
      "[129]\tvalidation_0-mlogloss:1.08145\n",
      "[130]\tvalidation_0-mlogloss:1.07938\n",
      "[131]\tvalidation_0-mlogloss:1.07803\n",
      "[132]\tvalidation_0-mlogloss:1.07624\n",
      "[133]\tvalidation_0-mlogloss:1.07530\n",
      "[134]\tvalidation_0-mlogloss:1.07678\n",
      "[135]\tvalidation_0-mlogloss:1.07528\n",
      "[136]\tvalidation_0-mlogloss:1.07393\n",
      "[137]\tvalidation_0-mlogloss:1.07225\n",
      "[138]\tvalidation_0-mlogloss:1.07154\n",
      "[139]\tvalidation_0-mlogloss:1.06949\n",
      "[140]\tvalidation_0-mlogloss:1.06691\n",
      "[141]\tvalidation_0-mlogloss:1.06517\n",
      "[142]\tvalidation_0-mlogloss:1.06473\n",
      "[143]\tvalidation_0-mlogloss:1.06575\n",
      "[144]\tvalidation_0-mlogloss:1.06367\n",
      "[145]\tvalidation_0-mlogloss:1.06498\n",
      "[146]\tvalidation_0-mlogloss:1.06505\n",
      "[147]\tvalidation_0-mlogloss:1.06512\n",
      "[148]\tvalidation_0-mlogloss:1.06457\n",
      "[149]\tvalidation_0-mlogloss:1.06330\n",
      "[150]\tvalidation_0-mlogloss:1.06442\n",
      "[151]\tvalidation_0-mlogloss:1.06372\n",
      "[152]\tvalidation_0-mlogloss:1.06499\n",
      "[153]\tvalidation_0-mlogloss:1.06428\n",
      "[154]\tvalidation_0-mlogloss:1.06228\n",
      "[155]\tvalidation_0-mlogloss:1.06080\n",
      "[156]\tvalidation_0-mlogloss:1.06011\n",
      "[157]\tvalidation_0-mlogloss:1.06134\n",
      "[158]\tvalidation_0-mlogloss:1.05958\n",
      "[159]\tvalidation_0-mlogloss:1.06210\n",
      "[160]\tvalidation_0-mlogloss:1.06092\n",
      "[161]\tvalidation_0-mlogloss:1.06308\n",
      "[162]\tvalidation_0-mlogloss:1.06145\n",
      "[163]\tvalidation_0-mlogloss:1.06001\n",
      "[164]\tvalidation_0-mlogloss:1.06030\n",
      "[165]\tvalidation_0-mlogloss:1.06014\n",
      "[166]\tvalidation_0-mlogloss:1.06053\n",
      "[167]\tvalidation_0-mlogloss:1.06107\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.69      0.64      1499\n",
      "           1       0.39      0.33      0.36      1499\n",
      "           2       0.74      0.89      0.80      1499\n",
      "           3       0.66      0.44      0.53      1499\n",
      "           4       0.87      0.92      0.89      1499\n",
      "           5       0.88      0.91      0.89      1499\n",
      "           6       0.87      0.76      0.81      1499\n",
      "           7       0.66      0.49      0.56      1499\n",
      "           8       0.81      0.93      0.87      1499\n",
      "           9       0.97      0.70      0.81      1499\n",
      "          10       0.89      0.73      0.80      1499\n",
      "          11       0.92      0.88      0.90      1499\n",
      "          12       0.69      0.87      0.77      1499\n",
      "          13       0.62      0.66      0.64      1499\n",
      "          14       0.56      0.72      0.63      1499\n",
      "          15       0.77      0.68      0.72      1499\n",
      "          16       0.84      0.67      0.75      1499\n",
      "          17       0.86      0.90      0.88      1499\n",
      "          18       0.70      0.89      0.78      1499\n",
      "          19       0.57      0.76      0.65      1499\n",
      "          20       0.93      0.96      0.94      1499\n",
      "          21       0.84      0.79      0.82      1499\n",
      "          22       0.70      0.76      0.73      1499\n",
      "          23       0.92      0.94      0.93      1499\n",
      "          24       0.79      0.59      0.67      1499\n",
      "\n",
      "    accuracy                           0.75     37475\n",
      "   macro avg       0.76      0.75      0.75     37475\n",
      "weighted avg       0.76      0.75      0.75     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8f654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(25, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "300aa492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9375/9375 [==============================] - 7s 725us/step - loss: 0.7932 - val_loss: 1.8528\n",
      "Epoch 2/10\n",
      "9375/9375 [==============================] - 7s 715us/step - loss: 0.2628 - val_loss: 1.7551\n",
      "Epoch 3/10\n",
      "9375/9375 [==============================] - 7s 707us/step - loss: 0.1860 - val_loss: 1.9163\n",
      "Epoch 4/10\n",
      "9375/9375 [==============================] - 7s 714us/step - loss: 0.1535 - val_loss: 1.8395\n",
      "Epoch 5/10\n",
      "9375/9375 [==============================] - 7s 710us/step - loss: 0.1339 - val_loss: 1.8014\n",
      "Epoch 6/10\n",
      "9375/9375 [==============================] - 7s 715us/step - loss: 0.1178 - val_loss: 1.6788\n",
      "Epoch 7/10\n",
      "9375/9375 [==============================] - 7s 720us/step - loss: 0.1066 - val_loss: 1.8251\n",
      "Epoch 8/10\n",
      "9375/9375 [==============================] - 7s 703us/step - loss: 0.1007 - val_loss: 1.8005\n",
      "Epoch 9/10\n",
      "9375/9375 [==============================] - 7s 724us/step - loss: 0.0931 - val_loss: 1.8089\n",
      "Epoch 10/10\n",
      "9375/9375 [==============================] - 7s 713us/step - loss: 0.0877 - val_loss: 1.6917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x287cbbf40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172/1172 [==============================] - 0s 291us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75      1499\n",
      "           1       0.34      0.16      0.22      1499\n",
      "           2       0.85      0.81      0.83      1499\n",
      "           3       0.52      0.34      0.41      1499\n",
      "           4       0.96      0.91      0.93      1499\n",
      "           5       0.98      0.83      0.90      1499\n",
      "           6       0.99      0.72      0.84      1499\n",
      "           7       0.77      0.52      0.62      1499\n",
      "           8       0.74      0.98      0.85      1499\n",
      "           9       0.91      0.91      0.91      1499\n",
      "          10       0.78      0.84      0.81      1499\n",
      "          11       0.97      0.85      0.91      1499\n",
      "          12       0.73      0.92      0.81      1499\n",
      "          13       0.42      0.92      0.58      1499\n",
      "          14       0.50      0.81      0.62      1499\n",
      "          15       0.88      0.59      0.71      1499\n",
      "          16       0.91      0.82      0.87      1499\n",
      "          17       0.97      0.73      0.83      1499\n",
      "          18       0.76      0.91      0.83      1499\n",
      "          19       0.63      0.49      0.55      1499\n",
      "          20       0.99      0.75      0.85      1499\n",
      "          21       0.69      0.85      0.76      1499\n",
      "          22       0.69      0.89      0.78      1499\n",
      "          23       0.93      0.96      0.95      1499\n",
      "          24       0.83      0.70      0.76      1499\n",
      "\n",
      "    accuracy                           0.76     37475\n",
      "   macro avg       0.78      0.76      0.75     37475\n",
      "weighted avg       0.78      0.76      0.75     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.40025\n",
      "[1]\tvalidation_0-mlogloss:2.15233\n",
      "[2]\tvalidation_0-mlogloss:1.99289\n",
      "[3]\tvalidation_0-mlogloss:1.86551\n",
      "[4]\tvalidation_0-mlogloss:1.75837\n",
      "[5]\tvalidation_0-mlogloss:1.66520\n",
      "[6]\tvalidation_0-mlogloss:1.58458\n",
      "[7]\tvalidation_0-mlogloss:1.52027\n",
      "[8]\tvalidation_0-mlogloss:1.46392\n",
      "[9]\tvalidation_0-mlogloss:1.40724\n",
      "[10]\tvalidation_0-mlogloss:1.36207\n",
      "[11]\tvalidation_0-mlogloss:1.31838\n",
      "[12]\tvalidation_0-mlogloss:1.27652\n",
      "[13]\tvalidation_0-mlogloss:1.24382\n",
      "[14]\tvalidation_0-mlogloss:1.21411\n",
      "[15]\tvalidation_0-mlogloss:1.18086\n",
      "[16]\tvalidation_0-mlogloss:1.15360\n",
      "[17]\tvalidation_0-mlogloss:1.12702\n",
      "[18]\tvalidation_0-mlogloss:1.10258\n",
      "[19]\tvalidation_0-mlogloss:1.07952\n",
      "[20]\tvalidation_0-mlogloss:1.05954\n",
      "[21]\tvalidation_0-mlogloss:1.03769\n",
      "[22]\tvalidation_0-mlogloss:1.02003\n",
      "[23]\tvalidation_0-mlogloss:1.00183\n",
      "[24]\tvalidation_0-mlogloss:0.98389\n",
      "[25]\tvalidation_0-mlogloss:0.96445\n",
      "[26]\tvalidation_0-mlogloss:0.94774\n",
      "[27]\tvalidation_0-mlogloss:0.93380\n",
      "[28]\tvalidation_0-mlogloss:0.91936\n",
      "[29]\tvalidation_0-mlogloss:0.90547\n",
      "[30]\tvalidation_0-mlogloss:0.89266\n",
      "[31]\tvalidation_0-mlogloss:0.88172\n",
      "[32]\tvalidation_0-mlogloss:0.86954\n",
      "[33]\tvalidation_0-mlogloss:0.85848\n",
      "[34]\tvalidation_0-mlogloss:0.85043\n",
      "[35]\tvalidation_0-mlogloss:0.83975\n",
      "[36]\tvalidation_0-mlogloss:0.83070\n",
      "[37]\tvalidation_0-mlogloss:0.82138\n",
      "[38]\tvalidation_0-mlogloss:0.81230\n",
      "[39]\tvalidation_0-mlogloss:0.80397\n",
      "[40]\tvalidation_0-mlogloss:0.79436\n",
      "[41]\tvalidation_0-mlogloss:0.78496\n",
      "[42]\tvalidation_0-mlogloss:0.77596\n",
      "[43]\tvalidation_0-mlogloss:0.76646\n",
      "[44]\tvalidation_0-mlogloss:0.75845\n",
      "[45]\tvalidation_0-mlogloss:0.75230\n",
      "[46]\tvalidation_0-mlogloss:0.74327\n",
      "[47]\tvalidation_0-mlogloss:0.73586\n",
      "[48]\tvalidation_0-mlogloss:0.73212\n",
      "[49]\tvalidation_0-mlogloss:0.72602\n",
      "[50]\tvalidation_0-mlogloss:0.71790\n",
      "[51]\tvalidation_0-mlogloss:0.71150\n",
      "[52]\tvalidation_0-mlogloss:0.70767\n",
      "[53]\tvalidation_0-mlogloss:0.70232\n",
      "[54]\tvalidation_0-mlogloss:0.69422\n",
      "[55]\tvalidation_0-mlogloss:0.68835\n",
      "[56]\tvalidation_0-mlogloss:0.68538\n",
      "[57]\tvalidation_0-mlogloss:0.67943\n",
      "[58]\tvalidation_0-mlogloss:0.67502\n",
      "[59]\tvalidation_0-mlogloss:0.66933\n",
      "[60]\tvalidation_0-mlogloss:0.66565\n",
      "[61]\tvalidation_0-mlogloss:0.66150\n",
      "[62]\tvalidation_0-mlogloss:0.65796\n",
      "[63]\tvalidation_0-mlogloss:0.65212\n",
      "[64]\tvalidation_0-mlogloss:0.65066\n",
      "[65]\tvalidation_0-mlogloss:0.64683\n",
      "[66]\tvalidation_0-mlogloss:0.64346\n",
      "[67]\tvalidation_0-mlogloss:0.64159\n",
      "[68]\tvalidation_0-mlogloss:0.63779\n",
      "[69]\tvalidation_0-mlogloss:0.63330\n",
      "[70]\tvalidation_0-mlogloss:0.62966\n",
      "[71]\tvalidation_0-mlogloss:0.62798\n",
      "[72]\tvalidation_0-mlogloss:0.62267\n",
      "[73]\tvalidation_0-mlogloss:0.61850\n",
      "[74]\tvalidation_0-mlogloss:0.61421\n",
      "[75]\tvalidation_0-mlogloss:0.61058\n",
      "[76]\tvalidation_0-mlogloss:0.60715\n",
      "[77]\tvalidation_0-mlogloss:0.60356\n",
      "[78]\tvalidation_0-mlogloss:0.60093\n",
      "[79]\tvalidation_0-mlogloss:0.59842\n",
      "[80]\tvalidation_0-mlogloss:0.59581\n",
      "[81]\tvalidation_0-mlogloss:0.59263\n",
      "[82]\tvalidation_0-mlogloss:0.58884\n",
      "[83]\tvalidation_0-mlogloss:0.58645\n",
      "[84]\tvalidation_0-mlogloss:0.58298\n",
      "[85]\tvalidation_0-mlogloss:0.58008\n",
      "[86]\tvalidation_0-mlogloss:0.57860\n",
      "[87]\tvalidation_0-mlogloss:0.57622\n",
      "[88]\tvalidation_0-mlogloss:0.57376\n",
      "[89]\tvalidation_0-mlogloss:0.57215\n",
      "[90]\tvalidation_0-mlogloss:0.57065\n",
      "[91]\tvalidation_0-mlogloss:0.56845\n",
      "[92]\tvalidation_0-mlogloss:0.56542\n",
      "[93]\tvalidation_0-mlogloss:0.56513\n",
      "[94]\tvalidation_0-mlogloss:0.56381\n",
      "[95]\tvalidation_0-mlogloss:0.56191\n",
      "[96]\tvalidation_0-mlogloss:0.55952\n",
      "[97]\tvalidation_0-mlogloss:0.55783\n",
      "[98]\tvalidation_0-mlogloss:0.55871\n",
      "[99]\tvalidation_0-mlogloss:0.55896\n",
      "[100]\tvalidation_0-mlogloss:0.55665\n",
      "[101]\tvalidation_0-mlogloss:0.55637\n",
      "[102]\tvalidation_0-mlogloss:0.55623\n",
      "[103]\tvalidation_0-mlogloss:0.55323\n",
      "[104]\tvalidation_0-mlogloss:0.55274\n",
      "[105]\tvalidation_0-mlogloss:0.55056\n",
      "[106]\tvalidation_0-mlogloss:0.54815\n",
      "[107]\tvalidation_0-mlogloss:0.54668\n",
      "[108]\tvalidation_0-mlogloss:0.54662\n",
      "[109]\tvalidation_0-mlogloss:0.54449\n",
      "[110]\tvalidation_0-mlogloss:0.54340\n",
      "[111]\tvalidation_0-mlogloss:0.54198\n",
      "[112]\tvalidation_0-mlogloss:0.54064\n",
      "[113]\tvalidation_0-mlogloss:0.53880\n",
      "[114]\tvalidation_0-mlogloss:0.53737\n",
      "[115]\tvalidation_0-mlogloss:0.53459\n",
      "[116]\tvalidation_0-mlogloss:0.53391\n",
      "[117]\tvalidation_0-mlogloss:0.53265\n",
      "[118]\tvalidation_0-mlogloss:0.53070\n",
      "[119]\tvalidation_0-mlogloss:0.52839\n",
      "[120]\tvalidation_0-mlogloss:0.52695\n",
      "[121]\tvalidation_0-mlogloss:0.52545\n",
      "[122]\tvalidation_0-mlogloss:0.52402\n",
      "[123]\tvalidation_0-mlogloss:0.52278\n",
      "[124]\tvalidation_0-mlogloss:0.52067\n",
      "[125]\tvalidation_0-mlogloss:0.52015\n",
      "[126]\tvalidation_0-mlogloss:0.52042\n",
      "[127]\tvalidation_0-mlogloss:0.52066\n",
      "[128]\tvalidation_0-mlogloss:0.52077\n",
      "[129]\tvalidation_0-mlogloss:0.52013\n",
      "[130]\tvalidation_0-mlogloss:0.52134\n",
      "[131]\tvalidation_0-mlogloss:0.52022\n",
      "[132]\tvalidation_0-mlogloss:0.51905\n",
      "[133]\tvalidation_0-mlogloss:0.51926\n",
      "[134]\tvalidation_0-mlogloss:0.52056\n",
      "[135]\tvalidation_0-mlogloss:0.51969\n",
      "[136]\tvalidation_0-mlogloss:0.52022\n",
      "[137]\tvalidation_0-mlogloss:0.51883\n",
      "[138]\tvalidation_0-mlogloss:0.51786\n",
      "[139]\tvalidation_0-mlogloss:0.51647\n",
      "[140]\tvalidation_0-mlogloss:0.51560\n",
      "[141]\tvalidation_0-mlogloss:0.51539\n",
      "[142]\tvalidation_0-mlogloss:0.51440\n",
      "[143]\tvalidation_0-mlogloss:0.51357\n",
      "[144]\tvalidation_0-mlogloss:0.51246\n",
      "[145]\tvalidation_0-mlogloss:0.51189\n",
      "[146]\tvalidation_0-mlogloss:0.51059\n",
      "[147]\tvalidation_0-mlogloss:0.50934\n",
      "[148]\tvalidation_0-mlogloss:0.50932\n",
      "[149]\tvalidation_0-mlogloss:0.50851\n",
      "[150]\tvalidation_0-mlogloss:0.50917\n",
      "[151]\tvalidation_0-mlogloss:0.51050\n",
      "[152]\tvalidation_0-mlogloss:0.50977\n",
      "[153]\tvalidation_0-mlogloss:0.50876\n",
      "[154]\tvalidation_0-mlogloss:0.50857\n",
      "[155]\tvalidation_0-mlogloss:0.50753\n",
      "[156]\tvalidation_0-mlogloss:0.50750\n",
      "[157]\tvalidation_0-mlogloss:0.50650\n",
      "[158]\tvalidation_0-mlogloss:0.50560\n",
      "[159]\tvalidation_0-mlogloss:0.50450\n",
      "[160]\tvalidation_0-mlogloss:0.50313\n",
      "[161]\tvalidation_0-mlogloss:0.50248\n",
      "[162]\tvalidation_0-mlogloss:0.50161\n",
      "[163]\tvalidation_0-mlogloss:0.50102\n",
      "[164]\tvalidation_0-mlogloss:0.49982\n",
      "[165]\tvalidation_0-mlogloss:0.49927\n",
      "[166]\tvalidation_0-mlogloss:0.49865\n",
      "[167]\tvalidation_0-mlogloss:0.49744\n",
      "[168]\tvalidation_0-mlogloss:0.49761\n",
      "[169]\tvalidation_0-mlogloss:0.49717\n",
      "[170]\tvalidation_0-mlogloss:0.49682\n",
      "[171]\tvalidation_0-mlogloss:0.49848\n",
      "[172]\tvalidation_0-mlogloss:0.49863\n",
      "[173]\tvalidation_0-mlogloss:0.49782\n",
      "[174]\tvalidation_0-mlogloss:0.49716\n",
      "[175]\tvalidation_0-mlogloss:0.49664\n",
      "[176]\tvalidation_0-mlogloss:0.49515\n",
      "[177]\tvalidation_0-mlogloss:0.49466\n",
      "[178]\tvalidation_0-mlogloss:0.49445\n",
      "[179]\tvalidation_0-mlogloss:0.49502\n",
      "[180]\tvalidation_0-mlogloss:0.49554\n",
      "[181]\tvalidation_0-mlogloss:0.49514\n",
      "[182]\tvalidation_0-mlogloss:0.49489\n",
      "[183]\tvalidation_0-mlogloss:0.49417\n",
      "[184]\tvalidation_0-mlogloss:0.49486\n",
      "[185]\tvalidation_0-mlogloss:0.49540\n",
      "[186]\tvalidation_0-mlogloss:0.49442\n",
      "[187]\tvalidation_0-mlogloss:0.49360\n",
      "[188]\tvalidation_0-mlogloss:0.49445\n",
      "[189]\tvalidation_0-mlogloss:0.49438\n",
      "[190]\tvalidation_0-mlogloss:0.49385\n",
      "[191]\tvalidation_0-mlogloss:0.49330\n",
      "[192]\tvalidation_0-mlogloss:0.49334\n",
      "[193]\tvalidation_0-mlogloss:0.49322\n",
      "[194]\tvalidation_0-mlogloss:0.49278\n",
      "[195]\tvalidation_0-mlogloss:0.49191\n",
      "[196]\tvalidation_0-mlogloss:0.49138\n",
      "[197]\tvalidation_0-mlogloss:0.49038\n",
      "[198]\tvalidation_0-mlogloss:0.48976\n",
      "[199]\tvalidation_0-mlogloss:0.48962\n",
      "[200]\tvalidation_0-mlogloss:0.49018\n",
      "[201]\tvalidation_0-mlogloss:0.48986\n",
      "[202]\tvalidation_0-mlogloss:0.49006\n",
      "[203]\tvalidation_0-mlogloss:0.48933\n",
      "[204]\tvalidation_0-mlogloss:0.48888\n",
      "[205]\tvalidation_0-mlogloss:0.48786\n",
      "[206]\tvalidation_0-mlogloss:0.48706\n",
      "[207]\tvalidation_0-mlogloss:0.48707\n",
      "[208]\tvalidation_0-mlogloss:0.48718\n",
      "[209]\tvalidation_0-mlogloss:0.48735\n",
      "[210]\tvalidation_0-mlogloss:0.48740\n",
      "[211]\tvalidation_0-mlogloss:0.48676\n",
      "[212]\tvalidation_0-mlogloss:0.48661\n",
      "[213]\tvalidation_0-mlogloss:0.48722\n",
      "[214]\tvalidation_0-mlogloss:0.48659\n",
      "[215]\tvalidation_0-mlogloss:0.48639\n",
      "[216]\tvalidation_0-mlogloss:0.48608\n",
      "[217]\tvalidation_0-mlogloss:0.48623\n",
      "[218]\tvalidation_0-mlogloss:0.48688\n",
      "[219]\tvalidation_0-mlogloss:0.48635\n",
      "[220]\tvalidation_0-mlogloss:0.48576\n",
      "[221]\tvalidation_0-mlogloss:0.48653\n",
      "[222]\tvalidation_0-mlogloss:0.48611\n",
      "[223]\tvalidation_0-mlogloss:0.48591\n",
      "[224]\tvalidation_0-mlogloss:0.48541\n",
      "[225]\tvalidation_0-mlogloss:0.48513\n",
      "[226]\tvalidation_0-mlogloss:0.48520\n",
      "[227]\tvalidation_0-mlogloss:0.48516\n",
      "[228]\tvalidation_0-mlogloss:0.48459\n",
      "[229]\tvalidation_0-mlogloss:0.48444\n",
      "[230]\tvalidation_0-mlogloss:0.48480\n",
      "[231]\tvalidation_0-mlogloss:0.48575\n",
      "[232]\tvalidation_0-mlogloss:0.48613\n",
      "[233]\tvalidation_0-mlogloss:0.48563\n",
      "[234]\tvalidation_0-mlogloss:0.48677\n",
      "[235]\tvalidation_0-mlogloss:0.48636\n",
      "[236]\tvalidation_0-mlogloss:0.48605\n",
      "[237]\tvalidation_0-mlogloss:0.48577\n",
      "[238]\tvalidation_0-mlogloss:0.48644\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83      1499\n",
      "           1       0.76      0.58      0.65      1499\n",
      "           2       0.92      0.95      0.94      1499\n",
      "           3       0.93      0.85      0.88      1499\n",
      "           4       0.98      0.99      0.99      1499\n",
      "           5       0.97      0.97      0.97      1499\n",
      "           6       0.97      0.87      0.92      1499\n",
      "           7       0.95      0.55      0.70      1499\n",
      "           8       0.89      0.97      0.93      1499\n",
      "           9       0.99      0.92      0.96      1499\n",
      "          10       0.97      0.78      0.86      1499\n",
      "          11       0.92      1.00      0.96      1499\n",
      "          12       0.84      0.93      0.89      1499\n",
      "          13       0.87      0.85      0.86      1499\n",
      "          14       0.84      0.93      0.88      1499\n",
      "          15       0.87      0.93      0.90      1499\n",
      "          16       0.98      0.95      0.97      1499\n",
      "          17       0.89      0.99      0.94      1499\n",
      "          18       0.87      0.96      0.91      1499\n",
      "          19       0.69      0.90      0.79      1499\n",
      "          20       0.96      0.99      0.98      1499\n",
      "          21       0.94      0.86      0.90      1499\n",
      "          22       0.78      0.90      0.83      1499\n",
      "          23       0.97      0.99      0.98      1499\n",
      "          24       0.88      0.87      0.88      1499\n",
      "\n",
      "    accuracy                           0.89     37475\n",
      "   macro avg       0.90      0.89      0.89     37475\n",
      "weighted avg       0.90      0.89      0.89     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1dad4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(25, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3361defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9375/9375 [==============================] - 7s 778us/step - loss: 0.5132 - val_loss: 1.5659\n",
      "Epoch 2/10\n",
      "9375/9375 [==============================] - 7s 794us/step - loss: 0.1567 - val_loss: 1.3371\n",
      "Epoch 3/10\n",
      "9375/9375 [==============================] - 7s 775us/step - loss: 0.1113 - val_loss: 1.3264\n",
      "Epoch 4/10\n",
      "9375/9375 [==============================] - 7s 777us/step - loss: 0.0915 - val_loss: 1.2577\n",
      "Epoch 5/10\n",
      "9375/9375 [==============================] - 7s 766us/step - loss: 0.0775 - val_loss: 1.2057\n",
      "Epoch 6/10\n",
      "9375/9375 [==============================] - 7s 775us/step - loss: 0.0695 - val_loss: 1.0995\n",
      "Epoch 7/10\n",
      "9375/9375 [==============================] - 7s 785us/step - loss: 0.0620 - val_loss: 1.3081\n",
      "Epoch 8/10\n",
      "9375/9375 [==============================] - 7s 766us/step - loss: 0.0574 - val_loss: 1.1754\n",
      "Epoch 9/10\n",
      "9375/9375 [==============================] - 7s 774us/step - loss: 0.0539 - val_loss: 1.1682\n",
      "Epoch 10/10\n",
      "9375/9375 [==============================] - 7s 772us/step - loss: 0.0516 - val_loss: 1.1430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ea5caa40>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172/1172 [==============================] - 0s 308us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.60      0.71      1499\n",
      "           1       0.58      0.46      0.51      1499\n",
      "           2       0.85      0.86      0.86      1499\n",
      "           3       0.85      0.82      0.84      1499\n",
      "           4       1.00      0.99      1.00      1499\n",
      "           5       0.94      0.95      0.94      1499\n",
      "           6       0.94      0.89      0.91      1499\n",
      "           7       0.86      0.60      0.71      1499\n",
      "           8       0.84      0.98      0.91      1499\n",
      "           9       0.97      0.97      0.97      1499\n",
      "          10       0.83      0.48      0.61      1499\n",
      "          11       0.81      0.99      0.89      1499\n",
      "          12       0.83      0.91      0.87      1499\n",
      "          13       0.61      0.69      0.65      1499\n",
      "          14       0.60      0.89      0.72      1499\n",
      "          15       0.95      0.89      0.92      1499\n",
      "          16       0.90      0.94      0.92      1499\n",
      "          17       0.90      0.99      0.94      1499\n",
      "          18       0.83      0.86      0.84      1499\n",
      "          19       0.79      0.85      0.82      1499\n",
      "          20       0.96      0.94      0.95      1499\n",
      "          21       0.80      0.80      0.80      1499\n",
      "          22       0.71      0.81      0.76      1499\n",
      "          23       0.97      0.99      0.98      1499\n",
      "          24       0.88      0.78      0.83      1499\n",
      "\n",
      "    accuracy                           0.84     37475\n",
      "   macro avg       0.84      0.84      0.83     37475\n",
      "weighted avg       0.84      0.84      0.83     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec61442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b9b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

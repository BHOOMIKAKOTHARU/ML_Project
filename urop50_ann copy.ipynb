{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 50 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 10:19:04.487674: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 10:19:04.520351: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 10:19:04.520399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 10:19:04.521609: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 10:19:04.527572: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 10:19:04.528124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 10:19:05.493603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081a3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /usr/local/python/3.10.8/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.3 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (1.11.3)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (3.8.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.8/lib/python3.10/site-packages (from mne) (4.66.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from mne) (1.7.0)\n",
      "Requirement already satisfied: decorator in /home/codespace/.local/lib/python3.10/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from mne) (23.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (4.43.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from pooch>=1.5->mne) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from pooch>=1.5->mne) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->mne) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files2/S001R04.edf',\n",
       " 'files2/S002R04.edf',\n",
       " 'files2/S003R04.edf',\n",
       " 'files2/S004R04.edf',\n",
       " 'files2/S005R04.edf',\n",
       " 'files2/S006R04.edf',\n",
       " 'files2/S007R04.edf',\n",
       " 'files2/S008R04.edf',\n",
       " 'files2/S009R04.edf',\n",
       " 'files2/S010R04.edf',\n",
       " 'files2/S011R04.edf',\n",
       " 'files2/S012R04.edf',\n",
       " 'files2/S013R04.edf',\n",
       " 'files2/S014R04.edf',\n",
       " 'files2/S015R04.edf',\n",
       " 'files2/S016R04.edf',\n",
       " 'files2/S017R04.edf',\n",
       " 'files2/S018R04.edf',\n",
       " 'files2/S019R04.edf',\n",
       " 'files2/S020R04.edf',\n",
       " 'files2/S021R04.edf',\n",
       " 'files2/S022R04.edf',\n",
       " 'files2/S023R04.edf',\n",
       " 'files2/S024R04.edf',\n",
       " 'files2/S025R04.edf',\n",
       " 'files2/S026R04.edf',\n",
       " 'files2/S027R04.edf',\n",
       " 'files2/S028R04.edf',\n",
       " 'files2/S029R04.edf',\n",
       " 'files2/S030R04.edf',\n",
       " 'files2/S031R04.edf',\n",
       " 'files2/S032R04.edf',\n",
       " 'files2/S033R04.edf',\n",
       " 'files2/S034R04.edf',\n",
       " 'files2/S035R04.edf',\n",
       " 'files2/S036R04.edf',\n",
       " 'files2/S037R04.edf',\n",
       " 'files2/S038R04.edf',\n",
       " 'files2/S039R04.edf',\n",
       " 'files2/S040R04.edf',\n",
       " 'files2/S041R04.edf',\n",
       " 'files2/S042R04.edf',\n",
       " 'files2/S043R04.edf',\n",
       " 'files2/S044R04.edf',\n",
       " 'files2/S045R04.edf',\n",
       " 'files2/S046R04.edf',\n",
       " 'files2/S047R04.edf',\n",
       " 'files2/S048R04.edf',\n",
       " 'files2/S049R04.edf',\n",
       " 'files2/S050R04.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files2/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 74950, 600000, 74950)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0e-05  1.5e-05  6.0e-06 ... -4.4e-05 -4.1e-05 -5.2e-05]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000005   0.000002   0.000037   0.000039   0.000030   0.000026   \n",
       "1       -0.000012  -0.000024   0.000001  -0.000002  -0.000015  -0.000022   \n",
       "2       -0.000077  -0.000078  -0.000059  -0.000065  -0.000063  -0.000055   \n",
       "3       -0.000066  -0.000067  -0.000050  -0.000065  -0.000060  -0.000055   \n",
       "4       -0.000045  -0.000055  -0.000033  -0.000053  -0.000054  -0.000063   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995  -0.000034  -0.000018  -0.000025  -0.000034  -0.000024  -0.000009   \n",
       "599996  -0.000052  -0.000042  -0.000040  -0.000046  -0.000043  -0.000022   \n",
       "599997  -0.000049  -0.000026  -0.000033  -0.000034  -0.000021  -0.000008   \n",
       "599998  -0.000036  -0.000033  -0.000033  -0.000035  -0.000031  -0.000016   \n",
       "599999  -0.000054  -0.000047  -0.000050  -0.000051  -0.000044  -0.000031   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000016  -0.000014   0.000004   0.000018  ...   -0.000021   \n",
       "1       -0.000055  -0.000036  -0.000027  -0.000025  ...   -0.000050   \n",
       "2       -0.000067  -0.000088  -0.000071  -0.000065  ...   -0.000017   \n",
       "3       -0.000068  -0.000062  -0.000053  -0.000054  ...   -0.000039   \n",
       "4       -0.000083  -0.000052  -0.000050  -0.000053  ...   -0.000044   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995  -0.000008  -0.000034  -0.000037  -0.000030  ...   -0.000021   \n",
       "599996  -0.000021  -0.000047  -0.000048  -0.000042  ...   -0.000005   \n",
       "599997  -0.000006  -0.000027  -0.000040  -0.000032  ...   -0.000002   \n",
       "599998  -0.000014  -0.000040  -0.000038  -0.000034  ...   -0.000023   \n",
       "599999  -0.000021  -0.000041  -0.000052  -0.000045  ...   -0.000008   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000008   -0.000035   -0.000045   -0.000066   -0.000039   \n",
       "1        -0.000040   -0.000068   -0.000065   -0.000084   -0.000052   \n",
       "2        -0.000022   -0.000050   -0.000035   -0.000048   -0.000018   \n",
       "3        -0.000060   -0.000078   -0.000064   -0.000068   -0.000041   \n",
       "4        -0.000055   -0.000070   -0.000054   -0.000063   -0.000037   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995   -0.000024   -0.000015   -0.000025   -0.000026   -0.000031   \n",
       "599996   -0.000017   -0.000006   -0.000012   -0.000011   -0.000013   \n",
       "599997   -0.000005    0.000006   -0.000005   -0.000006   -0.000008   \n",
       "599998   -0.000018   -0.000011   -0.000019   -0.000027   -0.000029   \n",
       "599999   -0.000013    0.000001   -0.000005   -0.000013   -0.000012   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000033   -0.000048   -0.000039   -0.000039  \n",
       "1        -0.000021   -0.000042   -0.000031   -0.000034  \n",
       "2        -0.000020   -0.000042   -0.000029   -0.000027  \n",
       "3        -0.000044   -0.000062   -0.000034   -0.000043  \n",
       "4        -0.000060   -0.000070   -0.000034   -0.000045  \n",
       "...            ...         ...         ...         ...  \n",
       "599995   -0.000030   -0.000021   -0.000026   -0.000049  \n",
       "599996   -0.000026   -0.000020   -0.000012   -0.000044  \n",
       "599997   -0.000012   -0.000011   -0.000006   -0.000035  \n",
       "599998   -0.000023   -0.000027   -0.000024   -0.000048  \n",
       "599999   -0.000018   -0.000025   -0.000011   -0.000034  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87757/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_87757/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_87757/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000005   0.000002   0.000037   0.000039   0.000030   0.000026   \n",
       "1       -0.000012  -0.000024   0.000001  -0.000002  -0.000015  -0.000022   \n",
       "2       -0.000077  -0.000078  -0.000059  -0.000065  -0.000063  -0.000055   \n",
       "3       -0.000066  -0.000067  -0.000050  -0.000065  -0.000060  -0.000055   \n",
       "4       -0.000045  -0.000055  -0.000033  -0.000053  -0.000054  -0.000063   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995  -0.000034  -0.000018  -0.000025  -0.000034  -0.000024  -0.000009   \n",
       "599996  -0.000052  -0.000042  -0.000040  -0.000046  -0.000043  -0.000022   \n",
       "599997  -0.000049  -0.000026  -0.000033  -0.000034  -0.000021  -0.000008   \n",
       "599998  -0.000036  -0.000033  -0.000033  -0.000035  -0.000031  -0.000016   \n",
       "599999  -0.000054  -0.000047  -0.000050  -0.000051  -0.000044  -0.000031   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000016  -0.000014   0.000004   0.000018  ...   -0.000021   \n",
       "1       -0.000055  -0.000036  -0.000027  -0.000025  ...   -0.000050   \n",
       "2       -0.000067  -0.000088  -0.000071  -0.000065  ...   -0.000017   \n",
       "3       -0.000068  -0.000062  -0.000053  -0.000054  ...   -0.000039   \n",
       "4       -0.000083  -0.000052  -0.000050  -0.000053  ...   -0.000044   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995  -0.000008  -0.000034  -0.000037  -0.000030  ...   -0.000021   \n",
       "599996  -0.000021  -0.000047  -0.000048  -0.000042  ...   -0.000005   \n",
       "599997  -0.000006  -0.000027  -0.000040  -0.000032  ...   -0.000002   \n",
       "599998  -0.000014  -0.000040  -0.000038  -0.000034  ...   -0.000023   \n",
       "599999  -0.000021  -0.000041  -0.000052  -0.000045  ...   -0.000008   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000008   -0.000035   -0.000045   -0.000066   -0.000039   \n",
       "1        -0.000040   -0.000068   -0.000065   -0.000084   -0.000052   \n",
       "2        -0.000022   -0.000050   -0.000035   -0.000048   -0.000018   \n",
       "3        -0.000060   -0.000078   -0.000064   -0.000068   -0.000041   \n",
       "4        -0.000055   -0.000070   -0.000054   -0.000063   -0.000037   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995   -0.000024   -0.000015   -0.000025   -0.000026   -0.000031   \n",
       "599996   -0.000017   -0.000006   -0.000012   -0.000011   -0.000013   \n",
       "599997   -0.000005    0.000006   -0.000005   -0.000006   -0.000008   \n",
       "599998   -0.000018   -0.000011   -0.000019   -0.000027   -0.000029   \n",
       "599999   -0.000013    0.000001   -0.000005   -0.000013   -0.000012   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000033   -0.000048   -0.000039   -0.000039  \n",
       "1        -0.000021   -0.000042   -0.000031   -0.000034  \n",
       "2        -0.000020   -0.000042   -0.000029   -0.000027  \n",
       "3        -0.000044   -0.000062   -0.000034   -0.000043  \n",
       "4        -0.000060   -0.000070   -0.000034   -0.000045  \n",
       "...            ...         ...         ...         ...  \n",
       "599995   -0.000030   -0.000021   -0.000026   -0.000049  \n",
       "599996   -0.000026   -0.000020   -0.000012   -0.000044  \n",
       "599997   -0.000012   -0.000011   -0.000006   -0.000035  \n",
       "599998   -0.000023   -0.000027   -0.000024   -0.000048  \n",
       "599999   -0.000018   -0.000025   -0.000011   -0.000034  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.66385\n",
      "[1]\tvalidation_0-mlogloss:3.54165\n",
      "[2]\tvalidation_0-mlogloss:3.45725\n",
      "[3]\tvalidation_0-mlogloss:3.38970\n",
      "[4]\tvalidation_0-mlogloss:3.33535\n",
      "[5]\tvalidation_0-mlogloss:3.28936\n",
      "[6]\tvalidation_0-mlogloss:3.25261\n",
      "[7]\tvalidation_0-mlogloss:3.21989\n",
      "[8]\tvalidation_0-mlogloss:3.18895\n",
      "[9]\tvalidation_0-mlogloss:3.16280\n",
      "[10]\tvalidation_0-mlogloss:3.13973\n",
      "[11]\tvalidation_0-mlogloss:3.11652\n",
      "[12]\tvalidation_0-mlogloss:3.10106\n",
      "[13]\tvalidation_0-mlogloss:3.08421\n",
      "[14]\tvalidation_0-mlogloss:3.06851\n",
      "[15]\tvalidation_0-mlogloss:3.05332\n",
      "[16]\tvalidation_0-mlogloss:3.03729\n",
      "[17]\tvalidation_0-mlogloss:3.02422\n",
      "[18]\tvalidation_0-mlogloss:3.01233\n",
      "[19]\tvalidation_0-mlogloss:3.00190\n",
      "[20]\tvalidation_0-mlogloss:2.99237\n",
      "[21]\tvalidation_0-mlogloss:2.98119\n",
      "[22]\tvalidation_0-mlogloss:2.97078\n",
      "[23]\tvalidation_0-mlogloss:2.96282\n",
      "[24]\tvalidation_0-mlogloss:2.95411\n",
      "[25]\tvalidation_0-mlogloss:2.94424\n",
      "[26]\tvalidation_0-mlogloss:2.93548\n",
      "[27]\tvalidation_0-mlogloss:2.92371\n",
      "[28]\tvalidation_0-mlogloss:2.91491\n",
      "[29]\tvalidation_0-mlogloss:2.90724\n",
      "[30]\tvalidation_0-mlogloss:2.90161\n",
      "[31]\tvalidation_0-mlogloss:2.89215\n",
      "[32]\tvalidation_0-mlogloss:2.88637\n",
      "[33]\tvalidation_0-mlogloss:2.87531\n",
      "[34]\tvalidation_0-mlogloss:2.86844\n",
      "[35]\tvalidation_0-mlogloss:2.85992\n",
      "[36]\tvalidation_0-mlogloss:2.85535\n",
      "[37]\tvalidation_0-mlogloss:2.84921\n",
      "[38]\tvalidation_0-mlogloss:2.84095\n",
      "[39]\tvalidation_0-mlogloss:2.83332\n",
      "[40]\tvalidation_0-mlogloss:2.82699\n",
      "[41]\tvalidation_0-mlogloss:2.82152\n",
      "[42]\tvalidation_0-mlogloss:2.81426\n",
      "[43]\tvalidation_0-mlogloss:2.80827\n",
      "[44]\tvalidation_0-mlogloss:2.80014\n",
      "[45]\tvalidation_0-mlogloss:2.79406\n",
      "[46]\tvalidation_0-mlogloss:2.78951\n",
      "[47]\tvalidation_0-mlogloss:2.78454\n",
      "[48]\tvalidation_0-mlogloss:2.77827\n",
      "[49]\tvalidation_0-mlogloss:2.77501\n",
      "[50]\tvalidation_0-mlogloss:2.77001\n",
      "[51]\tvalidation_0-mlogloss:2.76619\n",
      "[52]\tvalidation_0-mlogloss:2.76135\n",
      "[53]\tvalidation_0-mlogloss:2.75553\n",
      "[54]\tvalidation_0-mlogloss:2.75106\n",
      "[55]\tvalidation_0-mlogloss:2.74769\n",
      "[56]\tvalidation_0-mlogloss:2.74209\n",
      "[57]\tvalidation_0-mlogloss:2.73995\n",
      "[58]\tvalidation_0-mlogloss:2.73567\n",
      "[59]\tvalidation_0-mlogloss:2.73152\n",
      "[60]\tvalidation_0-mlogloss:2.72599\n",
      "[61]\tvalidation_0-mlogloss:2.72390\n",
      "[62]\tvalidation_0-mlogloss:2.71871\n",
      "[63]\tvalidation_0-mlogloss:2.71459\n",
      "[64]\tvalidation_0-mlogloss:2.71082\n",
      "[65]\tvalidation_0-mlogloss:2.70746\n",
      "[66]\tvalidation_0-mlogloss:2.70313\n",
      "[67]\tvalidation_0-mlogloss:2.69969\n",
      "[68]\tvalidation_0-mlogloss:2.69657\n",
      "[69]\tvalidation_0-mlogloss:2.69372\n",
      "[70]\tvalidation_0-mlogloss:2.69064\n",
      "[71]\tvalidation_0-mlogloss:2.68915\n",
      "[72]\tvalidation_0-mlogloss:2.68533\n",
      "[73]\tvalidation_0-mlogloss:2.68291\n",
      "[74]\tvalidation_0-mlogloss:2.67930\n",
      "[75]\tvalidation_0-mlogloss:2.67639\n",
      "[76]\tvalidation_0-mlogloss:2.67532\n",
      "[77]\tvalidation_0-mlogloss:2.67206\n",
      "[78]\tvalidation_0-mlogloss:2.66919\n",
      "[79]\tvalidation_0-mlogloss:2.66675\n",
      "[80]\tvalidation_0-mlogloss:2.66511\n",
      "[81]\tvalidation_0-mlogloss:2.66204\n",
      "[82]\tvalidation_0-mlogloss:2.65888\n",
      "[83]\tvalidation_0-mlogloss:2.65742\n",
      "[84]\tvalidation_0-mlogloss:2.65328\n",
      "[85]\tvalidation_0-mlogloss:2.64954\n",
      "[86]\tvalidation_0-mlogloss:2.64695\n",
      "[87]\tvalidation_0-mlogloss:2.64279\n",
      "[88]\tvalidation_0-mlogloss:2.64018\n",
      "[89]\tvalidation_0-mlogloss:2.63830\n",
      "[90]\tvalidation_0-mlogloss:2.63518\n",
      "[91]\tvalidation_0-mlogloss:2.63076\n",
      "[92]\tvalidation_0-mlogloss:2.62655\n",
      "[93]\tvalidation_0-mlogloss:2.62406\n",
      "[94]\tvalidation_0-mlogloss:2.62143\n",
      "[95]\tvalidation_0-mlogloss:2.61973\n",
      "[96]\tvalidation_0-mlogloss:2.61639\n",
      "[97]\tvalidation_0-mlogloss:2.61337\n",
      "[98]\tvalidation_0-mlogloss:2.61037\n",
      "[99]\tvalidation_0-mlogloss:2.60832\n",
      "[100]\tvalidation_0-mlogloss:2.60681\n",
      "[101]\tvalidation_0-mlogloss:2.60419\n",
      "[102]\tvalidation_0-mlogloss:2.60188\n",
      "[103]\tvalidation_0-mlogloss:2.59901\n",
      "[104]\tvalidation_0-mlogloss:2.59596\n",
      "[105]\tvalidation_0-mlogloss:2.59424\n",
      "[106]\tvalidation_0-mlogloss:2.59233\n",
      "[107]\tvalidation_0-mlogloss:2.58967\n",
      "[108]\tvalidation_0-mlogloss:2.58846\n",
      "[109]\tvalidation_0-mlogloss:2.58592\n",
      "[110]\tvalidation_0-mlogloss:2.58331\n",
      "[111]\tvalidation_0-mlogloss:2.58179\n",
      "[112]\tvalidation_0-mlogloss:2.57989\n",
      "[113]\tvalidation_0-mlogloss:2.57822\n",
      "[114]\tvalidation_0-mlogloss:2.57641\n",
      "[115]\tvalidation_0-mlogloss:2.57477\n",
      "[116]\tvalidation_0-mlogloss:2.57354\n",
      "[117]\tvalidation_0-mlogloss:2.57189\n",
      "[118]\tvalidation_0-mlogloss:2.57015\n",
      "[119]\tvalidation_0-mlogloss:2.56927\n",
      "[120]\tvalidation_0-mlogloss:2.56703\n",
      "[121]\tvalidation_0-mlogloss:2.56484\n",
      "[122]\tvalidation_0-mlogloss:2.56292\n",
      "[123]\tvalidation_0-mlogloss:2.56314\n",
      "[124]\tvalidation_0-mlogloss:2.56104\n",
      "[125]\tvalidation_0-mlogloss:2.55942\n",
      "[126]\tvalidation_0-mlogloss:2.55844\n",
      "[127]\tvalidation_0-mlogloss:2.55780\n",
      "[128]\tvalidation_0-mlogloss:2.55667\n",
      "[129]\tvalidation_0-mlogloss:2.55474\n",
      "[130]\tvalidation_0-mlogloss:2.55338\n",
      "[131]\tvalidation_0-mlogloss:2.55133\n",
      "[132]\tvalidation_0-mlogloss:2.54987\n",
      "[133]\tvalidation_0-mlogloss:2.54877\n",
      "[134]\tvalidation_0-mlogloss:2.54725\n",
      "[135]\tvalidation_0-mlogloss:2.54560\n",
      "[136]\tvalidation_0-mlogloss:2.54343\n",
      "[137]\tvalidation_0-mlogloss:2.54171\n",
      "[138]\tvalidation_0-mlogloss:2.54073\n",
      "[139]\tvalidation_0-mlogloss:2.53943\n",
      "[140]\tvalidation_0-mlogloss:2.53803\n",
      "[141]\tvalidation_0-mlogloss:2.53690\n",
      "[142]\tvalidation_0-mlogloss:2.53551\n",
      "[143]\tvalidation_0-mlogloss:2.53377\n",
      "[144]\tvalidation_0-mlogloss:2.53225\n",
      "[145]\tvalidation_0-mlogloss:2.53163\n",
      "[146]\tvalidation_0-mlogloss:2.53064\n",
      "[147]\tvalidation_0-mlogloss:2.52940\n",
      "[148]\tvalidation_0-mlogloss:2.52788\n",
      "[149]\tvalidation_0-mlogloss:2.52649\n",
      "[150]\tvalidation_0-mlogloss:2.52511\n",
      "[151]\tvalidation_0-mlogloss:2.52483\n",
      "[152]\tvalidation_0-mlogloss:2.52370\n",
      "[153]\tvalidation_0-mlogloss:2.52227\n",
      "[154]\tvalidation_0-mlogloss:2.52156\n",
      "[155]\tvalidation_0-mlogloss:2.52041\n",
      "[156]\tvalidation_0-mlogloss:2.52020\n",
      "[157]\tvalidation_0-mlogloss:2.51918\n",
      "[158]\tvalidation_0-mlogloss:2.51747\n",
      "[159]\tvalidation_0-mlogloss:2.51643\n",
      "[160]\tvalidation_0-mlogloss:2.51562\n",
      "[161]\tvalidation_0-mlogloss:2.51479\n",
      "[162]\tvalidation_0-mlogloss:2.51384\n",
      "[163]\tvalidation_0-mlogloss:2.51485\n",
      "[164]\tvalidation_0-mlogloss:2.51360\n",
      "[165]\tvalidation_0-mlogloss:2.51299\n",
      "[166]\tvalidation_0-mlogloss:2.51196\n",
      "[167]\tvalidation_0-mlogloss:2.51077\n",
      "[168]\tvalidation_0-mlogloss:2.50948\n",
      "[169]\tvalidation_0-mlogloss:2.50884\n",
      "[170]\tvalidation_0-mlogloss:2.50892\n",
      "[171]\tvalidation_0-mlogloss:2.50861\n",
      "[172]\tvalidation_0-mlogloss:2.50760\n",
      "[173]\tvalidation_0-mlogloss:2.50835\n",
      "[174]\tvalidation_0-mlogloss:2.50737\n",
      "[175]\tvalidation_0-mlogloss:2.50651\n",
      "[176]\tvalidation_0-mlogloss:2.50592\n",
      "[177]\tvalidation_0-mlogloss:2.50540\n",
      "[178]\tvalidation_0-mlogloss:2.50449\n",
      "[179]\tvalidation_0-mlogloss:2.50295\n",
      "[180]\tvalidation_0-mlogloss:2.50241\n",
      "[181]\tvalidation_0-mlogloss:2.50145\n",
      "[182]\tvalidation_0-mlogloss:2.50060\n",
      "[183]\tvalidation_0-mlogloss:2.49988\n",
      "[184]\tvalidation_0-mlogloss:2.49917\n",
      "[185]\tvalidation_0-mlogloss:2.49851\n",
      "[186]\tvalidation_0-mlogloss:2.49741\n",
      "[187]\tvalidation_0-mlogloss:2.49792\n",
      "[188]\tvalidation_0-mlogloss:2.49700\n",
      "[189]\tvalidation_0-mlogloss:2.49656\n",
      "[190]\tvalidation_0-mlogloss:2.49516\n",
      "[191]\tvalidation_0-mlogloss:2.49436\n",
      "[192]\tvalidation_0-mlogloss:2.49304\n",
      "[193]\tvalidation_0-mlogloss:2.49252\n",
      "[194]\tvalidation_0-mlogloss:2.49181\n",
      "[195]\tvalidation_0-mlogloss:2.49080\n",
      "[196]\tvalidation_0-mlogloss:2.49002\n",
      "[197]\tvalidation_0-mlogloss:2.48898\n",
      "[198]\tvalidation_0-mlogloss:2.48772\n",
      "[199]\tvalidation_0-mlogloss:2.48660\n",
      "[200]\tvalidation_0-mlogloss:2.48586\n",
      "[201]\tvalidation_0-mlogloss:2.48552\n",
      "[202]\tvalidation_0-mlogloss:2.48487\n",
      "[203]\tvalidation_0-mlogloss:2.48323\n",
      "[204]\tvalidation_0-mlogloss:2.48308\n",
      "[205]\tvalidation_0-mlogloss:2.48266\n",
      "[206]\tvalidation_0-mlogloss:2.48179\n",
      "[207]\tvalidation_0-mlogloss:2.48193\n",
      "[208]\tvalidation_0-mlogloss:2.48141\n",
      "[209]\tvalidation_0-mlogloss:2.48058\n",
      "[210]\tvalidation_0-mlogloss:2.48036\n",
      "[211]\tvalidation_0-mlogloss:2.47984\n",
      "[212]\tvalidation_0-mlogloss:2.48024\n",
      "[213]\tvalidation_0-mlogloss:2.47976\n",
      "[214]\tvalidation_0-mlogloss:2.48107\n",
      "[215]\tvalidation_0-mlogloss:2.48092\n",
      "[216]\tvalidation_0-mlogloss:2.48126\n",
      "[217]\tvalidation_0-mlogloss:2.48008\n",
      "[218]\tvalidation_0-mlogloss:2.47904\n",
      "[219]\tvalidation_0-mlogloss:2.48014\n",
      "[220]\tvalidation_0-mlogloss:2.48110\n",
      "[221]\tvalidation_0-mlogloss:2.48059\n",
      "[222]\tvalidation_0-mlogloss:2.48057\n",
      "[223]\tvalidation_0-mlogloss:2.48007\n",
      "[224]\tvalidation_0-mlogloss:2.47936\n",
      "[225]\tvalidation_0-mlogloss:2.47856\n",
      "[226]\tvalidation_0-mlogloss:2.47844\n",
      "[227]\tvalidation_0-mlogloss:2.47876\n",
      "[228]\tvalidation_0-mlogloss:2.47822\n",
      "[229]\tvalidation_0-mlogloss:2.47717\n",
      "[230]\tvalidation_0-mlogloss:2.47712\n",
      "[231]\tvalidation_0-mlogloss:2.47704\n",
      "[232]\tvalidation_0-mlogloss:2.47657\n",
      "[233]\tvalidation_0-mlogloss:2.47481\n",
      "[234]\tvalidation_0-mlogloss:2.47534\n",
      "[235]\tvalidation_0-mlogloss:2.47567\n",
      "[236]\tvalidation_0-mlogloss:2.47618\n",
      "[237]\tvalidation_0-mlogloss:2.47590\n",
      "[238]\tvalidation_0-mlogloss:2.47556\n",
      "[239]\tvalidation_0-mlogloss:2.47478\n",
      "[240]\tvalidation_0-mlogloss:2.47482\n",
      "[241]\tvalidation_0-mlogloss:2.47487\n",
      "[242]\tvalidation_0-mlogloss:2.47446\n",
      "[243]\tvalidation_0-mlogloss:2.47386\n",
      "[244]\tvalidation_0-mlogloss:2.47358\n",
      "[245]\tvalidation_0-mlogloss:2.47281\n",
      "[246]\tvalidation_0-mlogloss:2.47263\n",
      "[247]\tvalidation_0-mlogloss:2.47240\n",
      "[248]\tvalidation_0-mlogloss:2.47232\n",
      "[249]\tvalidation_0-mlogloss:2.47172\n",
      "[250]\tvalidation_0-mlogloss:2.47133\n",
      "[251]\tvalidation_0-mlogloss:2.47103\n",
      "[252]\tvalidation_0-mlogloss:2.47079\n",
      "[253]\tvalidation_0-mlogloss:2.47033\n",
      "[254]\tvalidation_0-mlogloss:2.46983\n",
      "[255]\tvalidation_0-mlogloss:2.46874\n",
      "[256]\tvalidation_0-mlogloss:2.46815\n",
      "[257]\tvalidation_0-mlogloss:2.46793\n",
      "[258]\tvalidation_0-mlogloss:2.46847\n",
      "[259]\tvalidation_0-mlogloss:2.46882\n",
      "[260]\tvalidation_0-mlogloss:2.46884\n",
      "[261]\tvalidation_0-mlogloss:2.46842\n",
      "[262]\tvalidation_0-mlogloss:2.46840\n",
      "[263]\tvalidation_0-mlogloss:2.46833\n",
      "[264]\tvalidation_0-mlogloss:2.46824\n",
      "[265]\tvalidation_0-mlogloss:2.46788\n",
      "[266]\tvalidation_0-mlogloss:2.46782\n",
      "[267]\tvalidation_0-mlogloss:2.46802\n",
      "[268]\tvalidation_0-mlogloss:2.46795\n",
      "[269]\tvalidation_0-mlogloss:2.46717\n",
      "[270]\tvalidation_0-mlogloss:2.46725\n",
      "[271]\tvalidation_0-mlogloss:2.46864\n",
      "[272]\tvalidation_0-mlogloss:2.46792\n",
      "[273]\tvalidation_0-mlogloss:2.46783\n",
      "[274]\tvalidation_0-mlogloss:2.46706\n",
      "[275]\tvalidation_0-mlogloss:2.46704\n",
      "[276]\tvalidation_0-mlogloss:2.46651\n",
      "[277]\tvalidation_0-mlogloss:2.46597\n",
      "[278]\tvalidation_0-mlogloss:2.46612\n",
      "[279]\tvalidation_0-mlogloss:2.46611\n",
      "[280]\tvalidation_0-mlogloss:2.46589\n",
      "[281]\tvalidation_0-mlogloss:2.46519\n",
      "[282]\tvalidation_0-mlogloss:2.46521\n",
      "[283]\tvalidation_0-mlogloss:2.46587\n",
      "[284]\tvalidation_0-mlogloss:2.46562\n",
      "[285]\tvalidation_0-mlogloss:2.46494\n",
      "[286]\tvalidation_0-mlogloss:2.46584\n",
      "[287]\tvalidation_0-mlogloss:2.46545\n",
      "[288]\tvalidation_0-mlogloss:2.46565\n",
      "[289]\tvalidation_0-mlogloss:2.46538\n",
      "[290]\tvalidation_0-mlogloss:2.46504\n",
      "[291]\tvalidation_0-mlogloss:2.46534\n",
      "[292]\tvalidation_0-mlogloss:2.46551\n",
      "[293]\tvalidation_0-mlogloss:2.46517\n",
      "[294]\tvalidation_0-mlogloss:2.46579\n",
      "[295]\tvalidation_0-mlogloss:2.46535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.21      0.22      1499\n",
      "           1       0.21      0.31      0.25      1499\n",
      "           2       0.34      0.34      0.34      1499\n",
      "           3       0.46      0.44      0.45      1499\n",
      "           4       0.70      0.75      0.72      1499\n",
      "           5       0.31      0.36      0.33      1499\n",
      "           6       0.33      0.57      0.42      1499\n",
      "           7       0.41      0.36      0.38      1499\n",
      "           8       0.33      0.44      0.38      1499\n",
      "           9       0.58      0.54      0.56      1499\n",
      "          10       0.20      0.20      0.20      1499\n",
      "          11       0.16      0.08      0.11      1499\n",
      "          12       0.13      0.07      0.09      1499\n",
      "          13       0.17      0.10      0.13      1499\n",
      "          14       0.19      0.09      0.12      1499\n",
      "          15       0.30      0.30      0.30      1499\n",
      "          16       0.32      0.29      0.30      1499\n",
      "          17       0.39      0.56      0.46      1499\n",
      "          18       0.15      0.07      0.09      1499\n",
      "          19       0.18      0.19      0.18      1499\n",
      "          20       0.58      0.90      0.70      1499\n",
      "          21       0.34      0.35      0.34      1499\n",
      "          22       0.24      0.16      0.19      1499\n",
      "          23       0.56      0.65      0.60      1499\n",
      "          24       0.25      0.24      0.25      1499\n",
      "          25       0.34      0.40      0.37      1499\n",
      "          26       0.32      0.27      0.29      1499\n",
      "          27       0.18      0.18      0.18      1499\n",
      "          28       0.23      0.20      0.21      1499\n",
      "          29       0.30      0.40      0.34      1499\n",
      "          30       0.25      0.28      0.26      1499\n",
      "          31       0.36      0.38      0.37      1499\n",
      "          32       0.16      0.11      0.13      1499\n",
      "          33       0.08      0.05      0.06      1499\n",
      "          34       0.65      0.69      0.67      1499\n",
      "          35       0.45      0.55      0.49      1499\n",
      "          36       0.38      0.45      0.41      1499\n",
      "          37       0.21      0.20      0.21      1499\n",
      "          38       0.30      0.27      0.29      1499\n",
      "          39       0.47      0.33      0.39      1499\n",
      "          40       0.30      0.37      0.33      1499\n",
      "          41       0.46      0.62      0.53      1499\n",
      "          42       0.29      0.19      0.23      1499\n",
      "          43       0.34      0.32      0.33      1499\n",
      "          44       0.44      0.52      0.47      1499\n",
      "          45       0.20      0.18      0.19      1499\n",
      "          46       0.33      0.38      0.36      1499\n",
      "          47       0.48      0.49      0.49      1499\n",
      "          48       0.53      0.70      0.61      1499\n",
      "          49       0.26      0.16      0.19      1499\n",
      "\n",
      "    accuracy                           0.35     74950\n",
      "   macro avg       0.33      0.35      0.33     74950\n",
      "weighted avg       0.33      0.35      0.33     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b271d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 33s 2ms/step - loss: 2.3773 - val_loss: 2.5539\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 32s 2ms/step - loss: 1.8357 - val_loss: 2.4673\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 32s 2ms/step - loss: 1.7162 - val_loss: 2.6008\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 32s 2ms/step - loss: 1.6626 - val_loss: 2.5154\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 32s 2ms/step - loss: 1.6293 - val_loss: 2.5408\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 32s 2ms/step - loss: 1.6032 - val_loss: 2.5498\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 35s 2ms/step - loss: 1.5831 - val_loss: 2.6097\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 33s 2ms/step - loss: 1.5696 - val_loss: 2.5869\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 42s 2ms/step - loss: 1.5555 - val_loss: 2.6296\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 55s 3ms/step - loss: 1.5440 - val_loss: 2.6066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe347932680>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  34/2343 [..............................] - ETA: 3s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 4s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.24      0.24      1499\n",
      "           1       0.22      0.37      0.28      1499\n",
      "           2       0.26      0.44      0.33      1499\n",
      "           3       0.42      0.48      0.45      1499\n",
      "           4       0.77      0.69      0.73      1499\n",
      "           5       0.33      0.25      0.29      1499\n",
      "           6       0.21      0.56      0.31      1499\n",
      "           7       0.41      0.26      0.32      1499\n",
      "           8       0.37      0.34      0.35      1499\n",
      "           9       0.56      0.61      0.58      1499\n",
      "          10       0.19      0.13      0.16      1499\n",
      "          11       0.13      0.36      0.19      1499\n",
      "          12       0.12      0.04      0.05      1499\n",
      "          13       0.19      0.11      0.14      1499\n",
      "          14       0.20      0.09      0.13      1499\n",
      "          15       0.33      0.37      0.35      1499\n",
      "          16       0.39      0.22      0.28      1499\n",
      "          17       0.36      0.14      0.21      1499\n",
      "          18       0.13      0.08      0.10      1499\n",
      "          19       0.20      0.08      0.12      1499\n",
      "          20       0.59      0.77      0.67      1499\n",
      "          21       0.28      0.49      0.36      1499\n",
      "          22       0.23      0.19      0.21      1499\n",
      "          23       0.55      0.68      0.61      1499\n",
      "          24       0.19      0.16      0.18      1499\n",
      "          25       0.38      0.29      0.33      1499\n",
      "          26       0.37      0.30      0.33      1499\n",
      "          27       0.15      0.16      0.16      1499\n",
      "          28       0.18      0.13      0.15      1499\n",
      "          29       0.26      0.26      0.26      1499\n",
      "          30       0.22      0.32      0.26      1499\n",
      "          31       0.39      0.34      0.36      1499\n",
      "          32       0.16      0.12      0.14      1499\n",
      "          33       0.08      0.02      0.04      1499\n",
      "          34       0.73      0.36      0.48      1499\n",
      "          35       0.58      0.49      0.53      1499\n",
      "          36       0.36      0.39      0.37      1499\n",
      "          37       0.17      0.28      0.21      1499\n",
      "          38       0.27      0.35      0.31      1499\n",
      "          39       0.42      0.28      0.34      1499\n",
      "          40       0.34      0.13      0.19      1499\n",
      "          41       0.55      0.54      0.55      1499\n",
      "          42       0.31      0.24      0.27      1499\n",
      "          43       0.39      0.35      0.37      1499\n",
      "          44       0.41      0.49      0.45      1499\n",
      "          45       0.14      0.16      0.15      1499\n",
      "          46       0.33      0.38      0.35      1499\n",
      "          47       0.52      0.53      0.52      1499\n",
      "          48       0.59      0.74      0.65      1499\n",
      "          49       0.21      0.23      0.22      1499\n",
      "\n",
      "    accuracy                           0.32     74950\n",
      "   macro avg       0.33      0.32      0.31     74950\n",
      "weighted avg       0.33      0.32      0.31     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87757/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_87757/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_87757/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000005   0.000002   0.000037   0.000039   0.000030   0.000026   \n",
       "1       -0.000012  -0.000024   0.000001  -0.000002  -0.000015  -0.000022   \n",
       "2       -0.000077  -0.000078  -0.000059  -0.000065  -0.000063  -0.000055   \n",
       "3       -0.000066  -0.000067  -0.000050  -0.000065  -0.000060  -0.000055   \n",
       "4       -0.000045  -0.000055  -0.000033  -0.000053  -0.000054  -0.000063   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995  -0.000034  -0.000018  -0.000025  -0.000034  -0.000024  -0.000009   \n",
       "599996  -0.000052  -0.000042  -0.000040  -0.000046  -0.000043  -0.000022   \n",
       "599997  -0.000049  -0.000026  -0.000033  -0.000034  -0.000021  -0.000008   \n",
       "599998  -0.000036  -0.000033  -0.000033  -0.000035  -0.000031  -0.000016   \n",
       "599999  -0.000054  -0.000047  -0.000050  -0.000051  -0.000044  -0.000031   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000016  -0.000014   0.000004   0.000018  ...   -0.000021   \n",
       "1       -0.000055  -0.000036  -0.000027  -0.000025  ...   -0.000050   \n",
       "2       -0.000067  -0.000088  -0.000071  -0.000065  ...   -0.000017   \n",
       "3       -0.000068  -0.000062  -0.000053  -0.000054  ...   -0.000039   \n",
       "4       -0.000083  -0.000052  -0.000050  -0.000053  ...   -0.000044   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995  -0.000008  -0.000034  -0.000037  -0.000030  ...   -0.000021   \n",
       "599996  -0.000021  -0.000047  -0.000048  -0.000042  ...   -0.000005   \n",
       "599997  -0.000006  -0.000027  -0.000040  -0.000032  ...   -0.000002   \n",
       "599998  -0.000014  -0.000040  -0.000038  -0.000034  ...   -0.000023   \n",
       "599999  -0.000021  -0.000041  -0.000052  -0.000045  ...   -0.000008   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000008   -0.000035   -0.000045   -0.000066   -0.000039   \n",
       "1        -0.000040   -0.000068   -0.000065   -0.000084   -0.000052   \n",
       "2        -0.000022   -0.000050   -0.000035   -0.000048   -0.000018   \n",
       "3        -0.000060   -0.000078   -0.000064   -0.000068   -0.000041   \n",
       "4        -0.000055   -0.000070   -0.000054   -0.000063   -0.000037   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995   -0.000024   -0.000015   -0.000025   -0.000026   -0.000031   \n",
       "599996   -0.000017   -0.000006   -0.000012   -0.000011   -0.000013   \n",
       "599997   -0.000005    0.000006   -0.000005   -0.000006   -0.000008   \n",
       "599998   -0.000018   -0.000011   -0.000019   -0.000027   -0.000029   \n",
       "599999   -0.000013    0.000001   -0.000005   -0.000013   -0.000012   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000033   -0.000048   -0.000039   -0.000039  \n",
       "1        -0.000021   -0.000042   -0.000031   -0.000034  \n",
       "2        -0.000020   -0.000042   -0.000029   -0.000027  \n",
       "3        -0.000044   -0.000062   -0.000034   -0.000043  \n",
       "4        -0.000060   -0.000070   -0.000034   -0.000045  \n",
       "...            ...         ...         ...         ...  \n",
       "599995   -0.000030   -0.000021   -0.000026   -0.000049  \n",
       "599996   -0.000026   -0.000020   -0.000012   -0.000044  \n",
       "599997   -0.000012   -0.000011   -0.000006   -0.000035  \n",
       "599998   -0.000023   -0.000027   -0.000024   -0.000048  \n",
       "599999   -0.000018   -0.000025   -0.000011   -0.000034  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.56163\n",
      "[1]\tvalidation_0-mlogloss:3.42411\n",
      "[2]\tvalidation_0-mlogloss:3.31813\n",
      "[3]\tvalidation_0-mlogloss:3.23333\n",
      "[4]\tvalidation_0-mlogloss:3.16410\n",
      "[5]\tvalidation_0-mlogloss:3.10846\n",
      "[6]\tvalidation_0-mlogloss:3.05473\n",
      "[7]\tvalidation_0-mlogloss:3.00951\n",
      "[8]\tvalidation_0-mlogloss:2.96933\n",
      "[9]\tvalidation_0-mlogloss:2.93859\n",
      "[10]\tvalidation_0-mlogloss:2.90400\n",
      "[11]\tvalidation_0-mlogloss:2.87456\n",
      "[12]\tvalidation_0-mlogloss:2.84697\n",
      "[13]\tvalidation_0-mlogloss:2.82412\n",
      "[14]\tvalidation_0-mlogloss:2.80206\n",
      "[15]\tvalidation_0-mlogloss:2.78312\n",
      "[16]\tvalidation_0-mlogloss:2.76387\n",
      "[17]\tvalidation_0-mlogloss:2.74291\n",
      "[18]\tvalidation_0-mlogloss:2.72463\n",
      "[19]\tvalidation_0-mlogloss:2.70981\n",
      "[20]\tvalidation_0-mlogloss:2.69301\n",
      "[21]\tvalidation_0-mlogloss:2.67855\n",
      "[22]\tvalidation_0-mlogloss:2.66316\n",
      "[23]\tvalidation_0-mlogloss:2.64503\n",
      "[24]\tvalidation_0-mlogloss:2.63211\n",
      "[25]\tvalidation_0-mlogloss:2.61913\n",
      "[26]\tvalidation_0-mlogloss:2.60373\n",
      "[27]\tvalidation_0-mlogloss:2.59260\n",
      "[28]\tvalidation_0-mlogloss:2.57881\n",
      "[29]\tvalidation_0-mlogloss:2.56645\n",
      "[30]\tvalidation_0-mlogloss:2.55804\n",
      "[31]\tvalidation_0-mlogloss:2.54330\n",
      "[32]\tvalidation_0-mlogloss:2.53009\n",
      "[33]\tvalidation_0-mlogloss:2.51920\n",
      "[34]\tvalidation_0-mlogloss:2.50906\n",
      "[35]\tvalidation_0-mlogloss:2.49750\n",
      "[36]\tvalidation_0-mlogloss:2.48608\n",
      "[37]\tvalidation_0-mlogloss:2.47447\n",
      "[38]\tvalidation_0-mlogloss:2.46254\n",
      "[39]\tvalidation_0-mlogloss:2.45323\n",
      "[40]\tvalidation_0-mlogloss:2.44359\n",
      "[41]\tvalidation_0-mlogloss:2.43366\n",
      "[42]\tvalidation_0-mlogloss:2.42452\n",
      "[43]\tvalidation_0-mlogloss:2.41578\n",
      "[44]\tvalidation_0-mlogloss:2.40541\n",
      "[45]\tvalidation_0-mlogloss:2.39948\n",
      "[46]\tvalidation_0-mlogloss:2.39240\n",
      "[47]\tvalidation_0-mlogloss:2.38361\n",
      "[48]\tvalidation_0-mlogloss:2.37539\n",
      "[49]\tvalidation_0-mlogloss:2.36733\n",
      "[50]\tvalidation_0-mlogloss:2.35920\n",
      "[51]\tvalidation_0-mlogloss:2.35083\n",
      "[52]\tvalidation_0-mlogloss:2.34413\n",
      "[53]\tvalidation_0-mlogloss:2.33771\n",
      "[54]\tvalidation_0-mlogloss:2.33080\n",
      "[55]\tvalidation_0-mlogloss:2.32390\n",
      "[56]\tvalidation_0-mlogloss:2.31624\n",
      "[57]\tvalidation_0-mlogloss:2.31136\n",
      "[58]\tvalidation_0-mlogloss:2.30313\n",
      "[59]\tvalidation_0-mlogloss:2.29622\n",
      "[60]\tvalidation_0-mlogloss:2.28899\n",
      "[61]\tvalidation_0-mlogloss:2.28182\n",
      "[62]\tvalidation_0-mlogloss:2.27435\n",
      "[63]\tvalidation_0-mlogloss:2.26826\n",
      "[64]\tvalidation_0-mlogloss:2.26226\n",
      "[65]\tvalidation_0-mlogloss:2.25731\n",
      "[66]\tvalidation_0-mlogloss:2.25002\n",
      "[67]\tvalidation_0-mlogloss:2.24269\n",
      "[68]\tvalidation_0-mlogloss:2.23563\n",
      "[69]\tvalidation_0-mlogloss:2.22982\n",
      "[70]\tvalidation_0-mlogloss:2.22440\n",
      "[71]\tvalidation_0-mlogloss:2.22113\n",
      "[72]\tvalidation_0-mlogloss:2.21754\n",
      "[73]\tvalidation_0-mlogloss:2.21169\n",
      "[74]\tvalidation_0-mlogloss:2.20602\n",
      "[75]\tvalidation_0-mlogloss:2.20252\n",
      "[76]\tvalidation_0-mlogloss:2.19786\n",
      "[77]\tvalidation_0-mlogloss:2.19378\n",
      "[78]\tvalidation_0-mlogloss:2.18801\n",
      "[79]\tvalidation_0-mlogloss:2.18266\n",
      "[80]\tvalidation_0-mlogloss:2.17723\n",
      "[81]\tvalidation_0-mlogloss:2.17261\n",
      "[82]\tvalidation_0-mlogloss:2.16850\n",
      "[83]\tvalidation_0-mlogloss:2.16262\n",
      "[84]\tvalidation_0-mlogloss:2.15709\n",
      "[85]\tvalidation_0-mlogloss:2.15409\n",
      "[86]\tvalidation_0-mlogloss:2.14900\n",
      "[87]\tvalidation_0-mlogloss:2.14544\n",
      "[88]\tvalidation_0-mlogloss:2.13964\n",
      "[89]\tvalidation_0-mlogloss:2.13427\n",
      "[90]\tvalidation_0-mlogloss:2.13082\n",
      "[91]\tvalidation_0-mlogloss:2.12719\n",
      "[92]\tvalidation_0-mlogloss:2.12155\n",
      "[93]\tvalidation_0-mlogloss:2.11841\n",
      "[94]\tvalidation_0-mlogloss:2.11492\n",
      "[95]\tvalidation_0-mlogloss:2.11072\n",
      "[96]\tvalidation_0-mlogloss:2.10655\n",
      "[97]\tvalidation_0-mlogloss:2.10275\n",
      "[98]\tvalidation_0-mlogloss:2.09978\n",
      "[99]\tvalidation_0-mlogloss:2.09499\n",
      "[100]\tvalidation_0-mlogloss:2.09194\n",
      "[101]\tvalidation_0-mlogloss:2.08897\n",
      "[102]\tvalidation_0-mlogloss:2.08685\n",
      "[103]\tvalidation_0-mlogloss:2.08441\n",
      "[104]\tvalidation_0-mlogloss:2.08142\n",
      "[105]\tvalidation_0-mlogloss:2.07814\n",
      "[106]\tvalidation_0-mlogloss:2.07536\n",
      "[107]\tvalidation_0-mlogloss:2.07230\n",
      "[108]\tvalidation_0-mlogloss:2.07120\n",
      "[109]\tvalidation_0-mlogloss:2.06808\n",
      "[110]\tvalidation_0-mlogloss:2.06506\n",
      "[111]\tvalidation_0-mlogloss:2.06123\n",
      "[112]\tvalidation_0-mlogloss:2.05758\n",
      "[113]\tvalidation_0-mlogloss:2.05467\n",
      "[114]\tvalidation_0-mlogloss:2.05159\n",
      "[115]\tvalidation_0-mlogloss:2.04778\n",
      "[116]\tvalidation_0-mlogloss:2.04382\n",
      "[117]\tvalidation_0-mlogloss:2.04297\n",
      "[118]\tvalidation_0-mlogloss:2.03960\n",
      "[119]\tvalidation_0-mlogloss:2.03665\n",
      "[120]\tvalidation_0-mlogloss:2.03353\n",
      "[121]\tvalidation_0-mlogloss:2.03057\n",
      "[122]\tvalidation_0-mlogloss:2.02808\n",
      "[123]\tvalidation_0-mlogloss:2.02520\n",
      "[124]\tvalidation_0-mlogloss:2.02250\n",
      "[125]\tvalidation_0-mlogloss:2.02092\n",
      "[126]\tvalidation_0-mlogloss:2.01920\n",
      "[127]\tvalidation_0-mlogloss:2.01681\n",
      "[128]\tvalidation_0-mlogloss:2.01452\n",
      "[129]\tvalidation_0-mlogloss:2.01032\n",
      "[130]\tvalidation_0-mlogloss:2.00858\n",
      "[131]\tvalidation_0-mlogloss:2.00549\n",
      "[132]\tvalidation_0-mlogloss:2.00323\n",
      "[133]\tvalidation_0-mlogloss:2.00021\n",
      "[134]\tvalidation_0-mlogloss:1.99674\n",
      "[135]\tvalidation_0-mlogloss:1.99458\n",
      "[136]\tvalidation_0-mlogloss:1.99142\n",
      "[137]\tvalidation_0-mlogloss:1.98966\n",
      "[138]\tvalidation_0-mlogloss:1.98687\n",
      "[139]\tvalidation_0-mlogloss:1.98503\n",
      "[140]\tvalidation_0-mlogloss:1.98328\n",
      "[141]\tvalidation_0-mlogloss:1.98164\n",
      "[142]\tvalidation_0-mlogloss:1.98027\n",
      "[143]\tvalidation_0-mlogloss:1.97761\n",
      "[144]\tvalidation_0-mlogloss:1.97599\n",
      "[145]\tvalidation_0-mlogloss:1.97478\n",
      "[146]\tvalidation_0-mlogloss:1.97304\n",
      "[147]\tvalidation_0-mlogloss:1.97141\n",
      "[148]\tvalidation_0-mlogloss:1.96972\n",
      "[149]\tvalidation_0-mlogloss:1.96762\n",
      "[150]\tvalidation_0-mlogloss:1.96428\n",
      "[151]\tvalidation_0-mlogloss:1.96289\n",
      "[152]\tvalidation_0-mlogloss:1.96054\n",
      "[153]\tvalidation_0-mlogloss:1.95739\n",
      "[154]\tvalidation_0-mlogloss:1.95521\n",
      "[155]\tvalidation_0-mlogloss:1.95305\n",
      "[156]\tvalidation_0-mlogloss:1.95142\n",
      "[157]\tvalidation_0-mlogloss:1.94942\n",
      "[158]\tvalidation_0-mlogloss:1.94890\n",
      "[159]\tvalidation_0-mlogloss:1.94772\n",
      "[160]\tvalidation_0-mlogloss:1.94798\n",
      "[161]\tvalidation_0-mlogloss:1.94576\n",
      "[162]\tvalidation_0-mlogloss:1.94408\n",
      "[163]\tvalidation_0-mlogloss:1.94203\n",
      "[164]\tvalidation_0-mlogloss:1.93936\n",
      "[165]\tvalidation_0-mlogloss:1.93830\n",
      "[166]\tvalidation_0-mlogloss:1.93689\n",
      "[167]\tvalidation_0-mlogloss:1.93525\n",
      "[168]\tvalidation_0-mlogloss:1.93316\n",
      "[169]\tvalidation_0-mlogloss:1.93113\n",
      "[170]\tvalidation_0-mlogloss:1.92972\n",
      "[171]\tvalidation_0-mlogloss:1.93028\n",
      "[172]\tvalidation_0-mlogloss:1.92760\n",
      "[173]\tvalidation_0-mlogloss:1.92546\n",
      "[174]\tvalidation_0-mlogloss:1.92296\n",
      "[175]\tvalidation_0-mlogloss:1.92239\n",
      "[176]\tvalidation_0-mlogloss:1.92125\n",
      "[177]\tvalidation_0-mlogloss:1.92015\n",
      "[178]\tvalidation_0-mlogloss:1.91940\n",
      "[179]\tvalidation_0-mlogloss:1.91851\n",
      "[180]\tvalidation_0-mlogloss:1.91614\n",
      "[181]\tvalidation_0-mlogloss:1.91432\n",
      "[182]\tvalidation_0-mlogloss:1.91341\n",
      "[183]\tvalidation_0-mlogloss:1.91173\n",
      "[184]\tvalidation_0-mlogloss:1.91038\n",
      "[185]\tvalidation_0-mlogloss:1.90922\n",
      "[186]\tvalidation_0-mlogloss:1.90689\n",
      "[187]\tvalidation_0-mlogloss:1.90544\n",
      "[188]\tvalidation_0-mlogloss:1.90404\n",
      "[189]\tvalidation_0-mlogloss:1.90272\n",
      "[190]\tvalidation_0-mlogloss:1.90147\n",
      "[191]\tvalidation_0-mlogloss:1.89991\n",
      "[192]\tvalidation_0-mlogloss:1.89900\n",
      "[193]\tvalidation_0-mlogloss:1.89762\n",
      "[194]\tvalidation_0-mlogloss:1.89645\n",
      "[195]\tvalidation_0-mlogloss:1.89533\n",
      "[196]\tvalidation_0-mlogloss:1.89350\n",
      "[197]\tvalidation_0-mlogloss:1.89266\n",
      "[198]\tvalidation_0-mlogloss:1.89207\n",
      "[199]\tvalidation_0-mlogloss:1.89044\n",
      "[200]\tvalidation_0-mlogloss:1.88940\n",
      "[201]\tvalidation_0-mlogloss:1.88856\n",
      "[202]\tvalidation_0-mlogloss:1.88715\n",
      "[203]\tvalidation_0-mlogloss:1.88596\n",
      "[204]\tvalidation_0-mlogloss:1.88514\n",
      "[205]\tvalidation_0-mlogloss:1.88417\n",
      "[206]\tvalidation_0-mlogloss:1.88343\n",
      "[207]\tvalidation_0-mlogloss:1.88278\n",
      "[208]\tvalidation_0-mlogloss:1.88221\n",
      "[209]\tvalidation_0-mlogloss:1.88026\n",
      "[210]\tvalidation_0-mlogloss:1.87800\n",
      "[211]\tvalidation_0-mlogloss:1.87619\n",
      "[212]\tvalidation_0-mlogloss:1.87501\n",
      "[213]\tvalidation_0-mlogloss:1.87325\n",
      "[214]\tvalidation_0-mlogloss:1.87141\n",
      "[215]\tvalidation_0-mlogloss:1.87067\n",
      "[216]\tvalidation_0-mlogloss:1.87000\n",
      "[217]\tvalidation_0-mlogloss:1.86859\n",
      "[218]\tvalidation_0-mlogloss:1.86745\n",
      "[219]\tvalidation_0-mlogloss:1.86573\n",
      "[220]\tvalidation_0-mlogloss:1.86427\n",
      "[221]\tvalidation_0-mlogloss:1.86356\n",
      "[222]\tvalidation_0-mlogloss:1.86152\n",
      "[223]\tvalidation_0-mlogloss:1.86153\n",
      "[224]\tvalidation_0-mlogloss:1.86134\n",
      "[225]\tvalidation_0-mlogloss:1.86089\n",
      "[226]\tvalidation_0-mlogloss:1.86011\n",
      "[227]\tvalidation_0-mlogloss:1.85901\n",
      "[228]\tvalidation_0-mlogloss:1.85782\n",
      "[229]\tvalidation_0-mlogloss:1.85798\n",
      "[230]\tvalidation_0-mlogloss:1.85625\n",
      "[231]\tvalidation_0-mlogloss:1.85570\n",
      "[232]\tvalidation_0-mlogloss:1.85371\n",
      "[233]\tvalidation_0-mlogloss:1.85332\n",
      "[234]\tvalidation_0-mlogloss:1.85307\n",
      "[235]\tvalidation_0-mlogloss:1.85245\n",
      "[236]\tvalidation_0-mlogloss:1.85150\n",
      "[237]\tvalidation_0-mlogloss:1.85081\n",
      "[238]\tvalidation_0-mlogloss:1.84956\n",
      "[239]\tvalidation_0-mlogloss:1.84935\n",
      "[240]\tvalidation_0-mlogloss:1.84874\n",
      "[241]\tvalidation_0-mlogloss:1.84748\n",
      "[242]\tvalidation_0-mlogloss:1.84670\n",
      "[243]\tvalidation_0-mlogloss:1.84489\n",
      "[244]\tvalidation_0-mlogloss:1.84356\n",
      "[245]\tvalidation_0-mlogloss:1.84258\n",
      "[246]\tvalidation_0-mlogloss:1.84226\n",
      "[247]\tvalidation_0-mlogloss:1.84176\n",
      "[248]\tvalidation_0-mlogloss:1.84061\n",
      "[249]\tvalidation_0-mlogloss:1.83970\n",
      "[250]\tvalidation_0-mlogloss:1.83868\n",
      "[251]\tvalidation_0-mlogloss:1.83809\n",
      "[252]\tvalidation_0-mlogloss:1.83753\n",
      "[253]\tvalidation_0-mlogloss:1.83682\n",
      "[254]\tvalidation_0-mlogloss:1.83624\n",
      "[255]\tvalidation_0-mlogloss:1.83538\n",
      "[256]\tvalidation_0-mlogloss:1.83408\n",
      "[257]\tvalidation_0-mlogloss:1.83336\n",
      "[258]\tvalidation_0-mlogloss:1.83221\n",
      "[259]\tvalidation_0-mlogloss:1.83096\n",
      "[260]\tvalidation_0-mlogloss:1.83083\n",
      "[261]\tvalidation_0-mlogloss:1.83000\n",
      "[262]\tvalidation_0-mlogloss:1.82922\n",
      "[263]\tvalidation_0-mlogloss:1.82860\n",
      "[264]\tvalidation_0-mlogloss:1.82773\n",
      "[265]\tvalidation_0-mlogloss:1.82726\n",
      "[266]\tvalidation_0-mlogloss:1.82717\n",
      "[267]\tvalidation_0-mlogloss:1.82598\n",
      "[268]\tvalidation_0-mlogloss:1.82437\n",
      "[269]\tvalidation_0-mlogloss:1.82410\n",
      "[270]\tvalidation_0-mlogloss:1.82273\n",
      "[271]\tvalidation_0-mlogloss:1.82217\n",
      "[272]\tvalidation_0-mlogloss:1.82157\n",
      "[273]\tvalidation_0-mlogloss:1.82062\n",
      "[274]\tvalidation_0-mlogloss:1.82021\n",
      "[275]\tvalidation_0-mlogloss:1.81944\n",
      "[276]\tvalidation_0-mlogloss:1.81905\n",
      "[277]\tvalidation_0-mlogloss:1.81889\n",
      "[278]\tvalidation_0-mlogloss:1.81849\n",
      "[279]\tvalidation_0-mlogloss:1.81812\n",
      "[280]\tvalidation_0-mlogloss:1.81748\n",
      "[281]\tvalidation_0-mlogloss:1.81726\n",
      "[282]\tvalidation_0-mlogloss:1.81622\n",
      "[283]\tvalidation_0-mlogloss:1.81578\n",
      "[284]\tvalidation_0-mlogloss:1.81546\n",
      "[285]\tvalidation_0-mlogloss:1.81524\n",
      "[286]\tvalidation_0-mlogloss:1.81486\n",
      "[287]\tvalidation_0-mlogloss:1.81448\n",
      "[288]\tvalidation_0-mlogloss:1.81449\n",
      "[289]\tvalidation_0-mlogloss:1.81380\n",
      "[290]\tvalidation_0-mlogloss:1.81327\n",
      "[291]\tvalidation_0-mlogloss:1.81225\n",
      "[292]\tvalidation_0-mlogloss:1.81166\n",
      "[293]\tvalidation_0-mlogloss:1.81227\n",
      "[294]\tvalidation_0-mlogloss:1.81146\n",
      "[295]\tvalidation_0-mlogloss:1.81026\n",
      "[296]\tvalidation_0-mlogloss:1.80910\n",
      "[297]\tvalidation_0-mlogloss:1.80875\n",
      "[298]\tvalidation_0-mlogloss:1.80846\n",
      "[299]\tvalidation_0-mlogloss:1.80894\n",
      "[300]\tvalidation_0-mlogloss:1.80856\n",
      "[301]\tvalidation_0-mlogloss:1.80779\n",
      "[302]\tvalidation_0-mlogloss:1.80792\n",
      "[303]\tvalidation_0-mlogloss:1.80749\n",
      "[304]\tvalidation_0-mlogloss:1.80673\n",
      "[305]\tvalidation_0-mlogloss:1.80679\n",
      "[306]\tvalidation_0-mlogloss:1.80624\n",
      "[307]\tvalidation_0-mlogloss:1.80553\n",
      "[308]\tvalidation_0-mlogloss:1.80511\n",
      "[309]\tvalidation_0-mlogloss:1.80482\n",
      "[310]\tvalidation_0-mlogloss:1.80467\n",
      "[311]\tvalidation_0-mlogloss:1.80411\n",
      "[312]\tvalidation_0-mlogloss:1.80372\n",
      "[313]\tvalidation_0-mlogloss:1.80283\n",
      "[314]\tvalidation_0-mlogloss:1.80249\n",
      "[315]\tvalidation_0-mlogloss:1.80257\n",
      "[316]\tvalidation_0-mlogloss:1.80187\n",
      "[317]\tvalidation_0-mlogloss:1.80181\n",
      "[318]\tvalidation_0-mlogloss:1.80111\n",
      "[319]\tvalidation_0-mlogloss:1.80056\n",
      "[320]\tvalidation_0-mlogloss:1.79994\n",
      "[321]\tvalidation_0-mlogloss:1.79964\n",
      "[322]\tvalidation_0-mlogloss:1.79916\n",
      "[323]\tvalidation_0-mlogloss:1.79859\n",
      "[324]\tvalidation_0-mlogloss:1.79751\n",
      "[325]\tvalidation_0-mlogloss:1.79735\n",
      "[326]\tvalidation_0-mlogloss:1.79701\n",
      "[327]\tvalidation_0-mlogloss:1.79773\n",
      "[328]\tvalidation_0-mlogloss:1.79799\n",
      "[329]\tvalidation_0-mlogloss:1.79822\n",
      "[330]\tvalidation_0-mlogloss:1.79804\n",
      "[331]\tvalidation_0-mlogloss:1.79831\n",
      "[332]\tvalidation_0-mlogloss:1.79809\n",
      "[333]\tvalidation_0-mlogloss:1.79818\n",
      "[334]\tvalidation_0-mlogloss:1.79852\n",
      "[335]\tvalidation_0-mlogloss:1.79919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.38      0.34      1499\n",
      "           1       0.38      0.48      0.42      1499\n",
      "           2       0.46      0.52      0.49      1499\n",
      "           3       0.73      0.41      0.52      1499\n",
      "           4       0.91      0.88      0.90      1499\n",
      "           5       0.49      0.50      0.49      1499\n",
      "           6       0.64      0.79      0.71      1499\n",
      "           7       0.77      0.58      0.66      1499\n",
      "           8       0.35      0.45      0.39      1499\n",
      "           9       0.66      0.69      0.68      1499\n",
      "          10       0.40      0.21      0.28      1499\n",
      "          11       0.39      0.47      0.43      1499\n",
      "          12       0.41      0.42      0.41      1499\n",
      "          13       0.29      0.36      0.32      1499\n",
      "          14       0.28      0.25      0.26      1499\n",
      "          15       0.64      0.46      0.53      1499\n",
      "          16       0.67      0.40      0.50      1499\n",
      "          17       0.58      0.62      0.60      1499\n",
      "          18       0.45      0.48      0.46      1499\n",
      "          19       0.43      0.47      0.45      1499\n",
      "          20       0.82      0.93      0.87      1499\n",
      "          21       0.42      0.46      0.44      1499\n",
      "          22       0.38      0.31      0.34      1499\n",
      "          23       0.73      0.80      0.76      1499\n",
      "          24       0.57      0.39      0.46      1499\n",
      "          25       0.68      0.76      0.72      1499\n",
      "          26       0.42      0.35      0.38      1499\n",
      "          27       0.41      0.46      0.44      1499\n",
      "          28       0.47      0.46      0.47      1499\n",
      "          29       0.58      0.63      0.60      1499\n",
      "          30       0.40      0.51      0.45      1499\n",
      "          31       0.47      0.69      0.56      1499\n",
      "          32       0.41      0.31      0.35      1499\n",
      "          33       0.36      0.37      0.37      1499\n",
      "          34       0.83      0.90      0.86      1499\n",
      "          35       0.61      0.64      0.63      1499\n",
      "          36       0.68      0.62      0.65      1499\n",
      "          37       0.36      0.38      0.37      1499\n",
      "          38       0.65      0.36      0.47      1499\n",
      "          39       0.67      0.45      0.54      1499\n",
      "          40       0.77      0.57      0.65      1499\n",
      "          41       0.70      0.73      0.72      1499\n",
      "          42       0.38      0.33      0.35      1499\n",
      "          43       0.33      0.39      0.36      1499\n",
      "          44       0.62      0.63      0.62      1499\n",
      "          45       0.36      0.42      0.38      1499\n",
      "          46       0.51      0.71      0.59      1499\n",
      "          47       0.68      0.59      0.63      1499\n",
      "          48       0.69      0.76      0.72      1499\n",
      "          49       0.49      0.42      0.45      1499\n",
      "\n",
      "    accuracy                           0.52     74950\n",
      "   macro avg       0.53      0.52      0.52     74950\n",
      "weighted avg       0.53      0.52      0.52     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7844baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7980b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 57s 3ms/step - loss: 1.7205 - val_loss: 2.2577\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 53s 3ms/step - loss: 1.0469 - val_loss: 2.2195\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 55s 3ms/step - loss: 0.8897 - val_loss: 2.2777\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 51s 3ms/step - loss: 0.8087 - val_loss: 2.3451\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 46s 2ms/step - loss: 0.7597 - val_loss: 2.4800\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 43s 2ms/step - loss: 0.7266 - val_loss: 2.5097\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 42s 2ms/step - loss: 0.6996 - val_loss: 2.4669\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 41s 2ms/step - loss: 0.6802 - val_loss: 2.4226\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 44s 2ms/step - loss: 0.6610 - val_loss: 2.5010\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 41s 2ms/step - loss: 0.6449 - val_loss: 2.7650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe3059b7280>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 3s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.50      0.42      1499\n",
      "           1       0.44      0.49      0.46      1499\n",
      "           2       0.43      0.48      0.46      1499\n",
      "           3       0.55      0.48      0.51      1499\n",
      "           4       0.86      0.89      0.87      1499\n",
      "           5       0.65      0.34      0.44      1499\n",
      "           6       0.64      0.95      0.77      1499\n",
      "           7       0.66      0.28      0.40      1499\n",
      "           8       0.38      0.36      0.37      1499\n",
      "           9       0.71      0.70      0.71      1499\n",
      "          10       0.37      0.22      0.27      1499\n",
      "          11       0.51      0.68      0.58      1499\n",
      "          12       0.47      0.39      0.42      1499\n",
      "          13       0.23      0.42      0.30      1499\n",
      "          14       0.31      0.26      0.28      1499\n",
      "          15       0.72      0.27      0.40      1499\n",
      "          16       0.59      0.21      0.31      1499\n",
      "          17       0.80      0.31      0.44      1499\n",
      "          18       0.41      0.57      0.47      1499\n",
      "          19       0.35      0.15      0.21      1499\n",
      "          20       0.85      0.83      0.84      1499\n",
      "          21       0.31      0.53      0.39      1499\n",
      "          22       0.45      0.34      0.39      1499\n",
      "          23       0.82      0.82      0.82      1499\n",
      "          24       0.57      0.46      0.51      1499\n",
      "          25       0.80      0.77      0.78      1499\n",
      "          26       0.19      0.10      0.13      1499\n",
      "          27       0.45      0.41      0.43      1499\n",
      "          28       0.57      0.43      0.49      1499\n",
      "          29       0.59      0.64      0.61      1499\n",
      "          30       0.46      0.71      0.56      1499\n",
      "          31       0.59      0.54      0.56      1499\n",
      "          32       0.33      0.37      0.35      1499\n",
      "          33       0.34      0.20      0.25      1499\n",
      "          34       0.93      0.90      0.91      1499\n",
      "          35       0.70      0.81      0.75      1499\n",
      "          36       0.79      0.63      0.70      1499\n",
      "          37       0.39      0.47      0.42      1499\n",
      "          38       0.49      0.30      0.37      1499\n",
      "          39       0.57      0.57      0.57      1499\n",
      "          40       0.90      0.20      0.32      1499\n",
      "          41       0.79      0.79      0.79      1499\n",
      "          42       0.21      0.48      0.30      1499\n",
      "          43       0.32      0.47      0.38      1499\n",
      "          44       0.60      0.78      0.67      1499\n",
      "          45       0.25      0.38      0.30      1499\n",
      "          46       0.45      0.84      0.58      1499\n",
      "          47       0.55      0.35      0.43      1499\n",
      "          48       0.78      0.78      0.78      1499\n",
      "          49       0.44      0.50      0.47      1499\n",
      "\n",
      "    accuracy                           0.51     74950\n",
      "   macro avg       0.54      0.51      0.50     74950\n",
      "weighted avg       0.54      0.51      0.50     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87757/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_87757/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_87757/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.37731\n",
      "[1]\tvalidation_0-mlogloss:3.15608\n",
      "[2]\tvalidation_0-mlogloss:3.01151\n",
      "[3]\tvalidation_0-mlogloss:2.90632\n",
      "[4]\tvalidation_0-mlogloss:2.81880\n",
      "[5]\tvalidation_0-mlogloss:2.73824\n",
      "[6]\tvalidation_0-mlogloss:2.67229\n",
      "[7]\tvalidation_0-mlogloss:2.60515\n",
      "[8]\tvalidation_0-mlogloss:2.54875\n",
      "[9]\tvalidation_0-mlogloss:2.49980\n",
      "[10]\tvalidation_0-mlogloss:2.45533\n",
      "[11]\tvalidation_0-mlogloss:2.41367\n",
      "[12]\tvalidation_0-mlogloss:2.37762\n",
      "[13]\tvalidation_0-mlogloss:2.34176\n",
      "[14]\tvalidation_0-mlogloss:2.30710\n",
      "[15]\tvalidation_0-mlogloss:2.27748\n",
      "[16]\tvalidation_0-mlogloss:2.24916\n",
      "[17]\tvalidation_0-mlogloss:2.22448\n",
      "[18]\tvalidation_0-mlogloss:2.19472\n",
      "[19]\tvalidation_0-mlogloss:2.17107\n",
      "[20]\tvalidation_0-mlogloss:2.14954\n",
      "[21]\tvalidation_0-mlogloss:2.12938\n",
      "[22]\tvalidation_0-mlogloss:2.10752\n",
      "[23]\tvalidation_0-mlogloss:2.08507\n",
      "[24]\tvalidation_0-mlogloss:2.06563\n",
      "[25]\tvalidation_0-mlogloss:2.04746\n",
      "[26]\tvalidation_0-mlogloss:2.02925\n",
      "[27]\tvalidation_0-mlogloss:2.00806\n",
      "[28]\tvalidation_0-mlogloss:1.99003\n",
      "[29]\tvalidation_0-mlogloss:1.97588\n",
      "[30]\tvalidation_0-mlogloss:1.96236\n",
      "[31]\tvalidation_0-mlogloss:1.94659\n",
      "[32]\tvalidation_0-mlogloss:1.93200\n",
      "[33]\tvalidation_0-mlogloss:1.91620\n",
      "[34]\tvalidation_0-mlogloss:1.90174\n",
      "[35]\tvalidation_0-mlogloss:1.88896\n",
      "[36]\tvalidation_0-mlogloss:1.87695\n",
      "[37]\tvalidation_0-mlogloss:1.86213\n",
      "[38]\tvalidation_0-mlogloss:1.85011\n",
      "[39]\tvalidation_0-mlogloss:1.83839\n",
      "[40]\tvalidation_0-mlogloss:1.82552\n",
      "[41]\tvalidation_0-mlogloss:1.81381\n",
      "[42]\tvalidation_0-mlogloss:1.80023\n",
      "[43]\tvalidation_0-mlogloss:1.78589\n",
      "[44]\tvalidation_0-mlogloss:1.77611\n",
      "[45]\tvalidation_0-mlogloss:1.76545\n",
      "[46]\tvalidation_0-mlogloss:1.75437\n",
      "[47]\tvalidation_0-mlogloss:1.74335\n",
      "[48]\tvalidation_0-mlogloss:1.73398\n",
      "[49]\tvalidation_0-mlogloss:1.72284\n",
      "[50]\tvalidation_0-mlogloss:1.71323\n",
      "[51]\tvalidation_0-mlogloss:1.70130\n",
      "[52]\tvalidation_0-mlogloss:1.69304\n",
      "[53]\tvalidation_0-mlogloss:1.68436\n",
      "[54]\tvalidation_0-mlogloss:1.67296\n",
      "[55]\tvalidation_0-mlogloss:1.66291\n",
      "[56]\tvalidation_0-mlogloss:1.65162\n",
      "[57]\tvalidation_0-mlogloss:1.64309\n",
      "[58]\tvalidation_0-mlogloss:1.63489\n",
      "[59]\tvalidation_0-mlogloss:1.62524\n",
      "[60]\tvalidation_0-mlogloss:1.61598\n",
      "[61]\tvalidation_0-mlogloss:1.60731\n",
      "[62]\tvalidation_0-mlogloss:1.59808\n",
      "[63]\tvalidation_0-mlogloss:1.59097\n",
      "[64]\tvalidation_0-mlogloss:1.58515\n",
      "[65]\tvalidation_0-mlogloss:1.57815\n",
      "[66]\tvalidation_0-mlogloss:1.57142\n",
      "[67]\tvalidation_0-mlogloss:1.56393\n",
      "[68]\tvalidation_0-mlogloss:1.55704\n",
      "[69]\tvalidation_0-mlogloss:1.55038\n",
      "[70]\tvalidation_0-mlogloss:1.54268\n",
      "[71]\tvalidation_0-mlogloss:1.53470\n",
      "[72]\tvalidation_0-mlogloss:1.52781\n",
      "[73]\tvalidation_0-mlogloss:1.52135\n",
      "[74]\tvalidation_0-mlogloss:1.51405\n",
      "[75]\tvalidation_0-mlogloss:1.50645\n",
      "[76]\tvalidation_0-mlogloss:1.49995\n",
      "[77]\tvalidation_0-mlogloss:1.49322\n",
      "[78]\tvalidation_0-mlogloss:1.48676\n",
      "[79]\tvalidation_0-mlogloss:1.48141\n",
      "[80]\tvalidation_0-mlogloss:1.47665\n",
      "[81]\tvalidation_0-mlogloss:1.47051\n",
      "[82]\tvalidation_0-mlogloss:1.46496\n",
      "[83]\tvalidation_0-mlogloss:1.45891\n",
      "[84]\tvalidation_0-mlogloss:1.45339\n",
      "[85]\tvalidation_0-mlogloss:1.44770\n",
      "[86]\tvalidation_0-mlogloss:1.44223\n",
      "[87]\tvalidation_0-mlogloss:1.43736\n",
      "[88]\tvalidation_0-mlogloss:1.43072\n",
      "[89]\tvalidation_0-mlogloss:1.42645\n",
      "[90]\tvalidation_0-mlogloss:1.42227\n",
      "[91]\tvalidation_0-mlogloss:1.41596\n",
      "[92]\tvalidation_0-mlogloss:1.41153\n",
      "[93]\tvalidation_0-mlogloss:1.40551\n",
      "[94]\tvalidation_0-mlogloss:1.40138\n",
      "[95]\tvalidation_0-mlogloss:1.39615\n",
      "[96]\tvalidation_0-mlogloss:1.39141\n",
      "[97]\tvalidation_0-mlogloss:1.38800\n",
      "[98]\tvalidation_0-mlogloss:1.38309\n",
      "[99]\tvalidation_0-mlogloss:1.37880\n",
      "[100]\tvalidation_0-mlogloss:1.37494\n",
      "[101]\tvalidation_0-mlogloss:1.37071\n",
      "[102]\tvalidation_0-mlogloss:1.36479\n",
      "[103]\tvalidation_0-mlogloss:1.36106\n",
      "[104]\tvalidation_0-mlogloss:1.35666\n",
      "[105]\tvalidation_0-mlogloss:1.35217\n",
      "[106]\tvalidation_0-mlogloss:1.34816\n",
      "[107]\tvalidation_0-mlogloss:1.34516\n",
      "[108]\tvalidation_0-mlogloss:1.34114\n",
      "[109]\tvalidation_0-mlogloss:1.33761\n",
      "[110]\tvalidation_0-mlogloss:1.33318\n",
      "[111]\tvalidation_0-mlogloss:1.32833\n",
      "[112]\tvalidation_0-mlogloss:1.32354\n",
      "[113]\tvalidation_0-mlogloss:1.32035\n",
      "[114]\tvalidation_0-mlogloss:1.31585\n",
      "[115]\tvalidation_0-mlogloss:1.31218\n",
      "[116]\tvalidation_0-mlogloss:1.30792\n",
      "[117]\tvalidation_0-mlogloss:1.30442\n",
      "[118]\tvalidation_0-mlogloss:1.30112\n",
      "[119]\tvalidation_0-mlogloss:1.29791\n",
      "[120]\tvalidation_0-mlogloss:1.29361\n",
      "[121]\tvalidation_0-mlogloss:1.29081\n",
      "[122]\tvalidation_0-mlogloss:1.28775\n",
      "[123]\tvalidation_0-mlogloss:1.28388\n",
      "[124]\tvalidation_0-mlogloss:1.28037\n",
      "[125]\tvalidation_0-mlogloss:1.27692\n",
      "[126]\tvalidation_0-mlogloss:1.27411\n",
      "[127]\tvalidation_0-mlogloss:1.27212\n",
      "[128]\tvalidation_0-mlogloss:1.26838\n",
      "[129]\tvalidation_0-mlogloss:1.26474\n",
      "[130]\tvalidation_0-mlogloss:1.26129\n",
      "[131]\tvalidation_0-mlogloss:1.25760\n",
      "[132]\tvalidation_0-mlogloss:1.25495\n",
      "[133]\tvalidation_0-mlogloss:1.25230\n",
      "[134]\tvalidation_0-mlogloss:1.24900\n",
      "[135]\tvalidation_0-mlogloss:1.24652\n",
      "[136]\tvalidation_0-mlogloss:1.24280\n",
      "[137]\tvalidation_0-mlogloss:1.23810\n",
      "[138]\tvalidation_0-mlogloss:1.23465\n",
      "[139]\tvalidation_0-mlogloss:1.23196\n",
      "[140]\tvalidation_0-mlogloss:1.22939\n",
      "[141]\tvalidation_0-mlogloss:1.22724\n",
      "[142]\tvalidation_0-mlogloss:1.22510\n",
      "[143]\tvalidation_0-mlogloss:1.22254\n",
      "[144]\tvalidation_0-mlogloss:1.21956\n",
      "[145]\tvalidation_0-mlogloss:1.21525\n",
      "[146]\tvalidation_0-mlogloss:1.21091\n",
      "[147]\tvalidation_0-mlogloss:1.20816\n",
      "[148]\tvalidation_0-mlogloss:1.20533\n",
      "[149]\tvalidation_0-mlogloss:1.20323\n",
      "[150]\tvalidation_0-mlogloss:1.20086\n",
      "[151]\tvalidation_0-mlogloss:1.19819\n",
      "[152]\tvalidation_0-mlogloss:1.19505\n",
      "[153]\tvalidation_0-mlogloss:1.19226\n",
      "[154]\tvalidation_0-mlogloss:1.18927\n",
      "[155]\tvalidation_0-mlogloss:1.18581\n",
      "[156]\tvalidation_0-mlogloss:1.18348\n",
      "[157]\tvalidation_0-mlogloss:1.18149\n",
      "[158]\tvalidation_0-mlogloss:1.17844\n",
      "[159]\tvalidation_0-mlogloss:1.17477\n",
      "[160]\tvalidation_0-mlogloss:1.17264\n",
      "[161]\tvalidation_0-mlogloss:1.17027\n",
      "[162]\tvalidation_0-mlogloss:1.16763\n",
      "[163]\tvalidation_0-mlogloss:1.16608\n",
      "[164]\tvalidation_0-mlogloss:1.16373\n",
      "[165]\tvalidation_0-mlogloss:1.16215\n",
      "[166]\tvalidation_0-mlogloss:1.15981\n",
      "[167]\tvalidation_0-mlogloss:1.15797\n",
      "[168]\tvalidation_0-mlogloss:1.15619\n",
      "[169]\tvalidation_0-mlogloss:1.15458\n",
      "[170]\tvalidation_0-mlogloss:1.15185\n",
      "[171]\tvalidation_0-mlogloss:1.15046\n",
      "[172]\tvalidation_0-mlogloss:1.14917\n",
      "[173]\tvalidation_0-mlogloss:1.14702\n",
      "[174]\tvalidation_0-mlogloss:1.14507\n",
      "[175]\tvalidation_0-mlogloss:1.14310\n",
      "[176]\tvalidation_0-mlogloss:1.14144\n",
      "[177]\tvalidation_0-mlogloss:1.13886\n",
      "[178]\tvalidation_0-mlogloss:1.13603\n",
      "[179]\tvalidation_0-mlogloss:1.13479\n",
      "[180]\tvalidation_0-mlogloss:1.13325\n",
      "[181]\tvalidation_0-mlogloss:1.13096\n",
      "[182]\tvalidation_0-mlogloss:1.12938\n",
      "[183]\tvalidation_0-mlogloss:1.12778\n",
      "[184]\tvalidation_0-mlogloss:1.12580\n",
      "[185]\tvalidation_0-mlogloss:1.12416\n",
      "[186]\tvalidation_0-mlogloss:1.12252\n",
      "[187]\tvalidation_0-mlogloss:1.12095\n",
      "[188]\tvalidation_0-mlogloss:1.11940\n",
      "[189]\tvalidation_0-mlogloss:1.11641\n",
      "[190]\tvalidation_0-mlogloss:1.11471\n",
      "[191]\tvalidation_0-mlogloss:1.11330\n",
      "[192]\tvalidation_0-mlogloss:1.11258\n",
      "[193]\tvalidation_0-mlogloss:1.11208\n",
      "[194]\tvalidation_0-mlogloss:1.11072\n",
      "[195]\tvalidation_0-mlogloss:1.10862\n",
      "[196]\tvalidation_0-mlogloss:1.10678\n",
      "[197]\tvalidation_0-mlogloss:1.10539\n",
      "[198]\tvalidation_0-mlogloss:1.10341\n",
      "[199]\tvalidation_0-mlogloss:1.10215\n",
      "[200]\tvalidation_0-mlogloss:1.09939\n",
      "[201]\tvalidation_0-mlogloss:1.09739\n",
      "[202]\tvalidation_0-mlogloss:1.09613\n",
      "[203]\tvalidation_0-mlogloss:1.09462\n",
      "[204]\tvalidation_0-mlogloss:1.09307\n",
      "[205]\tvalidation_0-mlogloss:1.09159\n",
      "[206]\tvalidation_0-mlogloss:1.08989\n",
      "[207]\tvalidation_0-mlogloss:1.08844\n",
      "[208]\tvalidation_0-mlogloss:1.08671\n",
      "[209]\tvalidation_0-mlogloss:1.08494\n",
      "[210]\tvalidation_0-mlogloss:1.08264\n",
      "[211]\tvalidation_0-mlogloss:1.08131\n",
      "[212]\tvalidation_0-mlogloss:1.07971\n",
      "[213]\tvalidation_0-mlogloss:1.07898\n",
      "[214]\tvalidation_0-mlogloss:1.07725\n",
      "[215]\tvalidation_0-mlogloss:1.07576\n",
      "[216]\tvalidation_0-mlogloss:1.07384\n",
      "[217]\tvalidation_0-mlogloss:1.07156\n",
      "[218]\tvalidation_0-mlogloss:1.07032\n",
      "[219]\tvalidation_0-mlogloss:1.06923\n",
      "[220]\tvalidation_0-mlogloss:1.06785\n",
      "[221]\tvalidation_0-mlogloss:1.06638\n",
      "[222]\tvalidation_0-mlogloss:1.06535\n",
      "[223]\tvalidation_0-mlogloss:1.06434\n",
      "[224]\tvalidation_0-mlogloss:1.06287\n",
      "[225]\tvalidation_0-mlogloss:1.06221\n",
      "[226]\tvalidation_0-mlogloss:1.06167\n",
      "[227]\tvalidation_0-mlogloss:1.06054\n",
      "[228]\tvalidation_0-mlogloss:1.05956\n",
      "[229]\tvalidation_0-mlogloss:1.05806\n",
      "[230]\tvalidation_0-mlogloss:1.05745\n",
      "[231]\tvalidation_0-mlogloss:1.05620\n",
      "[232]\tvalidation_0-mlogloss:1.05556\n",
      "[233]\tvalidation_0-mlogloss:1.05453\n",
      "[234]\tvalidation_0-mlogloss:1.05302\n",
      "[235]\tvalidation_0-mlogloss:1.05134\n",
      "[236]\tvalidation_0-mlogloss:1.05025\n",
      "[237]\tvalidation_0-mlogloss:1.04914\n",
      "[238]\tvalidation_0-mlogloss:1.04832\n",
      "[239]\tvalidation_0-mlogloss:1.04748\n",
      "[240]\tvalidation_0-mlogloss:1.04660\n",
      "[241]\tvalidation_0-mlogloss:1.04635\n",
      "[242]\tvalidation_0-mlogloss:1.04520\n",
      "[243]\tvalidation_0-mlogloss:1.04387\n",
      "[244]\tvalidation_0-mlogloss:1.04281\n",
      "[245]\tvalidation_0-mlogloss:1.04194\n",
      "[246]\tvalidation_0-mlogloss:1.04113\n",
      "[247]\tvalidation_0-mlogloss:1.03992\n",
      "[248]\tvalidation_0-mlogloss:1.03910\n",
      "[249]\tvalidation_0-mlogloss:1.03833\n",
      "[250]\tvalidation_0-mlogloss:1.03710\n",
      "[251]\tvalidation_0-mlogloss:1.03668\n",
      "[252]\tvalidation_0-mlogloss:1.03620\n",
      "[253]\tvalidation_0-mlogloss:1.03584\n",
      "[254]\tvalidation_0-mlogloss:1.03558\n",
      "[255]\tvalidation_0-mlogloss:1.03472\n",
      "[256]\tvalidation_0-mlogloss:1.03380\n",
      "[257]\tvalidation_0-mlogloss:1.03250\n",
      "[258]\tvalidation_0-mlogloss:1.03091\n",
      "[259]\tvalidation_0-mlogloss:1.03070\n",
      "[260]\tvalidation_0-mlogloss:1.02968\n",
      "[261]\tvalidation_0-mlogloss:1.02874\n",
      "[262]\tvalidation_0-mlogloss:1.02814\n",
      "[263]\tvalidation_0-mlogloss:1.02761\n",
      "[264]\tvalidation_0-mlogloss:1.02655\n",
      "[265]\tvalidation_0-mlogloss:1.02535\n",
      "[266]\tvalidation_0-mlogloss:1.02407\n",
      "[267]\tvalidation_0-mlogloss:1.02313\n",
      "[268]\tvalidation_0-mlogloss:1.02220\n",
      "[269]\tvalidation_0-mlogloss:1.02055\n",
      "[270]\tvalidation_0-mlogloss:1.02004\n",
      "[271]\tvalidation_0-mlogloss:1.01920\n",
      "[272]\tvalidation_0-mlogloss:1.01809\n",
      "[273]\tvalidation_0-mlogloss:1.01726\n",
      "[274]\tvalidation_0-mlogloss:1.01665\n",
      "[275]\tvalidation_0-mlogloss:1.01618\n",
      "[276]\tvalidation_0-mlogloss:1.01559\n",
      "[277]\tvalidation_0-mlogloss:1.01479\n",
      "[278]\tvalidation_0-mlogloss:1.01362\n",
      "[279]\tvalidation_0-mlogloss:1.01305\n",
      "[280]\tvalidation_0-mlogloss:1.01198\n",
      "[281]\tvalidation_0-mlogloss:1.01115\n",
      "[282]\tvalidation_0-mlogloss:1.01076\n",
      "[283]\tvalidation_0-mlogloss:1.00969\n",
      "[284]\tvalidation_0-mlogloss:1.00936\n",
      "[285]\tvalidation_0-mlogloss:1.00837\n",
      "[286]\tvalidation_0-mlogloss:1.00727\n",
      "[287]\tvalidation_0-mlogloss:1.00626\n",
      "[288]\tvalidation_0-mlogloss:1.00454\n",
      "[289]\tvalidation_0-mlogloss:1.00317\n",
      "[290]\tvalidation_0-mlogloss:1.00277\n",
      "[291]\tvalidation_0-mlogloss:1.00188\n",
      "[292]\tvalidation_0-mlogloss:1.00120\n",
      "[293]\tvalidation_0-mlogloss:1.00046\n",
      "[294]\tvalidation_0-mlogloss:0.99957\n",
      "[295]\tvalidation_0-mlogloss:0.99916\n",
      "[296]\tvalidation_0-mlogloss:0.99846\n",
      "[297]\tvalidation_0-mlogloss:0.99773\n",
      "[298]\tvalidation_0-mlogloss:0.99694\n",
      "[299]\tvalidation_0-mlogloss:0.99680\n",
      "[300]\tvalidation_0-mlogloss:0.99634\n",
      "[301]\tvalidation_0-mlogloss:0.99579\n",
      "[302]\tvalidation_0-mlogloss:0.99525\n",
      "[303]\tvalidation_0-mlogloss:0.99507\n",
      "[304]\tvalidation_0-mlogloss:0.99365\n",
      "[305]\tvalidation_0-mlogloss:0.99311\n",
      "[306]\tvalidation_0-mlogloss:0.99206\n",
      "[307]\tvalidation_0-mlogloss:0.99139\n",
      "[308]\tvalidation_0-mlogloss:0.99113\n",
      "[309]\tvalidation_0-mlogloss:0.99097\n",
      "[310]\tvalidation_0-mlogloss:0.99048\n",
      "[311]\tvalidation_0-mlogloss:0.98974\n",
      "[312]\tvalidation_0-mlogloss:0.98899\n",
      "[313]\tvalidation_0-mlogloss:0.98879\n",
      "[314]\tvalidation_0-mlogloss:0.98802\n",
      "[315]\tvalidation_0-mlogloss:0.98730\n",
      "[316]\tvalidation_0-mlogloss:0.98704\n",
      "[317]\tvalidation_0-mlogloss:0.98664\n",
      "[318]\tvalidation_0-mlogloss:0.98582\n",
      "[319]\tvalidation_0-mlogloss:0.98547\n",
      "[320]\tvalidation_0-mlogloss:0.98483\n",
      "[321]\tvalidation_0-mlogloss:0.98458\n",
      "[322]\tvalidation_0-mlogloss:0.98425\n",
      "[323]\tvalidation_0-mlogloss:0.98342\n",
      "[324]\tvalidation_0-mlogloss:0.98243\n",
      "[325]\tvalidation_0-mlogloss:0.98207\n",
      "[326]\tvalidation_0-mlogloss:0.98119\n",
      "[327]\tvalidation_0-mlogloss:0.98114\n",
      "[328]\tvalidation_0-mlogloss:0.98087\n",
      "[329]\tvalidation_0-mlogloss:0.98086\n",
      "[330]\tvalidation_0-mlogloss:0.98096\n",
      "[331]\tvalidation_0-mlogloss:0.98044\n",
      "[332]\tvalidation_0-mlogloss:0.98048\n",
      "[333]\tvalidation_0-mlogloss:0.97974\n",
      "[334]\tvalidation_0-mlogloss:0.97918\n",
      "[335]\tvalidation_0-mlogloss:0.97867\n",
      "[336]\tvalidation_0-mlogloss:0.97744\n",
      "[337]\tvalidation_0-mlogloss:0.97696\n",
      "[338]\tvalidation_0-mlogloss:0.97666\n",
      "[339]\tvalidation_0-mlogloss:0.97609\n",
      "[340]\tvalidation_0-mlogloss:0.97557\n",
      "[341]\tvalidation_0-mlogloss:0.97493\n",
      "[342]\tvalidation_0-mlogloss:0.97463\n",
      "[343]\tvalidation_0-mlogloss:0.97422\n",
      "[344]\tvalidation_0-mlogloss:0.97335\n",
      "[345]\tvalidation_0-mlogloss:0.97296\n",
      "[346]\tvalidation_0-mlogloss:0.97245\n",
      "[347]\tvalidation_0-mlogloss:0.97198\n",
      "[348]\tvalidation_0-mlogloss:0.97153\n",
      "[349]\tvalidation_0-mlogloss:0.97084\n",
      "[350]\tvalidation_0-mlogloss:0.97073\n",
      "[351]\tvalidation_0-mlogloss:0.97026\n",
      "[352]\tvalidation_0-mlogloss:0.97003\n",
      "[353]\tvalidation_0-mlogloss:0.96947\n",
      "[354]\tvalidation_0-mlogloss:0.96863\n",
      "[355]\tvalidation_0-mlogloss:0.96827\n",
      "[356]\tvalidation_0-mlogloss:0.96809\n",
      "[357]\tvalidation_0-mlogloss:0.96786\n",
      "[358]\tvalidation_0-mlogloss:0.96710\n",
      "[359]\tvalidation_0-mlogloss:0.96683\n",
      "[360]\tvalidation_0-mlogloss:0.96632\n",
      "[361]\tvalidation_0-mlogloss:0.96599\n",
      "[362]\tvalidation_0-mlogloss:0.96578\n",
      "[363]\tvalidation_0-mlogloss:0.96556\n",
      "[364]\tvalidation_0-mlogloss:0.96531\n",
      "[365]\tvalidation_0-mlogloss:0.96476\n",
      "[366]\tvalidation_0-mlogloss:0.96447\n",
      "[367]\tvalidation_0-mlogloss:0.96446\n",
      "[368]\tvalidation_0-mlogloss:0.96412\n",
      "[369]\tvalidation_0-mlogloss:0.96385\n",
      "[370]\tvalidation_0-mlogloss:0.96327\n",
      "[371]\tvalidation_0-mlogloss:0.96306\n",
      "[372]\tvalidation_0-mlogloss:0.96305\n",
      "[373]\tvalidation_0-mlogloss:0.96256\n",
      "[374]\tvalidation_0-mlogloss:0.96240\n",
      "[375]\tvalidation_0-mlogloss:0.96174\n",
      "[376]\tvalidation_0-mlogloss:0.96072\n",
      "[377]\tvalidation_0-mlogloss:0.96038\n",
      "[378]\tvalidation_0-mlogloss:0.95991\n",
      "[379]\tvalidation_0-mlogloss:0.95920\n",
      "[380]\tvalidation_0-mlogloss:0.95927\n",
      "[381]\tvalidation_0-mlogloss:0.95937\n",
      "[382]\tvalidation_0-mlogloss:0.95960\n",
      "[383]\tvalidation_0-mlogloss:0.95929\n",
      "[384]\tvalidation_0-mlogloss:0.95890\n",
      "[385]\tvalidation_0-mlogloss:0.95832\n",
      "[386]\tvalidation_0-mlogloss:0.95849\n",
      "[387]\tvalidation_0-mlogloss:0.95844\n",
      "[388]\tvalidation_0-mlogloss:0.95816\n",
      "[389]\tvalidation_0-mlogloss:0.95801\n",
      "[390]\tvalidation_0-mlogloss:0.95754\n",
      "[391]\tvalidation_0-mlogloss:0.95761\n",
      "[392]\tvalidation_0-mlogloss:0.95729\n",
      "[393]\tvalidation_0-mlogloss:0.95680\n",
      "[394]\tvalidation_0-mlogloss:0.95678\n",
      "[395]\tvalidation_0-mlogloss:0.95608\n",
      "[396]\tvalidation_0-mlogloss:0.95558\n",
      "[397]\tvalidation_0-mlogloss:0.95523\n",
      "[398]\tvalidation_0-mlogloss:0.95468\n",
      "[399]\tvalidation_0-mlogloss:0.95500\n",
      "[400]\tvalidation_0-mlogloss:0.95469\n",
      "[401]\tvalidation_0-mlogloss:0.95510\n",
      "[402]\tvalidation_0-mlogloss:0.95467\n",
      "[403]\tvalidation_0-mlogloss:0.95459\n",
      "[404]\tvalidation_0-mlogloss:0.95447\n",
      "[405]\tvalidation_0-mlogloss:0.95426\n",
      "[406]\tvalidation_0-mlogloss:0.95384\n",
      "[407]\tvalidation_0-mlogloss:0.95402\n",
      "[408]\tvalidation_0-mlogloss:0.95355\n",
      "[409]\tvalidation_0-mlogloss:0.95342\n",
      "[410]\tvalidation_0-mlogloss:0.95321\n",
      "[411]\tvalidation_0-mlogloss:0.95289\n",
      "[412]\tvalidation_0-mlogloss:0.95258\n",
      "[413]\tvalidation_0-mlogloss:0.95211\n",
      "[414]\tvalidation_0-mlogloss:0.95193\n",
      "[415]\tvalidation_0-mlogloss:0.95209\n",
      "[416]\tvalidation_0-mlogloss:0.95160\n",
      "[417]\tvalidation_0-mlogloss:0.95123\n",
      "[418]\tvalidation_0-mlogloss:0.95082\n",
      "[419]\tvalidation_0-mlogloss:0.95096\n",
      "[420]\tvalidation_0-mlogloss:0.95075\n",
      "[421]\tvalidation_0-mlogloss:0.95037\n",
      "[422]\tvalidation_0-mlogloss:0.95023\n",
      "[423]\tvalidation_0-mlogloss:0.94978\n",
      "[424]\tvalidation_0-mlogloss:0.94904\n",
      "[425]\tvalidation_0-mlogloss:0.94937\n",
      "[426]\tvalidation_0-mlogloss:0.94913\n",
      "[427]\tvalidation_0-mlogloss:0.94883\n",
      "[428]\tvalidation_0-mlogloss:0.94845\n",
      "[429]\tvalidation_0-mlogloss:0.94809\n",
      "[430]\tvalidation_0-mlogloss:0.94757\n",
      "[431]\tvalidation_0-mlogloss:0.94723\n",
      "[432]\tvalidation_0-mlogloss:0.94688\n",
      "[433]\tvalidation_0-mlogloss:0.94668\n",
      "[434]\tvalidation_0-mlogloss:0.94626\n",
      "[435]\tvalidation_0-mlogloss:0.94576\n",
      "[436]\tvalidation_0-mlogloss:0.94550\n",
      "[437]\tvalidation_0-mlogloss:0.94518\n",
      "[438]\tvalidation_0-mlogloss:0.94493\n",
      "[439]\tvalidation_0-mlogloss:0.94506\n",
      "[440]\tvalidation_0-mlogloss:0.94474\n",
      "[441]\tvalidation_0-mlogloss:0.94461\n",
      "[442]\tvalidation_0-mlogloss:0.94440\n",
      "[443]\tvalidation_0-mlogloss:0.94415\n",
      "[444]\tvalidation_0-mlogloss:0.94383\n",
      "[445]\tvalidation_0-mlogloss:0.94375\n",
      "[446]\tvalidation_0-mlogloss:0.94350\n",
      "[447]\tvalidation_0-mlogloss:0.94288\n",
      "[448]\tvalidation_0-mlogloss:0.94285\n",
      "[449]\tvalidation_0-mlogloss:0.94275\n",
      "[450]\tvalidation_0-mlogloss:0.94259\n",
      "[451]\tvalidation_0-mlogloss:0.94257\n",
      "[452]\tvalidation_0-mlogloss:0.94241\n",
      "[453]\tvalidation_0-mlogloss:0.94216\n",
      "[454]\tvalidation_0-mlogloss:0.94221\n",
      "[455]\tvalidation_0-mlogloss:0.94206\n",
      "[456]\tvalidation_0-mlogloss:0.94170\n",
      "[457]\tvalidation_0-mlogloss:0.94172\n",
      "[458]\tvalidation_0-mlogloss:0.94158\n",
      "[459]\tvalidation_0-mlogloss:0.94137\n",
      "[460]\tvalidation_0-mlogloss:0.94123\n",
      "[461]\tvalidation_0-mlogloss:0.94087\n",
      "[462]\tvalidation_0-mlogloss:0.94133\n",
      "[463]\tvalidation_0-mlogloss:0.94128\n",
      "[464]\tvalidation_0-mlogloss:0.94093\n",
      "[465]\tvalidation_0-mlogloss:0.94071\n",
      "[466]\tvalidation_0-mlogloss:0.94048\n",
      "[467]\tvalidation_0-mlogloss:0.94002\n",
      "[468]\tvalidation_0-mlogloss:0.93988\n",
      "[469]\tvalidation_0-mlogloss:0.93950\n",
      "[470]\tvalidation_0-mlogloss:0.93919\n",
      "[471]\tvalidation_0-mlogloss:0.93905\n",
      "[472]\tvalidation_0-mlogloss:0.93895\n",
      "[473]\tvalidation_0-mlogloss:0.93875\n",
      "[474]\tvalidation_0-mlogloss:0.93875\n",
      "[475]\tvalidation_0-mlogloss:0.93872\n",
      "[476]\tvalidation_0-mlogloss:0.93880\n",
      "[477]\tvalidation_0-mlogloss:0.93891\n",
      "[478]\tvalidation_0-mlogloss:0.93893\n",
      "[479]\tvalidation_0-mlogloss:0.93901\n",
      "[480]\tvalidation_0-mlogloss:0.93869\n",
      "[481]\tvalidation_0-mlogloss:0.93862\n",
      "[482]\tvalidation_0-mlogloss:0.93816\n",
      "[483]\tvalidation_0-mlogloss:0.93789\n",
      "[484]\tvalidation_0-mlogloss:0.93776\n",
      "[485]\tvalidation_0-mlogloss:0.93779\n",
      "[486]\tvalidation_0-mlogloss:0.93728\n",
      "[487]\tvalidation_0-mlogloss:0.93688\n",
      "[488]\tvalidation_0-mlogloss:0.93658\n",
      "[489]\tvalidation_0-mlogloss:0.93638\n",
      "[490]\tvalidation_0-mlogloss:0.93632\n",
      "[491]\tvalidation_0-mlogloss:0.93586\n",
      "[492]\tvalidation_0-mlogloss:0.93565\n",
      "[493]\tvalidation_0-mlogloss:0.93519\n",
      "[494]\tvalidation_0-mlogloss:0.93495\n",
      "[495]\tvalidation_0-mlogloss:0.93473\n",
      "[496]\tvalidation_0-mlogloss:0.93456\n",
      "[497]\tvalidation_0-mlogloss:0.93455\n",
      "[498]\tvalidation_0-mlogloss:0.93476\n",
      "[499]\tvalidation_0-mlogloss:0.93457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.82      0.70      1499\n",
      "           1       0.78      0.86      0.82      1499\n",
      "           2       0.65      0.81      0.72      1499\n",
      "           3       0.85      0.63      0.72      1499\n",
      "           4       0.96      0.97      0.96      1499\n",
      "           5       0.91      0.87      0.89      1499\n",
      "           6       0.88      0.97      0.92      1499\n",
      "           7       0.92      0.81      0.86      1499\n",
      "           8       0.48      0.80      0.60      1499\n",
      "           9       0.82      0.80      0.81      1499\n",
      "          10       0.93      0.74      0.82      1499\n",
      "          11       0.93      0.91      0.92      1499\n",
      "          12       0.73      0.82      0.77      1499\n",
      "          13       0.69      0.75      0.72      1499\n",
      "          14       0.57      0.75      0.65      1499\n",
      "          15       0.88      0.63      0.73      1499\n",
      "          16       0.70      0.45      0.55      1499\n",
      "          17       0.73      0.73      0.73      1499\n",
      "          18       0.76      0.84      0.80      1499\n",
      "          19       0.71      0.64      0.67      1499\n",
      "          20       0.96      0.99      0.97      1499\n",
      "          21       0.67      0.77      0.72      1499\n",
      "          22       0.70      0.83      0.76      1499\n",
      "          23       0.95      0.95      0.95      1499\n",
      "          24       0.78      0.52      0.63      1499\n",
      "          25       0.77      0.80      0.79      1499\n",
      "          26       0.83      0.62      0.71      1499\n",
      "          27       0.75      0.62      0.68      1499\n",
      "          28       0.72      0.79      0.75      1499\n",
      "          29       0.82      0.66      0.73      1499\n",
      "          30       0.69      0.86      0.77      1499\n",
      "          31       0.63      0.89      0.74      1499\n",
      "          32       0.84      0.91      0.87      1499\n",
      "          33       0.82      0.65      0.72      1499\n",
      "          34       0.98      0.99      0.99      1499\n",
      "          35       0.78      0.83      0.80      1499\n",
      "          36       0.85      0.81      0.83      1499\n",
      "          37       0.70      0.79      0.74      1499\n",
      "          38       0.71      0.37      0.49      1499\n",
      "          39       0.80      0.65      0.72      1499\n",
      "          40       0.92      0.72      0.81      1499\n",
      "          41       0.86      0.68      0.76      1499\n",
      "          42       0.60      0.49      0.54      1499\n",
      "          43       0.70      0.87      0.78      1499\n",
      "          44       0.84      0.89      0.87      1499\n",
      "          45       0.69      0.62      0.65      1499\n",
      "          46       0.64      0.86      0.73      1499\n",
      "          47       0.77      0.52      0.62      1499\n",
      "          48       0.80      0.87      0.84      1499\n",
      "          49       0.72      0.78      0.75      1499\n",
      "\n",
      "    accuracy                           0.76     74950\n",
      "   macro avg       0.78      0.76      0.76     74950\n",
      "weighted avg       0.78      0.76      0.76     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8f654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "300aa492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 13:44:54.086747: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 76800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 43s 2ms/step - loss: 1.1774 - val_loss: 1.9038\n",
      "Epoch 2/10\n",
      "  826/18750 [>.............................] - ETA: 40s - loss: 0.5807"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 40s 2ms/step - loss: 0.4657 - val_loss: 1.7318\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 42s 2ms/step - loss: 0.3313 - val_loss: 1.7933\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 42s 2ms/step - loss: 0.2760 - val_loss: 1.8028\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 42s 2ms/step - loss: 0.2445 - val_loss: 1.9427\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 44s 2ms/step - loss: 0.2229 - val_loss: 1.9116\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 42s 2ms/step - loss: 0.2063 - val_loss: 1.9929\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 42s 2ms/step - loss: 0.1937 - val_loss: 1.8743\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 42s 2ms/step - loss: 0.1825 - val_loss: 1.8914\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 44s 2ms/step - loss: 0.1738 - val_loss: 1.9320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe30197a290>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  36/2343 [..............................] - ETA: 3s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 3s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.72      1499\n",
      "           1       0.79      0.70      0.75      1499\n",
      "           2       0.71      0.51      0.59      1499\n",
      "           3       0.56      0.55      0.55      1499\n",
      "           4       0.94      0.97      0.95      1499\n",
      "           5       0.83      0.87      0.85      1499\n",
      "           6       0.95      0.91      0.93      1499\n",
      "           7       0.90      0.68      0.78      1499\n",
      "           8       0.52      0.79      0.63      1499\n",
      "           9       0.69      0.82      0.75      1499\n",
      "          10       0.65      0.54      0.59      1499\n",
      "          11       0.96      0.93      0.95      1499\n",
      "          12       0.73      0.80      0.76      1499\n",
      "          13       0.47      0.75      0.58      1499\n",
      "          14       0.61      0.70      0.65      1499\n",
      "          15       0.74      0.31      0.44      1499\n",
      "          16       0.71      0.51      0.59      1499\n",
      "          17       0.90      0.63      0.74      1499\n",
      "          18       0.73      0.84      0.78      1499\n",
      "          19       0.53      0.52      0.52      1499\n",
      "          20       0.97      0.93      0.95      1499\n",
      "          21       0.46      0.84      0.59      1499\n",
      "          22       0.68      0.82      0.74      1499\n",
      "          23       0.91      0.97      0.94      1499\n",
      "          24       0.69      0.23      0.34      1499\n",
      "          25       0.71      0.85      0.77      1499\n",
      "          26       0.59      0.41      0.48      1499\n",
      "          27       0.78      0.30      0.43      1499\n",
      "          28       0.88      0.61      0.72      1499\n",
      "          29       0.90      0.83      0.86      1499\n",
      "          30       0.82      0.88      0.85      1499\n",
      "          31       0.65      0.85      0.74      1499\n",
      "          32       0.81      0.91      0.86      1499\n",
      "          33       0.79      0.67      0.73      1499\n",
      "          34       0.99      0.96      0.98      1499\n",
      "          35       0.76      0.79      0.78      1499\n",
      "          36       0.80      0.88      0.84      1499\n",
      "          37       0.65      0.79      0.71      1499\n",
      "          38       0.50      0.19      0.27      1499\n",
      "          39       0.48      0.76      0.59      1499\n",
      "          40       0.93      0.79      0.85      1499\n",
      "          41       0.85      0.70      0.76      1499\n",
      "          42       0.54      0.64      0.58      1499\n",
      "          43       0.61      0.84      0.70      1499\n",
      "          44       0.84      0.84      0.84      1499\n",
      "          45       0.72      0.59      0.65      1499\n",
      "          46       0.60      0.92      0.72      1499\n",
      "          47       0.76      0.66      0.71      1499\n",
      "          48       0.73      0.90      0.81      1499\n",
      "          49       0.77      0.51      0.61      1499\n",
      "\n",
      "    accuracy                           0.72     74950\n",
      "   macro avg       0.74      0.72      0.71     74950\n",
      "weighted avg       0.74      0.72      0.71     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.08094\n",
      "[1]\tvalidation_0-mlogloss:2.82037\n",
      "[2]\tvalidation_0-mlogloss:2.64979\n",
      "[3]\tvalidation_0-mlogloss:2.51031\n",
      "[4]\tvalidation_0-mlogloss:2.39406\n",
      "[5]\tvalidation_0-mlogloss:2.29201\n",
      "[6]\tvalidation_0-mlogloss:2.20938\n",
      "[7]\tvalidation_0-mlogloss:2.12898\n",
      "[8]\tvalidation_0-mlogloss:2.06606\n",
      "[9]\tvalidation_0-mlogloss:2.00590\n",
      "[10]\tvalidation_0-mlogloss:1.95438\n",
      "[11]\tvalidation_0-mlogloss:1.90107\n",
      "[12]\tvalidation_0-mlogloss:1.85245\n",
      "[13]\tvalidation_0-mlogloss:1.81452\n",
      "[14]\tvalidation_0-mlogloss:1.77738\n",
      "[15]\tvalidation_0-mlogloss:1.73712\n",
      "[16]\tvalidation_0-mlogloss:1.70189\n",
      "[17]\tvalidation_0-mlogloss:1.66835\n",
      "[18]\tvalidation_0-mlogloss:1.63731\n",
      "[19]\tvalidation_0-mlogloss:1.60701\n",
      "[20]\tvalidation_0-mlogloss:1.58140\n",
      "[21]\tvalidation_0-mlogloss:1.55667\n",
      "[22]\tvalidation_0-mlogloss:1.53134\n",
      "[23]\tvalidation_0-mlogloss:1.50634\n",
      "[24]\tvalidation_0-mlogloss:1.48351\n",
      "[25]\tvalidation_0-mlogloss:1.46111\n",
      "[26]\tvalidation_0-mlogloss:1.43913\n",
      "[27]\tvalidation_0-mlogloss:1.41868\n",
      "[28]\tvalidation_0-mlogloss:1.39783\n",
      "[29]\tvalidation_0-mlogloss:1.37679\n",
      "[30]\tvalidation_0-mlogloss:1.36003\n",
      "[31]\tvalidation_0-mlogloss:1.34658\n",
      "[32]\tvalidation_0-mlogloss:1.33082\n",
      "[33]\tvalidation_0-mlogloss:1.31319\n",
      "[34]\tvalidation_0-mlogloss:1.29735\n",
      "[35]\tvalidation_0-mlogloss:1.28368\n",
      "[36]\tvalidation_0-mlogloss:1.27114\n",
      "[37]\tvalidation_0-mlogloss:1.25770\n",
      "[38]\tvalidation_0-mlogloss:1.24404\n",
      "[39]\tvalidation_0-mlogloss:1.23129\n",
      "[40]\tvalidation_0-mlogloss:1.21798\n",
      "[41]\tvalidation_0-mlogloss:1.20319\n",
      "[42]\tvalidation_0-mlogloss:1.19156\n",
      "[43]\tvalidation_0-mlogloss:1.17794\n",
      "[44]\tvalidation_0-mlogloss:1.16646\n",
      "[45]\tvalidation_0-mlogloss:1.15588\n",
      "[46]\tvalidation_0-mlogloss:1.14669\n",
      "[47]\tvalidation_0-mlogloss:1.13628\n",
      "[48]\tvalidation_0-mlogloss:1.12686\n",
      "[49]\tvalidation_0-mlogloss:1.11523\n",
      "[50]\tvalidation_0-mlogloss:1.10570\n",
      "[51]\tvalidation_0-mlogloss:1.09514\n",
      "[52]\tvalidation_0-mlogloss:1.08409\n",
      "[53]\tvalidation_0-mlogloss:1.07431\n",
      "[54]\tvalidation_0-mlogloss:1.06495\n",
      "[55]\tvalidation_0-mlogloss:1.05440\n",
      "[56]\tvalidation_0-mlogloss:1.04589\n",
      "[57]\tvalidation_0-mlogloss:1.03642\n",
      "[58]\tvalidation_0-mlogloss:1.02830\n",
      "[59]\tvalidation_0-mlogloss:1.01913\n",
      "[60]\tvalidation_0-mlogloss:1.01047\n",
      "[61]\tvalidation_0-mlogloss:1.00257\n",
      "[62]\tvalidation_0-mlogloss:0.99506\n",
      "[63]\tvalidation_0-mlogloss:0.98678\n",
      "[64]\tvalidation_0-mlogloss:0.97830\n",
      "[65]\tvalidation_0-mlogloss:0.96948\n",
      "[66]\tvalidation_0-mlogloss:0.96130\n",
      "[67]\tvalidation_0-mlogloss:0.95246\n",
      "[68]\tvalidation_0-mlogloss:0.94692\n",
      "[69]\tvalidation_0-mlogloss:0.93943\n",
      "[70]\tvalidation_0-mlogloss:0.93377\n",
      "[71]\tvalidation_0-mlogloss:0.92644\n",
      "[72]\tvalidation_0-mlogloss:0.92131\n",
      "[73]\tvalidation_0-mlogloss:0.91404\n",
      "[74]\tvalidation_0-mlogloss:0.90809\n",
      "[75]\tvalidation_0-mlogloss:0.90210\n",
      "[76]\tvalidation_0-mlogloss:0.89644\n",
      "[77]\tvalidation_0-mlogloss:0.89007\n",
      "[78]\tvalidation_0-mlogloss:0.88450\n",
      "[79]\tvalidation_0-mlogloss:0.87923\n",
      "[80]\tvalidation_0-mlogloss:0.87372\n",
      "[81]\tvalidation_0-mlogloss:0.86848\n",
      "[82]\tvalidation_0-mlogloss:0.86189\n",
      "[83]\tvalidation_0-mlogloss:0.85689\n",
      "[84]\tvalidation_0-mlogloss:0.85121\n",
      "[85]\tvalidation_0-mlogloss:0.84624\n",
      "[86]\tvalidation_0-mlogloss:0.84031\n",
      "[87]\tvalidation_0-mlogloss:0.83473\n",
      "[88]\tvalidation_0-mlogloss:0.82959\n",
      "[89]\tvalidation_0-mlogloss:0.82383\n",
      "[90]\tvalidation_0-mlogloss:0.81999\n",
      "[91]\tvalidation_0-mlogloss:0.81476\n",
      "[92]\tvalidation_0-mlogloss:0.81085\n",
      "[93]\tvalidation_0-mlogloss:0.80684\n",
      "[94]\tvalidation_0-mlogloss:0.80176\n",
      "[95]\tvalidation_0-mlogloss:0.79791\n",
      "[96]\tvalidation_0-mlogloss:0.79400\n",
      "[97]\tvalidation_0-mlogloss:0.78907\n",
      "[98]\tvalidation_0-mlogloss:0.78611\n",
      "[99]\tvalidation_0-mlogloss:0.78284\n",
      "[100]\tvalidation_0-mlogloss:0.77919\n",
      "[101]\tvalidation_0-mlogloss:0.77530\n",
      "[102]\tvalidation_0-mlogloss:0.77132\n",
      "[103]\tvalidation_0-mlogloss:0.76725\n",
      "[104]\tvalidation_0-mlogloss:0.76359\n",
      "[105]\tvalidation_0-mlogloss:0.76075\n",
      "[106]\tvalidation_0-mlogloss:0.75573\n",
      "[107]\tvalidation_0-mlogloss:0.75183\n",
      "[108]\tvalidation_0-mlogloss:0.74902\n",
      "[109]\tvalidation_0-mlogloss:0.74536\n",
      "[110]\tvalidation_0-mlogloss:0.74244\n",
      "[111]\tvalidation_0-mlogloss:0.73850\n",
      "[112]\tvalidation_0-mlogloss:0.73518\n",
      "[113]\tvalidation_0-mlogloss:0.73151\n",
      "[114]\tvalidation_0-mlogloss:0.72871\n",
      "[115]\tvalidation_0-mlogloss:0.72499\n",
      "[116]\tvalidation_0-mlogloss:0.72156\n",
      "[117]\tvalidation_0-mlogloss:0.71854\n",
      "[118]\tvalidation_0-mlogloss:0.71569\n",
      "[119]\tvalidation_0-mlogloss:0.71324\n",
      "[120]\tvalidation_0-mlogloss:0.71003\n",
      "[121]\tvalidation_0-mlogloss:0.70692\n",
      "[122]\tvalidation_0-mlogloss:0.70394\n",
      "[123]\tvalidation_0-mlogloss:0.70095\n",
      "[124]\tvalidation_0-mlogloss:0.69861\n",
      "[125]\tvalidation_0-mlogloss:0.69582\n",
      "[126]\tvalidation_0-mlogloss:0.69241\n",
      "[127]\tvalidation_0-mlogloss:0.68981\n",
      "[128]\tvalidation_0-mlogloss:0.68681\n",
      "[129]\tvalidation_0-mlogloss:0.68421\n",
      "[130]\tvalidation_0-mlogloss:0.68078\n",
      "[131]\tvalidation_0-mlogloss:0.67866\n",
      "[132]\tvalidation_0-mlogloss:0.67599\n",
      "[133]\tvalidation_0-mlogloss:0.67348\n",
      "[134]\tvalidation_0-mlogloss:0.67181\n",
      "[135]\tvalidation_0-mlogloss:0.66841\n",
      "[136]\tvalidation_0-mlogloss:0.66631\n",
      "[137]\tvalidation_0-mlogloss:0.66401\n",
      "[138]\tvalidation_0-mlogloss:0.66118\n",
      "[139]\tvalidation_0-mlogloss:0.65921\n",
      "[140]\tvalidation_0-mlogloss:0.65715\n",
      "[141]\tvalidation_0-mlogloss:0.65439\n",
      "[142]\tvalidation_0-mlogloss:0.65307\n",
      "[143]\tvalidation_0-mlogloss:0.65091\n",
      "[144]\tvalidation_0-mlogloss:0.64935\n",
      "[145]\tvalidation_0-mlogloss:0.64720\n",
      "[146]\tvalidation_0-mlogloss:0.64464\n",
      "[147]\tvalidation_0-mlogloss:0.64295\n",
      "[148]\tvalidation_0-mlogloss:0.64112\n",
      "[149]\tvalidation_0-mlogloss:0.63858\n",
      "[150]\tvalidation_0-mlogloss:0.63585\n",
      "[151]\tvalidation_0-mlogloss:0.63404\n",
      "[152]\tvalidation_0-mlogloss:0.63252\n",
      "[153]\tvalidation_0-mlogloss:0.63110\n",
      "[154]\tvalidation_0-mlogloss:0.62899\n",
      "[155]\tvalidation_0-mlogloss:0.62737\n",
      "[156]\tvalidation_0-mlogloss:0.62458\n",
      "[157]\tvalidation_0-mlogloss:0.62270\n",
      "[158]\tvalidation_0-mlogloss:0.62100\n",
      "[159]\tvalidation_0-mlogloss:0.61934\n",
      "[160]\tvalidation_0-mlogloss:0.61771\n",
      "[161]\tvalidation_0-mlogloss:0.61609\n",
      "[162]\tvalidation_0-mlogloss:0.61521\n",
      "[163]\tvalidation_0-mlogloss:0.61399\n",
      "[164]\tvalidation_0-mlogloss:0.61215\n",
      "[165]\tvalidation_0-mlogloss:0.61038\n",
      "[166]\tvalidation_0-mlogloss:0.60919\n",
      "[167]\tvalidation_0-mlogloss:0.60722\n",
      "[168]\tvalidation_0-mlogloss:0.60578\n",
      "[169]\tvalidation_0-mlogloss:0.60434\n",
      "[170]\tvalidation_0-mlogloss:0.60207\n",
      "[171]\tvalidation_0-mlogloss:0.60059\n",
      "[172]\tvalidation_0-mlogloss:0.59902\n",
      "[173]\tvalidation_0-mlogloss:0.59759\n",
      "[174]\tvalidation_0-mlogloss:0.59609\n",
      "[175]\tvalidation_0-mlogloss:0.59546\n",
      "[176]\tvalidation_0-mlogloss:0.59433\n",
      "[177]\tvalidation_0-mlogloss:0.59256\n",
      "[178]\tvalidation_0-mlogloss:0.59087\n",
      "[179]\tvalidation_0-mlogloss:0.58937\n",
      "[180]\tvalidation_0-mlogloss:0.58827\n",
      "[181]\tvalidation_0-mlogloss:0.58648\n",
      "[182]\tvalidation_0-mlogloss:0.58529\n",
      "[183]\tvalidation_0-mlogloss:0.58348\n",
      "[184]\tvalidation_0-mlogloss:0.58194\n",
      "[185]\tvalidation_0-mlogloss:0.58057\n",
      "[186]\tvalidation_0-mlogloss:0.57883\n",
      "[187]\tvalidation_0-mlogloss:0.57780\n",
      "[188]\tvalidation_0-mlogloss:0.57685\n",
      "[189]\tvalidation_0-mlogloss:0.57510\n",
      "[190]\tvalidation_0-mlogloss:0.57396\n",
      "[191]\tvalidation_0-mlogloss:0.57243\n",
      "[192]\tvalidation_0-mlogloss:0.57138\n",
      "[193]\tvalidation_0-mlogloss:0.56951\n",
      "[194]\tvalidation_0-mlogloss:0.56793\n",
      "[195]\tvalidation_0-mlogloss:0.56643\n",
      "[196]\tvalidation_0-mlogloss:0.56570\n",
      "[197]\tvalidation_0-mlogloss:0.56424\n",
      "[198]\tvalidation_0-mlogloss:0.56322\n",
      "[199]\tvalidation_0-mlogloss:0.56206\n",
      "[200]\tvalidation_0-mlogloss:0.56094\n",
      "[201]\tvalidation_0-mlogloss:0.55954\n",
      "[202]\tvalidation_0-mlogloss:0.55832\n",
      "[203]\tvalidation_0-mlogloss:0.55702\n",
      "[204]\tvalidation_0-mlogloss:0.55563\n",
      "[205]\tvalidation_0-mlogloss:0.55453\n",
      "[206]\tvalidation_0-mlogloss:0.55341\n",
      "[207]\tvalidation_0-mlogloss:0.55196\n",
      "[208]\tvalidation_0-mlogloss:0.55077\n",
      "[209]\tvalidation_0-mlogloss:0.54965\n",
      "[210]\tvalidation_0-mlogloss:0.54828\n",
      "[211]\tvalidation_0-mlogloss:0.54709\n",
      "[212]\tvalidation_0-mlogloss:0.54556\n",
      "[213]\tvalidation_0-mlogloss:0.54388\n",
      "[214]\tvalidation_0-mlogloss:0.54402\n",
      "[215]\tvalidation_0-mlogloss:0.54264\n",
      "[216]\tvalidation_0-mlogloss:0.54251\n",
      "[217]\tvalidation_0-mlogloss:0.54132\n",
      "[218]\tvalidation_0-mlogloss:0.53978\n",
      "[219]\tvalidation_0-mlogloss:0.53869\n",
      "[220]\tvalidation_0-mlogloss:0.53767\n",
      "[221]\tvalidation_0-mlogloss:0.53658\n",
      "[222]\tvalidation_0-mlogloss:0.53458\n",
      "[223]\tvalidation_0-mlogloss:0.53385\n",
      "[224]\tvalidation_0-mlogloss:0.53322\n",
      "[225]\tvalidation_0-mlogloss:0.53247\n",
      "[226]\tvalidation_0-mlogloss:0.53131\n",
      "[227]\tvalidation_0-mlogloss:0.52975\n",
      "[228]\tvalidation_0-mlogloss:0.52866\n",
      "[229]\tvalidation_0-mlogloss:0.52771\n",
      "[230]\tvalidation_0-mlogloss:0.52686\n",
      "[231]\tvalidation_0-mlogloss:0.52602\n",
      "[232]\tvalidation_0-mlogloss:0.52538\n",
      "[233]\tvalidation_0-mlogloss:0.52414\n",
      "[234]\tvalidation_0-mlogloss:0.52280\n",
      "[235]\tvalidation_0-mlogloss:0.52220\n",
      "[236]\tvalidation_0-mlogloss:0.52097\n",
      "[237]\tvalidation_0-mlogloss:0.52002\n",
      "[238]\tvalidation_0-mlogloss:0.51940\n",
      "[239]\tvalidation_0-mlogloss:0.51863\n",
      "[240]\tvalidation_0-mlogloss:0.51746\n",
      "[241]\tvalidation_0-mlogloss:0.51676\n",
      "[242]\tvalidation_0-mlogloss:0.51598\n",
      "[243]\tvalidation_0-mlogloss:0.51535\n",
      "[244]\tvalidation_0-mlogloss:0.51425\n",
      "[245]\tvalidation_0-mlogloss:0.51353\n",
      "[246]\tvalidation_0-mlogloss:0.51298\n",
      "[247]\tvalidation_0-mlogloss:0.51252\n",
      "[248]\tvalidation_0-mlogloss:0.51140\n",
      "[249]\tvalidation_0-mlogloss:0.51059\n",
      "[250]\tvalidation_0-mlogloss:0.50944\n",
      "[251]\tvalidation_0-mlogloss:0.50845\n",
      "[252]\tvalidation_0-mlogloss:0.50728\n",
      "[253]\tvalidation_0-mlogloss:0.50684\n",
      "[254]\tvalidation_0-mlogloss:0.50639\n",
      "[255]\tvalidation_0-mlogloss:0.50574\n",
      "[256]\tvalidation_0-mlogloss:0.50513\n",
      "[257]\tvalidation_0-mlogloss:0.50457\n",
      "[258]\tvalidation_0-mlogloss:0.50409\n",
      "[259]\tvalidation_0-mlogloss:0.50308\n",
      "[260]\tvalidation_0-mlogloss:0.50203\n",
      "[261]\tvalidation_0-mlogloss:0.50164\n",
      "[262]\tvalidation_0-mlogloss:0.50102\n",
      "[263]\tvalidation_0-mlogloss:0.50072\n",
      "[264]\tvalidation_0-mlogloss:0.49986\n",
      "[265]\tvalidation_0-mlogloss:0.49917\n",
      "[266]\tvalidation_0-mlogloss:0.49835\n",
      "[267]\tvalidation_0-mlogloss:0.49752\n",
      "[268]\tvalidation_0-mlogloss:0.49676\n",
      "[269]\tvalidation_0-mlogloss:0.49588\n",
      "[270]\tvalidation_0-mlogloss:0.49555\n",
      "[271]\tvalidation_0-mlogloss:0.49435\n",
      "[272]\tvalidation_0-mlogloss:0.49384\n",
      "[273]\tvalidation_0-mlogloss:0.49317\n",
      "[274]\tvalidation_0-mlogloss:0.49244\n",
      "[275]\tvalidation_0-mlogloss:0.49204\n",
      "[276]\tvalidation_0-mlogloss:0.49127\n",
      "[277]\tvalidation_0-mlogloss:0.49055\n",
      "[278]\tvalidation_0-mlogloss:0.49014\n",
      "[279]\tvalidation_0-mlogloss:0.48978\n",
      "[280]\tvalidation_0-mlogloss:0.48961\n",
      "[281]\tvalidation_0-mlogloss:0.48879\n",
      "[282]\tvalidation_0-mlogloss:0.48796\n",
      "[283]\tvalidation_0-mlogloss:0.48778\n",
      "[284]\tvalidation_0-mlogloss:0.48691\n",
      "[285]\tvalidation_0-mlogloss:0.48640\n",
      "[286]\tvalidation_0-mlogloss:0.48560\n",
      "[287]\tvalidation_0-mlogloss:0.48514\n",
      "[288]\tvalidation_0-mlogloss:0.48459\n",
      "[289]\tvalidation_0-mlogloss:0.48390\n",
      "[290]\tvalidation_0-mlogloss:0.48339\n",
      "[291]\tvalidation_0-mlogloss:0.48291\n",
      "[292]\tvalidation_0-mlogloss:0.48246\n",
      "[293]\tvalidation_0-mlogloss:0.48216\n",
      "[294]\tvalidation_0-mlogloss:0.48151\n",
      "[295]\tvalidation_0-mlogloss:0.48069\n",
      "[296]\tvalidation_0-mlogloss:0.47991\n",
      "[297]\tvalidation_0-mlogloss:0.47964\n",
      "[298]\tvalidation_0-mlogloss:0.47897\n",
      "[299]\tvalidation_0-mlogloss:0.47849\n",
      "[300]\tvalidation_0-mlogloss:0.47816\n",
      "[301]\tvalidation_0-mlogloss:0.47742\n",
      "[302]\tvalidation_0-mlogloss:0.47693\n",
      "[303]\tvalidation_0-mlogloss:0.47641\n",
      "[304]\tvalidation_0-mlogloss:0.47602\n",
      "[305]\tvalidation_0-mlogloss:0.47539\n",
      "[306]\tvalidation_0-mlogloss:0.47482\n",
      "[307]\tvalidation_0-mlogloss:0.47412\n",
      "[308]\tvalidation_0-mlogloss:0.47363\n",
      "[309]\tvalidation_0-mlogloss:0.47310\n",
      "[310]\tvalidation_0-mlogloss:0.47252\n",
      "[311]\tvalidation_0-mlogloss:0.47213\n",
      "[312]\tvalidation_0-mlogloss:0.47170\n",
      "[313]\tvalidation_0-mlogloss:0.47146\n",
      "[314]\tvalidation_0-mlogloss:0.47111\n",
      "[315]\tvalidation_0-mlogloss:0.47065\n",
      "[316]\tvalidation_0-mlogloss:0.47057\n",
      "[317]\tvalidation_0-mlogloss:0.47010\n",
      "[318]\tvalidation_0-mlogloss:0.46944\n",
      "[319]\tvalidation_0-mlogloss:0.46898\n",
      "[320]\tvalidation_0-mlogloss:0.46854\n",
      "[321]\tvalidation_0-mlogloss:0.46829\n",
      "[322]\tvalidation_0-mlogloss:0.46749\n",
      "[323]\tvalidation_0-mlogloss:0.46721\n",
      "[324]\tvalidation_0-mlogloss:0.46651\n",
      "[325]\tvalidation_0-mlogloss:0.46633\n",
      "[326]\tvalidation_0-mlogloss:0.46586\n",
      "[327]\tvalidation_0-mlogloss:0.46543\n",
      "[328]\tvalidation_0-mlogloss:0.46484\n",
      "[329]\tvalidation_0-mlogloss:0.46426\n",
      "[330]\tvalidation_0-mlogloss:0.46408\n",
      "[331]\tvalidation_0-mlogloss:0.46364\n",
      "[332]\tvalidation_0-mlogloss:0.46312\n",
      "[333]\tvalidation_0-mlogloss:0.46282\n",
      "[334]\tvalidation_0-mlogloss:0.46239\n",
      "[335]\tvalidation_0-mlogloss:0.46177\n",
      "[336]\tvalidation_0-mlogloss:0.46151\n",
      "[337]\tvalidation_0-mlogloss:0.46139\n",
      "[338]\tvalidation_0-mlogloss:0.46109\n",
      "[339]\tvalidation_0-mlogloss:0.46056\n",
      "[340]\tvalidation_0-mlogloss:0.46012\n",
      "[341]\tvalidation_0-mlogloss:0.45966\n",
      "[342]\tvalidation_0-mlogloss:0.45935\n",
      "[343]\tvalidation_0-mlogloss:0.45904\n",
      "[344]\tvalidation_0-mlogloss:0.45844\n",
      "[345]\tvalidation_0-mlogloss:0.45817\n",
      "[346]\tvalidation_0-mlogloss:0.45775\n",
      "[347]\tvalidation_0-mlogloss:0.45725\n",
      "[348]\tvalidation_0-mlogloss:0.45705\n",
      "[349]\tvalidation_0-mlogloss:0.45642\n",
      "[350]\tvalidation_0-mlogloss:0.45602\n",
      "[351]\tvalidation_0-mlogloss:0.45598\n",
      "[352]\tvalidation_0-mlogloss:0.45564\n",
      "[353]\tvalidation_0-mlogloss:0.45495\n",
      "[354]\tvalidation_0-mlogloss:0.45456\n",
      "[355]\tvalidation_0-mlogloss:0.45395\n",
      "[356]\tvalidation_0-mlogloss:0.45361\n",
      "[357]\tvalidation_0-mlogloss:0.45334\n",
      "[358]\tvalidation_0-mlogloss:0.45296\n",
      "[359]\tvalidation_0-mlogloss:0.45241\n",
      "[360]\tvalidation_0-mlogloss:0.45221\n",
      "[361]\tvalidation_0-mlogloss:0.45195\n",
      "[362]\tvalidation_0-mlogloss:0.45177\n",
      "[363]\tvalidation_0-mlogloss:0.45116\n",
      "[364]\tvalidation_0-mlogloss:0.45095\n",
      "[365]\tvalidation_0-mlogloss:0.45057\n",
      "[366]\tvalidation_0-mlogloss:0.45011\n",
      "[367]\tvalidation_0-mlogloss:0.45006\n",
      "[368]\tvalidation_0-mlogloss:0.44971\n",
      "[369]\tvalidation_0-mlogloss:0.44911\n",
      "[370]\tvalidation_0-mlogloss:0.44874\n",
      "[371]\tvalidation_0-mlogloss:0.44827\n",
      "[372]\tvalidation_0-mlogloss:0.44798\n",
      "[373]\tvalidation_0-mlogloss:0.44758\n",
      "[374]\tvalidation_0-mlogloss:0.44731\n",
      "[375]\tvalidation_0-mlogloss:0.44714\n",
      "[376]\tvalidation_0-mlogloss:0.44697\n",
      "[377]\tvalidation_0-mlogloss:0.44684\n",
      "[378]\tvalidation_0-mlogloss:0.44642\n",
      "[379]\tvalidation_0-mlogloss:0.44609\n",
      "[380]\tvalidation_0-mlogloss:0.44575\n",
      "[381]\tvalidation_0-mlogloss:0.44532\n",
      "[382]\tvalidation_0-mlogloss:0.44511\n",
      "[383]\tvalidation_0-mlogloss:0.44480\n",
      "[384]\tvalidation_0-mlogloss:0.44455\n",
      "[385]\tvalidation_0-mlogloss:0.44402\n",
      "[386]\tvalidation_0-mlogloss:0.44366\n",
      "[387]\tvalidation_0-mlogloss:0.44342\n",
      "[388]\tvalidation_0-mlogloss:0.44307\n",
      "[389]\tvalidation_0-mlogloss:0.44298\n",
      "[390]\tvalidation_0-mlogloss:0.44264\n",
      "[391]\tvalidation_0-mlogloss:0.44237\n",
      "[392]\tvalidation_0-mlogloss:0.44196\n",
      "[393]\tvalidation_0-mlogloss:0.44143\n",
      "[394]\tvalidation_0-mlogloss:0.44097\n",
      "[395]\tvalidation_0-mlogloss:0.44066\n",
      "[396]\tvalidation_0-mlogloss:0.44041\n",
      "[397]\tvalidation_0-mlogloss:0.43999\n",
      "[398]\tvalidation_0-mlogloss:0.43994\n",
      "[399]\tvalidation_0-mlogloss:0.43954\n",
      "[400]\tvalidation_0-mlogloss:0.43925\n",
      "[401]\tvalidation_0-mlogloss:0.43882\n",
      "[402]\tvalidation_0-mlogloss:0.43871\n",
      "[403]\tvalidation_0-mlogloss:0.43862\n",
      "[404]\tvalidation_0-mlogloss:0.43846\n",
      "[405]\tvalidation_0-mlogloss:0.43829\n",
      "[406]\tvalidation_0-mlogloss:0.43805\n",
      "[407]\tvalidation_0-mlogloss:0.43804\n",
      "[408]\tvalidation_0-mlogloss:0.43779\n",
      "[409]\tvalidation_0-mlogloss:0.43750\n",
      "[410]\tvalidation_0-mlogloss:0.43713\n",
      "[411]\tvalidation_0-mlogloss:0.43678\n",
      "[412]\tvalidation_0-mlogloss:0.43627\n",
      "[413]\tvalidation_0-mlogloss:0.43610\n",
      "[414]\tvalidation_0-mlogloss:0.43556\n",
      "[415]\tvalidation_0-mlogloss:0.43539\n",
      "[416]\tvalidation_0-mlogloss:0.43521\n",
      "[417]\tvalidation_0-mlogloss:0.43492\n",
      "[418]\tvalidation_0-mlogloss:0.43450\n",
      "[419]\tvalidation_0-mlogloss:0.43429\n",
      "[420]\tvalidation_0-mlogloss:0.43408\n",
      "[421]\tvalidation_0-mlogloss:0.43382\n",
      "[422]\tvalidation_0-mlogloss:0.43373\n",
      "[423]\tvalidation_0-mlogloss:0.43387\n",
      "[424]\tvalidation_0-mlogloss:0.43361\n",
      "[425]\tvalidation_0-mlogloss:0.43324\n",
      "[426]\tvalidation_0-mlogloss:0.43305\n",
      "[427]\tvalidation_0-mlogloss:0.43307\n",
      "[428]\tvalidation_0-mlogloss:0.43287\n",
      "[429]\tvalidation_0-mlogloss:0.43269\n",
      "[430]\tvalidation_0-mlogloss:0.43241\n",
      "[431]\tvalidation_0-mlogloss:0.43227\n",
      "[432]\tvalidation_0-mlogloss:0.43214\n",
      "[433]\tvalidation_0-mlogloss:0.43173\n",
      "[434]\tvalidation_0-mlogloss:0.43165\n",
      "[435]\tvalidation_0-mlogloss:0.43154\n",
      "[436]\tvalidation_0-mlogloss:0.43130\n",
      "[437]\tvalidation_0-mlogloss:0.43096\n",
      "[438]\tvalidation_0-mlogloss:0.43057\n",
      "[439]\tvalidation_0-mlogloss:0.43057\n",
      "[440]\tvalidation_0-mlogloss:0.43026\n",
      "[441]\tvalidation_0-mlogloss:0.43001\n",
      "[442]\tvalidation_0-mlogloss:0.43000\n",
      "[443]\tvalidation_0-mlogloss:0.42996\n",
      "[444]\tvalidation_0-mlogloss:0.42994\n",
      "[445]\tvalidation_0-mlogloss:0.43003\n",
      "[446]\tvalidation_0-mlogloss:0.42986\n",
      "[447]\tvalidation_0-mlogloss:0.42974\n",
      "[448]\tvalidation_0-mlogloss:0.42950\n",
      "[449]\tvalidation_0-mlogloss:0.42947\n",
      "[450]\tvalidation_0-mlogloss:0.42919\n",
      "[451]\tvalidation_0-mlogloss:0.42896\n",
      "[452]\tvalidation_0-mlogloss:0.42880\n",
      "[453]\tvalidation_0-mlogloss:0.42856\n",
      "[454]\tvalidation_0-mlogloss:0.42853\n",
      "[455]\tvalidation_0-mlogloss:0.42817\n",
      "[456]\tvalidation_0-mlogloss:0.42797\n",
      "[457]\tvalidation_0-mlogloss:0.42793\n",
      "[458]\tvalidation_0-mlogloss:0.42808\n",
      "[459]\tvalidation_0-mlogloss:0.42768\n",
      "[460]\tvalidation_0-mlogloss:0.42754\n",
      "[461]\tvalidation_0-mlogloss:0.42742\n",
      "[462]\tvalidation_0-mlogloss:0.42718\n",
      "[463]\tvalidation_0-mlogloss:0.42688\n",
      "[464]\tvalidation_0-mlogloss:0.42677\n",
      "[465]\tvalidation_0-mlogloss:0.42651\n",
      "[466]\tvalidation_0-mlogloss:0.42644\n",
      "[467]\tvalidation_0-mlogloss:0.42633\n",
      "[468]\tvalidation_0-mlogloss:0.42589\n",
      "[469]\tvalidation_0-mlogloss:0.42577\n",
      "[470]\tvalidation_0-mlogloss:0.42566\n",
      "[471]\tvalidation_0-mlogloss:0.42542\n",
      "[472]\tvalidation_0-mlogloss:0.42517\n",
      "[473]\tvalidation_0-mlogloss:0.42499\n",
      "[474]\tvalidation_0-mlogloss:0.42493\n",
      "[475]\tvalidation_0-mlogloss:0.42450\n",
      "[476]\tvalidation_0-mlogloss:0.42448\n",
      "[477]\tvalidation_0-mlogloss:0.42434\n",
      "[478]\tvalidation_0-mlogloss:0.42397\n",
      "[479]\tvalidation_0-mlogloss:0.42387\n",
      "[480]\tvalidation_0-mlogloss:0.42363\n",
      "[481]\tvalidation_0-mlogloss:0.42349\n",
      "[482]\tvalidation_0-mlogloss:0.42341\n",
      "[483]\tvalidation_0-mlogloss:0.42332\n",
      "[484]\tvalidation_0-mlogloss:0.42298\n",
      "[485]\tvalidation_0-mlogloss:0.42271\n",
      "[486]\tvalidation_0-mlogloss:0.42247\n",
      "[487]\tvalidation_0-mlogloss:0.42236\n",
      "[488]\tvalidation_0-mlogloss:0.42232\n",
      "[489]\tvalidation_0-mlogloss:0.42209\n",
      "[490]\tvalidation_0-mlogloss:0.42178\n",
      "[491]\tvalidation_0-mlogloss:0.42164\n",
      "[492]\tvalidation_0-mlogloss:0.42132\n",
      "[493]\tvalidation_0-mlogloss:0.42129\n",
      "[494]\tvalidation_0-mlogloss:0.42106\n",
      "[495]\tvalidation_0-mlogloss:0.42090\n",
      "[496]\tvalidation_0-mlogloss:0.42071\n",
      "[497]\tvalidation_0-mlogloss:0.42027\n",
      "[498]\tvalidation_0-mlogloss:0.42024\n",
      "[499]\tvalidation_0-mlogloss:0.42006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86      1499\n",
      "           1       0.96      0.98      0.97      1499\n",
      "           2       0.85      0.88      0.86      1499\n",
      "           3       0.93      0.94      0.93      1499\n",
      "           4       0.94      1.00      0.97      1499\n",
      "           5       0.95      0.98      0.96      1499\n",
      "           6       0.96      0.99      0.97      1499\n",
      "           7       0.97      0.90      0.93      1499\n",
      "           8       0.72      0.84      0.78      1499\n",
      "           9       0.94      0.94      0.94      1499\n",
      "          10       0.97      0.80      0.88      1499\n",
      "          11       0.96      0.99      0.97      1499\n",
      "          12       0.82      0.92      0.87      1499\n",
      "          13       0.85      0.85      0.85      1499\n",
      "          14       0.85      0.90      0.87      1499\n",
      "          15       0.96      0.83      0.89      1499\n",
      "          16       0.92      0.77      0.84      1499\n",
      "          17       0.82      0.88      0.85      1499\n",
      "          18       0.88      0.95      0.91      1499\n",
      "          19       0.78      0.69      0.73      1499\n",
      "          20       0.97      0.98      0.97      1499\n",
      "          21       0.85      0.90      0.87      1499\n",
      "          22       0.80      0.93      0.86      1499\n",
      "          23       0.99      0.99      0.99      1499\n",
      "          24       0.93      0.82      0.87      1499\n",
      "          25       0.93      0.81      0.87      1499\n",
      "          26       0.85      0.73      0.78      1499\n",
      "          27       0.74      0.74      0.74      1499\n",
      "          28       0.79      0.93      0.85      1499\n",
      "          29       0.95      0.92      0.93      1499\n",
      "          30       0.86      0.98      0.92      1499\n",
      "          31       0.88      0.96      0.92      1499\n",
      "          32       0.87      0.97      0.92      1499\n",
      "          33       0.93      0.75      0.83      1499\n",
      "          34       0.99      1.00      0.99      1499\n",
      "          35       0.87      0.90      0.88      1499\n",
      "          36       0.93      0.91      0.92      1499\n",
      "          37       0.88      0.92      0.90      1499\n",
      "          38       0.84      0.48      0.61      1499\n",
      "          39       0.95      0.88      0.91      1499\n",
      "          40       0.95      0.91      0.93      1499\n",
      "          41       0.91      0.84      0.87      1499\n",
      "          42       0.94      0.90      0.92      1499\n",
      "          43       0.89      0.92      0.90      1499\n",
      "          44       0.92      0.98      0.95      1499\n",
      "          45       0.81      0.79      0.80      1499\n",
      "          46       0.77      0.90      0.83      1499\n",
      "          47       0.88      0.79      0.83      1499\n",
      "          48       0.86      0.91      0.89      1499\n",
      "          49       0.88      0.90      0.89      1499\n",
      "\n",
      "    accuracy                           0.89     74950\n",
      "   macro avg       0.89      0.89      0.88     74950\n",
      "weighted avg       0.89      0.89      0.88     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1dad4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3361defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 13:52:03.722677: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 153600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 42s 2ms/step - loss: 0.7872 - val_loss: 1.6301\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 45s 2ms/step - loss: 0.2821 - val_loss: 1.5910\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 45s 2ms/step - loss: 0.2059 - val_loss: 1.5886\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 45s 2ms/step - loss: 0.1695 - val_loss: 1.6502\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 45s 2ms/step - loss: 0.1453 - val_loss: 1.7106\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 43s 2ms/step - loss: 0.1274 - val_loss: 1.7814\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 43s 2ms/step - loss: 0.1152 - val_loss: 1.5659\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 42s 2ms/step - loss: 0.1045 - val_loss: 1.5328\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 45s 2ms/step - loss: 0.0964 - val_loss: 1.5563\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 43s 2ms/step - loss: 0.0895 - val_loss: 1.4618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe2fb7d6fe0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  37/2343 [..............................] - ETA: 3s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 3s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88      1499\n",
      "           1       0.87      0.95      0.91      1499\n",
      "           2       0.79      0.77      0.78      1499\n",
      "           3       0.86      0.90      0.88      1499\n",
      "           4       0.98      0.99      0.99      1499\n",
      "           5       0.88      0.93      0.91      1499\n",
      "           6       0.94      0.96      0.95      1499\n",
      "           7       0.74      0.76      0.75      1499\n",
      "           8       0.65      0.85      0.74      1499\n",
      "           9       0.81      0.91      0.86      1499\n",
      "          10       0.86      0.39      0.54      1499\n",
      "          11       0.89      0.97      0.93      1499\n",
      "          12       0.82      0.87      0.85      1499\n",
      "          13       0.58      0.85      0.69      1499\n",
      "          14       0.69      0.84      0.76      1499\n",
      "          15       0.88      0.61      0.72      1499\n",
      "          16       0.78      0.65      0.71      1499\n",
      "          17       0.94      0.80      0.86      1499\n",
      "          18       0.79      0.93      0.85      1499\n",
      "          19       0.58      0.44      0.50      1499\n",
      "          20       1.00      0.97      0.98      1499\n",
      "          21       0.79      0.75      0.77      1499\n",
      "          22       0.90      0.81      0.85      1499\n",
      "          23       0.93      1.00      0.96      1499\n",
      "          24       0.95      0.62      0.75      1499\n",
      "          25       0.80      0.75      0.78      1499\n",
      "          26       0.73      0.47      0.58      1499\n",
      "          27       0.73      0.59      0.65      1499\n",
      "          28       0.79      0.76      0.78      1499\n",
      "          29       0.96      0.87      0.91      1499\n",
      "          30       0.89      0.95      0.92      1499\n",
      "          31       0.69      0.97      0.81      1499\n",
      "          32       0.94      0.93      0.93      1499\n",
      "          33       0.80      0.70      0.74      1499\n",
      "          34       1.00      0.99      0.99      1499\n",
      "          35       0.92      0.77      0.84      1499\n",
      "          36       0.65      0.92      0.76      1499\n",
      "          37       0.82      0.89      0.86      1499\n",
      "          38       0.57      0.68      0.62      1499\n",
      "          39       0.61      0.73      0.67      1499\n",
      "          40       0.92      0.65      0.76      1499\n",
      "          41       0.71      0.77      0.74      1499\n",
      "          42       0.76      0.80      0.78      1499\n",
      "          43       0.79      0.89      0.84      1499\n",
      "          44       0.94      0.86      0.90      1499\n",
      "          45       0.70      0.53      0.60      1499\n",
      "          46       0.68      0.76      0.72      1499\n",
      "          47       0.86      0.54      0.66      1499\n",
      "          48       0.65      0.92      0.76      1499\n",
      "          49       0.71      0.82      0.76      1499\n",
      "\n",
      "    accuracy                           0.80     74950\n",
      "   macro avg       0.81      0.80      0.79     74950\n",
      "weighted avg       0.81      0.80      0.79     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 50 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 16:24:15.733698: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 16:24:15.765957: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 16:24:15.766005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 16:24:15.767081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 16:24:15.772859: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 16:24:15.773387: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 16:24:16.725977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081a3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.6.3 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.7.0)\n",
      "Requirement already satisfied: tqdm in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (4.64.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.7.0)\n",
      "Requirement already satisfied: decorator in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: packaging in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (22.0)\n",
      "Requirement already satisfied: jinja2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (3.8.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from jinja2->mne) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files3/S001R05.edf',\n",
       " 'files3/S002R05.edf',\n",
       " 'files3/S003R05.edf',\n",
       " 'files3/S004R05.edf',\n",
       " 'files3/S005R05.edf',\n",
       " 'files3/S006R05.edf',\n",
       " 'files3/S007R05.edf',\n",
       " 'files3/S008R05.edf',\n",
       " 'files3/S009R05.edf',\n",
       " 'files3/S010R05.edf',\n",
       " 'files3/S011R05.edf',\n",
       " 'files3/S012R05.edf',\n",
       " 'files3/S013R05.edf',\n",
       " 'files3/S014R05.edf',\n",
       " 'files3/S015R05.edf',\n",
       " 'files3/S016R05.edf',\n",
       " 'files3/S017R05.edf',\n",
       " 'files3/S018R05.edf',\n",
       " 'files3/S019R05.edf',\n",
       " 'files3/S020R05.edf',\n",
       " 'files3/S021R05.edf',\n",
       " 'files3/S022R05.edf',\n",
       " 'files3/S023R05.edf',\n",
       " 'files3/S024R05.edf',\n",
       " 'files3/S025R05.edf',\n",
       " 'files3/S026R05.edf',\n",
       " 'files3/S027R05.edf',\n",
       " 'files3/S028R05.edf',\n",
       " 'files3/S029R05.edf',\n",
       " 'files3/S030R05.edf',\n",
       " 'files3/S031R05.edf',\n",
       " 'files3/S032R05.edf',\n",
       " 'files3/S033R05.edf',\n",
       " 'files3/S034R05.edf',\n",
       " 'files3/S035R05.edf',\n",
       " 'files3/S036R05.edf',\n",
       " 'files3/S037R05.edf',\n",
       " 'files3/S038R05.edf',\n",
       " 'files3/S039R05.edf',\n",
       " 'files3/S040R05.edf',\n",
       " 'files3/S041R05.edf',\n",
       " 'files3/S042R05.edf',\n",
       " 'files3/S043R05.edf',\n",
       " 'files3/S044R05.edf',\n",
       " 'files3/S045R05.edf',\n",
       " 'files3/S046R05.edf',\n",
       " 'files3/S047R05.edf',\n",
       " 'files3/S048R05.edf',\n",
       " 'files3/S049R05.edf',\n",
       " 'files3/S050R05.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files3/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 74950, 600000, 74950)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.01e-04  1.30e-05 -1.50e-05 ... -1.20e-05 -7.00e-06  1.50e-05]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000010   0.000020   0.000007  -0.000018   0.000000   0.000004   \n",
       "1        0.000010   0.000050   0.000049   0.000022   0.000038   0.000031   \n",
       "2        0.000017   0.000055   0.000059   0.000030   0.000037   0.000025   \n",
       "3        0.000021   0.000063   0.000066   0.000035   0.000040   0.000029   \n",
       "4        0.000027   0.000072   0.000079   0.000048   0.000052   0.000042   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995  -0.000016  -0.000025  -0.000018  -0.000031  -0.000033  -0.000026   \n",
       "599996  -0.000005  -0.000013  -0.000003  -0.000014  -0.000016  -0.000008   \n",
       "599997  -0.000003  -0.000006  -0.000002  -0.000011  -0.000008  -0.000004   \n",
       "599998  -0.000005  -0.000015  -0.000001  -0.000014  -0.000021  -0.000011   \n",
       "599999   0.000013   0.000016   0.000023   0.000013   0.000010   0.000017   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000060   0.000004   0.000012   0.000006  ...   -0.000006   \n",
       "1        0.000084   0.000025   0.000043   0.000049  ...    0.000001   \n",
       "2        0.000079   0.000030   0.000048   0.000053  ...    0.000006   \n",
       "3        0.000076   0.000035   0.000055   0.000065  ...    0.000029   \n",
       "4        0.000091   0.000043   0.000060   0.000076  ...    0.000035   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995  -0.000028   0.000004  -0.000003   0.000007  ...    0.000021   \n",
       "599996  -0.000009   0.000018   0.000002   0.000016  ...    0.000014   \n",
       "599997  -0.000003   0.000011  -0.000007   0.000011  ...   -0.000003   \n",
       "599998  -0.000008  -0.000008  -0.000010   0.000006  ...    0.000034   \n",
       "599999   0.000016   0.000032   0.000013   0.000029  ...    0.000017   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000030    0.000025    0.000022    0.000013   -0.000009   \n",
       "1         0.000044    0.000031    0.000027    0.000025    0.000012   \n",
       "2         0.000048    0.000036    0.000033    0.000034    0.000034   \n",
       "3         0.000048    0.000041    0.000050    0.000059    0.000063   \n",
       "4         0.000029    0.000022    0.000042    0.000062    0.000071   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995    0.000016    0.000020    0.000014    0.000021    0.000011   \n",
       "599996   -0.000002   -0.000002    0.000001    0.000010    0.000003   \n",
       "599997   -0.000024   -0.000021   -0.000022   -0.000008   -0.000015   \n",
       "599998    0.000008    0.000013    0.000018    0.000034    0.000036   \n",
       "599999    0.000005    0.000002   -0.000004   -0.000001   -0.000015   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000020    0.000024    0.000089    0.000068  \n",
       "1         0.000030    0.000025    0.000083    0.000065  \n",
       "2         0.000041    0.000034    0.000092    0.000078  \n",
       "3         0.000045    0.000047    0.000121    0.000086  \n",
       "4         0.000037    0.000051    0.000143    0.000090  \n",
       "...            ...         ...         ...         ...  \n",
       "599995    0.000018    0.000014    0.000019    0.000027  \n",
       "599996   -0.000001   -0.000005    0.000005    0.000013  \n",
       "599997   -0.000020   -0.000019   -0.000008    0.000004  \n",
       "599998    0.000020    0.000037    0.000054    0.000072  \n",
       "599999    0.000001   -0.000025   -0.000011   -0.000015  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20950/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_20950/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_20950/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000010   0.000020   0.000007  -0.000018   0.000000   0.000004   \n",
       "1        0.000010   0.000050   0.000049   0.000022   0.000038   0.000031   \n",
       "2        0.000017   0.000055   0.000059   0.000030   0.000037   0.000025   \n",
       "3        0.000021   0.000063   0.000066   0.000035   0.000040   0.000029   \n",
       "4        0.000027   0.000072   0.000079   0.000048   0.000052   0.000042   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995  -0.000016  -0.000025  -0.000018  -0.000031  -0.000033  -0.000026   \n",
       "599996  -0.000005  -0.000013  -0.000003  -0.000014  -0.000016  -0.000008   \n",
       "599997  -0.000003  -0.000006  -0.000002  -0.000011  -0.000008  -0.000004   \n",
       "599998  -0.000005  -0.000015  -0.000001  -0.000014  -0.000021  -0.000011   \n",
       "599999   0.000013   0.000016   0.000023   0.000013   0.000010   0.000017   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000060   0.000004   0.000012   0.000006  ...   -0.000006   \n",
       "1        0.000084   0.000025   0.000043   0.000049  ...    0.000001   \n",
       "2        0.000079   0.000030   0.000048   0.000053  ...    0.000006   \n",
       "3        0.000076   0.000035   0.000055   0.000065  ...    0.000029   \n",
       "4        0.000091   0.000043   0.000060   0.000076  ...    0.000035   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995  -0.000028   0.000004  -0.000003   0.000007  ...    0.000021   \n",
       "599996  -0.000009   0.000018   0.000002   0.000016  ...    0.000014   \n",
       "599997  -0.000003   0.000011  -0.000007   0.000011  ...   -0.000003   \n",
       "599998  -0.000008  -0.000008  -0.000010   0.000006  ...    0.000034   \n",
       "599999   0.000016   0.000032   0.000013   0.000029  ...    0.000017   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000030    0.000025    0.000022    0.000013   -0.000009   \n",
       "1         0.000044    0.000031    0.000027    0.000025    0.000012   \n",
       "2         0.000048    0.000036    0.000033    0.000034    0.000034   \n",
       "3         0.000048    0.000041    0.000050    0.000059    0.000063   \n",
       "4         0.000029    0.000022    0.000042    0.000062    0.000071   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995    0.000016    0.000020    0.000014    0.000021    0.000011   \n",
       "599996   -0.000002   -0.000002    0.000001    0.000010    0.000003   \n",
       "599997   -0.000024   -0.000021   -0.000022   -0.000008   -0.000015   \n",
       "599998    0.000008    0.000013    0.000018    0.000034    0.000036   \n",
       "599999    0.000005    0.000002   -0.000004   -0.000001   -0.000015   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000020    0.000024    0.000089    0.000068  \n",
       "1         0.000030    0.000025    0.000083    0.000065  \n",
       "2         0.000041    0.000034    0.000092    0.000078  \n",
       "3         0.000045    0.000047    0.000121    0.000086  \n",
       "4         0.000037    0.000051    0.000143    0.000090  \n",
       "...            ...         ...         ...         ...  \n",
       "599995    0.000018    0.000014    0.000019    0.000027  \n",
       "599996   -0.000001   -0.000005    0.000005    0.000013  \n",
       "599997   -0.000020   -0.000019   -0.000008    0.000004  \n",
       "599998    0.000020    0.000037    0.000054    0.000072  \n",
       "599999    0.000001   -0.000025   -0.000011   -0.000015  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.62314\n",
      "[1]\tvalidation_0-mlogloss:3.50683\n",
      "[2]\tvalidation_0-mlogloss:3.42044\n",
      "[3]\tvalidation_0-mlogloss:3.35586\n",
      "[4]\tvalidation_0-mlogloss:3.30245\n",
      "[5]\tvalidation_0-mlogloss:3.25946\n",
      "[6]\tvalidation_0-mlogloss:3.22365\n",
      "[7]\tvalidation_0-mlogloss:3.19035\n",
      "[8]\tvalidation_0-mlogloss:3.16422\n",
      "[9]\tvalidation_0-mlogloss:3.13997\n",
      "[10]\tvalidation_0-mlogloss:3.11837\n",
      "[11]\tvalidation_0-mlogloss:3.09870\n",
      "[12]\tvalidation_0-mlogloss:3.08202\n",
      "[13]\tvalidation_0-mlogloss:3.06454\n",
      "[14]\tvalidation_0-mlogloss:3.04789\n",
      "[15]\tvalidation_0-mlogloss:3.03457\n",
      "[16]\tvalidation_0-mlogloss:3.01966\n",
      "[17]\tvalidation_0-mlogloss:3.00399\n",
      "[18]\tvalidation_0-mlogloss:2.99344\n",
      "[19]\tvalidation_0-mlogloss:2.98219\n",
      "[20]\tvalidation_0-mlogloss:2.96891\n",
      "[21]\tvalidation_0-mlogloss:2.95982\n",
      "[22]\tvalidation_0-mlogloss:2.94947\n",
      "[23]\tvalidation_0-mlogloss:2.93916\n",
      "[24]\tvalidation_0-mlogloss:2.92707\n",
      "[25]\tvalidation_0-mlogloss:2.91699\n",
      "[26]\tvalidation_0-mlogloss:2.91043\n",
      "[27]\tvalidation_0-mlogloss:2.90048\n",
      "[28]\tvalidation_0-mlogloss:2.89109\n",
      "[29]\tvalidation_0-mlogloss:2.88320\n",
      "[30]\tvalidation_0-mlogloss:2.87205\n",
      "[31]\tvalidation_0-mlogloss:2.86549\n",
      "[32]\tvalidation_0-mlogloss:2.85728\n",
      "[33]\tvalidation_0-mlogloss:2.84967\n",
      "[34]\tvalidation_0-mlogloss:2.84340\n",
      "[35]\tvalidation_0-mlogloss:2.83621\n",
      "[36]\tvalidation_0-mlogloss:2.82847\n",
      "[37]\tvalidation_0-mlogloss:2.82093\n",
      "[38]\tvalidation_0-mlogloss:2.81515\n",
      "[39]\tvalidation_0-mlogloss:2.80753\n",
      "[40]\tvalidation_0-mlogloss:2.80011\n",
      "[41]\tvalidation_0-mlogloss:2.79263\n",
      "[42]\tvalidation_0-mlogloss:2.78793\n",
      "[43]\tvalidation_0-mlogloss:2.78221\n",
      "[44]\tvalidation_0-mlogloss:2.77754\n",
      "[45]\tvalidation_0-mlogloss:2.77077\n",
      "[46]\tvalidation_0-mlogloss:2.76402\n",
      "[47]\tvalidation_0-mlogloss:2.75819\n",
      "[48]\tvalidation_0-mlogloss:2.75255\n",
      "[49]\tvalidation_0-mlogloss:2.74655\n",
      "[50]\tvalidation_0-mlogloss:2.74144\n",
      "[51]\tvalidation_0-mlogloss:2.73586\n",
      "[52]\tvalidation_0-mlogloss:2.72948\n",
      "[53]\tvalidation_0-mlogloss:2.72470\n",
      "[54]\tvalidation_0-mlogloss:2.72112\n",
      "[55]\tvalidation_0-mlogloss:2.71703\n",
      "[56]\tvalidation_0-mlogloss:2.71248\n",
      "[57]\tvalidation_0-mlogloss:2.70827\n",
      "[58]\tvalidation_0-mlogloss:2.70476\n",
      "[59]\tvalidation_0-mlogloss:2.70024\n",
      "[60]\tvalidation_0-mlogloss:2.69492\n",
      "[61]\tvalidation_0-mlogloss:2.69187\n",
      "[62]\tvalidation_0-mlogloss:2.68722\n",
      "[63]\tvalidation_0-mlogloss:2.68368\n",
      "[64]\tvalidation_0-mlogloss:2.67954\n",
      "[65]\tvalidation_0-mlogloss:2.67646\n",
      "[66]\tvalidation_0-mlogloss:2.67371\n",
      "[67]\tvalidation_0-mlogloss:2.67076\n",
      "[68]\tvalidation_0-mlogloss:2.66635\n",
      "[69]\tvalidation_0-mlogloss:2.66149\n",
      "[70]\tvalidation_0-mlogloss:2.65908\n",
      "[71]\tvalidation_0-mlogloss:2.65425\n",
      "[72]\tvalidation_0-mlogloss:2.65202\n",
      "[73]\tvalidation_0-mlogloss:2.64816\n",
      "[74]\tvalidation_0-mlogloss:2.64449\n",
      "[75]\tvalidation_0-mlogloss:2.64073\n",
      "[76]\tvalidation_0-mlogloss:2.63720\n",
      "[77]\tvalidation_0-mlogloss:2.63320\n",
      "[78]\tvalidation_0-mlogloss:2.63088\n",
      "[79]\tvalidation_0-mlogloss:2.62874\n",
      "[80]\tvalidation_0-mlogloss:2.62551\n",
      "[81]\tvalidation_0-mlogloss:2.62203\n",
      "[82]\tvalidation_0-mlogloss:2.61945\n",
      "[83]\tvalidation_0-mlogloss:2.61792\n",
      "[84]\tvalidation_0-mlogloss:2.61479\n",
      "[85]\tvalidation_0-mlogloss:2.61122\n",
      "[86]\tvalidation_0-mlogloss:2.60896\n",
      "[87]\tvalidation_0-mlogloss:2.60686\n",
      "[88]\tvalidation_0-mlogloss:2.60424\n",
      "[89]\tvalidation_0-mlogloss:2.60200\n",
      "[90]\tvalidation_0-mlogloss:2.60000\n",
      "[91]\tvalidation_0-mlogloss:2.59638\n",
      "[92]\tvalidation_0-mlogloss:2.59317\n",
      "[93]\tvalidation_0-mlogloss:2.59091\n",
      "[94]\tvalidation_0-mlogloss:2.58849\n",
      "[95]\tvalidation_0-mlogloss:2.58635\n",
      "[96]\tvalidation_0-mlogloss:2.58287\n",
      "[97]\tvalidation_0-mlogloss:2.58075\n",
      "[98]\tvalidation_0-mlogloss:2.57721\n",
      "[99]\tvalidation_0-mlogloss:2.57563\n",
      "[100]\tvalidation_0-mlogloss:2.57377\n",
      "[101]\tvalidation_0-mlogloss:2.57174\n",
      "[102]\tvalidation_0-mlogloss:2.57001\n",
      "[103]\tvalidation_0-mlogloss:2.56869\n",
      "[104]\tvalidation_0-mlogloss:2.56670\n",
      "[105]\tvalidation_0-mlogloss:2.56520\n",
      "[106]\tvalidation_0-mlogloss:2.56281\n",
      "[107]\tvalidation_0-mlogloss:2.56199\n",
      "[108]\tvalidation_0-mlogloss:2.56021\n",
      "[109]\tvalidation_0-mlogloss:2.55901\n",
      "[110]\tvalidation_0-mlogloss:2.55791\n",
      "[111]\tvalidation_0-mlogloss:2.55619\n",
      "[112]\tvalidation_0-mlogloss:2.55339\n",
      "[113]\tvalidation_0-mlogloss:2.55201\n",
      "[114]\tvalidation_0-mlogloss:2.55117\n",
      "[115]\tvalidation_0-mlogloss:2.54917\n",
      "[116]\tvalidation_0-mlogloss:2.54645\n",
      "[117]\tvalidation_0-mlogloss:2.54382\n",
      "[118]\tvalidation_0-mlogloss:2.54228\n",
      "[119]\tvalidation_0-mlogloss:2.53931\n",
      "[120]\tvalidation_0-mlogloss:2.53759\n",
      "[121]\tvalidation_0-mlogloss:2.53600\n",
      "[122]\tvalidation_0-mlogloss:2.53395\n",
      "[123]\tvalidation_0-mlogloss:2.53341\n",
      "[124]\tvalidation_0-mlogloss:2.53166\n",
      "[125]\tvalidation_0-mlogloss:2.52963\n",
      "[126]\tvalidation_0-mlogloss:2.52795\n",
      "[127]\tvalidation_0-mlogloss:2.52658\n",
      "[128]\tvalidation_0-mlogloss:2.52466\n",
      "[129]\tvalidation_0-mlogloss:2.52301\n",
      "[130]\tvalidation_0-mlogloss:2.52068\n",
      "[131]\tvalidation_0-mlogloss:2.51880\n",
      "[132]\tvalidation_0-mlogloss:2.51741\n",
      "[133]\tvalidation_0-mlogloss:2.51621\n",
      "[134]\tvalidation_0-mlogloss:2.51402\n",
      "[135]\tvalidation_0-mlogloss:2.51196\n",
      "[136]\tvalidation_0-mlogloss:2.51164\n",
      "[137]\tvalidation_0-mlogloss:2.51059\n",
      "[138]\tvalidation_0-mlogloss:2.50927\n",
      "[139]\tvalidation_0-mlogloss:2.50776\n",
      "[140]\tvalidation_0-mlogloss:2.50697\n",
      "[141]\tvalidation_0-mlogloss:2.50623\n",
      "[142]\tvalidation_0-mlogloss:2.50506\n",
      "[143]\tvalidation_0-mlogloss:2.50456\n",
      "[144]\tvalidation_0-mlogloss:2.50302\n",
      "[145]\tvalidation_0-mlogloss:2.50213\n",
      "[146]\tvalidation_0-mlogloss:2.50119\n",
      "[147]\tvalidation_0-mlogloss:2.50007\n",
      "[148]\tvalidation_0-mlogloss:2.49836\n",
      "[149]\tvalidation_0-mlogloss:2.49717\n",
      "[150]\tvalidation_0-mlogloss:2.49562\n",
      "[151]\tvalidation_0-mlogloss:2.49456\n",
      "[152]\tvalidation_0-mlogloss:2.49307\n",
      "[153]\tvalidation_0-mlogloss:2.49114\n",
      "[154]\tvalidation_0-mlogloss:2.49010\n",
      "[155]\tvalidation_0-mlogloss:2.48889\n",
      "[156]\tvalidation_0-mlogloss:2.48819\n",
      "[157]\tvalidation_0-mlogloss:2.48755\n",
      "[158]\tvalidation_0-mlogloss:2.48695\n",
      "[159]\tvalidation_0-mlogloss:2.48584\n",
      "[160]\tvalidation_0-mlogloss:2.48550\n",
      "[161]\tvalidation_0-mlogloss:2.48446\n",
      "[162]\tvalidation_0-mlogloss:2.48415\n",
      "[163]\tvalidation_0-mlogloss:2.48332\n",
      "[164]\tvalidation_0-mlogloss:2.48200\n",
      "[165]\tvalidation_0-mlogloss:2.48068\n",
      "[166]\tvalidation_0-mlogloss:2.47995\n",
      "[167]\tvalidation_0-mlogloss:2.47955\n",
      "[168]\tvalidation_0-mlogloss:2.47863\n",
      "[169]\tvalidation_0-mlogloss:2.47851\n",
      "[170]\tvalidation_0-mlogloss:2.47772\n",
      "[171]\tvalidation_0-mlogloss:2.47677\n",
      "[172]\tvalidation_0-mlogloss:2.47651\n",
      "[173]\tvalidation_0-mlogloss:2.47572\n",
      "[174]\tvalidation_0-mlogloss:2.47495\n",
      "[175]\tvalidation_0-mlogloss:2.47408\n",
      "[176]\tvalidation_0-mlogloss:2.47309\n",
      "[177]\tvalidation_0-mlogloss:2.47243\n",
      "[178]\tvalidation_0-mlogloss:2.47191\n",
      "[179]\tvalidation_0-mlogloss:2.47146\n",
      "[180]\tvalidation_0-mlogloss:2.47035\n",
      "[181]\tvalidation_0-mlogloss:2.46937\n",
      "[182]\tvalidation_0-mlogloss:2.46854\n",
      "[183]\tvalidation_0-mlogloss:2.46754\n",
      "[184]\tvalidation_0-mlogloss:2.46638\n",
      "[185]\tvalidation_0-mlogloss:2.46578\n",
      "[186]\tvalidation_0-mlogloss:2.46561\n",
      "[187]\tvalidation_0-mlogloss:2.46488\n",
      "[188]\tvalidation_0-mlogloss:2.46463\n",
      "[189]\tvalidation_0-mlogloss:2.46407\n",
      "[190]\tvalidation_0-mlogloss:2.46382\n",
      "[191]\tvalidation_0-mlogloss:2.46311\n",
      "[192]\tvalidation_0-mlogloss:2.46189\n",
      "[193]\tvalidation_0-mlogloss:2.46156\n",
      "[194]\tvalidation_0-mlogloss:2.46132\n",
      "[195]\tvalidation_0-mlogloss:2.46073\n",
      "[196]\tvalidation_0-mlogloss:2.45990\n",
      "[197]\tvalidation_0-mlogloss:2.45845\n",
      "[198]\tvalidation_0-mlogloss:2.45813\n",
      "[199]\tvalidation_0-mlogloss:2.45701\n",
      "[200]\tvalidation_0-mlogloss:2.45659\n",
      "[201]\tvalidation_0-mlogloss:2.45596\n",
      "[202]\tvalidation_0-mlogloss:2.45537\n",
      "[203]\tvalidation_0-mlogloss:2.45561\n",
      "[204]\tvalidation_0-mlogloss:2.45510\n",
      "[205]\tvalidation_0-mlogloss:2.45341\n",
      "[206]\tvalidation_0-mlogloss:2.45279\n",
      "[207]\tvalidation_0-mlogloss:2.45162\n",
      "[208]\tvalidation_0-mlogloss:2.45157\n",
      "[209]\tvalidation_0-mlogloss:2.45011\n",
      "[210]\tvalidation_0-mlogloss:2.44922\n",
      "[211]\tvalidation_0-mlogloss:2.44846\n",
      "[212]\tvalidation_0-mlogloss:2.44767\n",
      "[213]\tvalidation_0-mlogloss:2.44728\n",
      "[214]\tvalidation_0-mlogloss:2.44729\n",
      "[215]\tvalidation_0-mlogloss:2.44764\n",
      "[216]\tvalidation_0-mlogloss:2.44674\n",
      "[217]\tvalidation_0-mlogloss:2.44716\n",
      "[218]\tvalidation_0-mlogloss:2.44683\n",
      "[219]\tvalidation_0-mlogloss:2.44715\n",
      "[220]\tvalidation_0-mlogloss:2.44709\n",
      "[221]\tvalidation_0-mlogloss:2.44665\n",
      "[222]\tvalidation_0-mlogloss:2.44653\n",
      "[223]\tvalidation_0-mlogloss:2.44572\n",
      "[224]\tvalidation_0-mlogloss:2.44536\n",
      "[225]\tvalidation_0-mlogloss:2.44554\n",
      "[226]\tvalidation_0-mlogloss:2.44576\n",
      "[227]\tvalidation_0-mlogloss:2.44547\n",
      "[228]\tvalidation_0-mlogloss:2.44468\n",
      "[229]\tvalidation_0-mlogloss:2.44497\n",
      "[230]\tvalidation_0-mlogloss:2.44413\n",
      "[231]\tvalidation_0-mlogloss:2.44400\n",
      "[232]\tvalidation_0-mlogloss:2.44369\n",
      "[233]\tvalidation_0-mlogloss:2.44265\n",
      "[234]\tvalidation_0-mlogloss:2.44225\n",
      "[235]\tvalidation_0-mlogloss:2.44136\n",
      "[236]\tvalidation_0-mlogloss:2.44055\n",
      "[237]\tvalidation_0-mlogloss:2.44020\n",
      "[238]\tvalidation_0-mlogloss:2.43982\n",
      "[239]\tvalidation_0-mlogloss:2.43932\n",
      "[240]\tvalidation_0-mlogloss:2.43884\n",
      "[241]\tvalidation_0-mlogloss:2.43743\n",
      "[242]\tvalidation_0-mlogloss:2.43710\n",
      "[243]\tvalidation_0-mlogloss:2.43635\n",
      "[244]\tvalidation_0-mlogloss:2.43582\n",
      "[245]\tvalidation_0-mlogloss:2.43520\n",
      "[246]\tvalidation_0-mlogloss:2.43502\n",
      "[247]\tvalidation_0-mlogloss:2.43432\n",
      "[248]\tvalidation_0-mlogloss:2.43400\n",
      "[249]\tvalidation_0-mlogloss:2.43313\n",
      "[250]\tvalidation_0-mlogloss:2.43254\n",
      "[251]\tvalidation_0-mlogloss:2.43235\n",
      "[252]\tvalidation_0-mlogloss:2.43185\n",
      "[253]\tvalidation_0-mlogloss:2.43138\n",
      "[254]\tvalidation_0-mlogloss:2.43108\n",
      "[255]\tvalidation_0-mlogloss:2.43082\n",
      "[256]\tvalidation_0-mlogloss:2.43032\n",
      "[257]\tvalidation_0-mlogloss:2.43033\n",
      "[258]\tvalidation_0-mlogloss:2.43032\n",
      "[259]\tvalidation_0-mlogloss:2.43008\n",
      "[260]\tvalidation_0-mlogloss:2.42988\n",
      "[261]\tvalidation_0-mlogloss:2.43045\n",
      "[262]\tvalidation_0-mlogloss:2.43018\n",
      "[263]\tvalidation_0-mlogloss:2.42999\n",
      "[264]\tvalidation_0-mlogloss:2.43015\n",
      "[265]\tvalidation_0-mlogloss:2.42992\n",
      "[266]\tvalidation_0-mlogloss:2.42919\n",
      "[267]\tvalidation_0-mlogloss:2.42869\n",
      "[268]\tvalidation_0-mlogloss:2.42812\n",
      "[269]\tvalidation_0-mlogloss:2.42774\n",
      "[270]\tvalidation_0-mlogloss:2.42707\n",
      "[271]\tvalidation_0-mlogloss:2.42700\n",
      "[272]\tvalidation_0-mlogloss:2.42654\n",
      "[273]\tvalidation_0-mlogloss:2.42675\n",
      "[274]\tvalidation_0-mlogloss:2.42632\n",
      "[275]\tvalidation_0-mlogloss:2.42586\n",
      "[276]\tvalidation_0-mlogloss:2.42529\n",
      "[277]\tvalidation_0-mlogloss:2.42558\n",
      "[278]\tvalidation_0-mlogloss:2.42486\n",
      "[279]\tvalidation_0-mlogloss:2.42457\n",
      "[280]\tvalidation_0-mlogloss:2.42418\n",
      "[281]\tvalidation_0-mlogloss:2.42413\n",
      "[282]\tvalidation_0-mlogloss:2.42338\n",
      "[283]\tvalidation_0-mlogloss:2.42341\n",
      "[284]\tvalidation_0-mlogloss:2.42333\n",
      "[285]\tvalidation_0-mlogloss:2.42311\n",
      "[286]\tvalidation_0-mlogloss:2.42271\n",
      "[287]\tvalidation_0-mlogloss:2.42219\n",
      "[288]\tvalidation_0-mlogloss:2.42172\n",
      "[289]\tvalidation_0-mlogloss:2.42158\n",
      "[290]\tvalidation_0-mlogloss:2.42070\n",
      "[291]\tvalidation_0-mlogloss:2.42050\n",
      "[292]\tvalidation_0-mlogloss:2.42040\n",
      "[293]\tvalidation_0-mlogloss:2.42097\n",
      "[294]\tvalidation_0-mlogloss:2.42085\n",
      "[295]\tvalidation_0-mlogloss:2.42091\n",
      "[296]\tvalidation_0-mlogloss:2.42066\n",
      "[297]\tvalidation_0-mlogloss:2.42030\n",
      "[298]\tvalidation_0-mlogloss:2.42020\n",
      "[299]\tvalidation_0-mlogloss:2.42021\n",
      "[300]\tvalidation_0-mlogloss:2.41984\n",
      "[301]\tvalidation_0-mlogloss:2.41915\n",
      "[302]\tvalidation_0-mlogloss:2.41911\n",
      "[303]\tvalidation_0-mlogloss:2.41935\n",
      "[304]\tvalidation_0-mlogloss:2.41872\n",
      "[305]\tvalidation_0-mlogloss:2.41848\n",
      "[306]\tvalidation_0-mlogloss:2.41847\n",
      "[307]\tvalidation_0-mlogloss:2.41813\n",
      "[308]\tvalidation_0-mlogloss:2.41820\n",
      "[309]\tvalidation_0-mlogloss:2.41767\n",
      "[310]\tvalidation_0-mlogloss:2.41790\n",
      "[311]\tvalidation_0-mlogloss:2.41739\n",
      "[312]\tvalidation_0-mlogloss:2.41711\n",
      "[313]\tvalidation_0-mlogloss:2.41677\n",
      "[314]\tvalidation_0-mlogloss:2.41662\n",
      "[315]\tvalidation_0-mlogloss:2.41660\n",
      "[316]\tvalidation_0-mlogloss:2.41649\n",
      "[317]\tvalidation_0-mlogloss:2.41608\n",
      "[318]\tvalidation_0-mlogloss:2.41624\n",
      "[319]\tvalidation_0-mlogloss:2.41612\n",
      "[320]\tvalidation_0-mlogloss:2.41600\n",
      "[321]\tvalidation_0-mlogloss:2.41616\n",
      "[322]\tvalidation_0-mlogloss:2.41632\n",
      "[323]\tvalidation_0-mlogloss:2.41559\n",
      "[324]\tvalidation_0-mlogloss:2.41489\n",
      "[325]\tvalidation_0-mlogloss:2.41491\n",
      "[326]\tvalidation_0-mlogloss:2.41454\n",
      "[327]\tvalidation_0-mlogloss:2.41429\n",
      "[328]\tvalidation_0-mlogloss:2.41478\n",
      "[329]\tvalidation_0-mlogloss:2.41479\n",
      "[330]\tvalidation_0-mlogloss:2.41476\n",
      "[331]\tvalidation_0-mlogloss:2.41469\n",
      "[332]\tvalidation_0-mlogloss:2.41431\n",
      "[333]\tvalidation_0-mlogloss:2.41420\n",
      "[334]\tvalidation_0-mlogloss:2.41424\n",
      "[335]\tvalidation_0-mlogloss:2.41467\n",
      "[336]\tvalidation_0-mlogloss:2.41455\n",
      "[337]\tvalidation_0-mlogloss:2.41455\n",
      "[338]\tvalidation_0-mlogloss:2.41444\n",
      "[339]\tvalidation_0-mlogloss:2.41453\n",
      "[340]\tvalidation_0-mlogloss:2.41497\n",
      "[341]\tvalidation_0-mlogloss:2.41536\n",
      "[342]\tvalidation_0-mlogloss:2.41500\n",
      "[343]\tvalidation_0-mlogloss:2.41509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.09      0.11      1499\n",
      "           1       0.14      0.12      0.13      1499\n",
      "           2       0.35      0.31      0.33      1499\n",
      "           3       0.40      0.33      0.36      1499\n",
      "           4       0.72      0.73      0.73      1499\n",
      "           5       0.31      0.44      0.36      1499\n",
      "           6       0.27      0.54      0.36      1499\n",
      "           7       0.44      0.33      0.37      1499\n",
      "           8       0.52      0.59      0.56      1499\n",
      "           9       0.47      0.44      0.46      1499\n",
      "          10       0.30      0.28      0.29      1499\n",
      "          11       0.37      0.32      0.34      1499\n",
      "          12       0.36      0.35      0.36      1499\n",
      "          13       0.28      0.25      0.26      1499\n",
      "          14       0.16      0.09      0.11      1499\n",
      "          15       0.59      0.68      0.63      1499\n",
      "          16       0.28      0.14      0.19      1499\n",
      "          17       0.39      0.36      0.38      1499\n",
      "          18       0.20      0.14      0.17      1499\n",
      "          19       0.18      0.18      0.18      1499\n",
      "          20       0.54      0.65      0.59      1499\n",
      "          21       0.51      0.56      0.53      1499\n",
      "          22       0.35      0.19      0.25      1499\n",
      "          23       0.65      0.70      0.67      1499\n",
      "          24       0.60      0.61      0.60      1499\n",
      "          25       0.29      0.30      0.29      1499\n",
      "          26       0.38      0.39      0.38      1499\n",
      "          27       0.37      0.40      0.38      1499\n",
      "          28       0.28      0.35      0.31      1499\n",
      "          29       0.33      0.60      0.42      1499\n",
      "          30       0.23      0.26      0.24      1499\n",
      "          31       0.37      0.33      0.35      1499\n",
      "          32       0.16      0.14      0.15      1499\n",
      "          33       0.22      0.21      0.22      1499\n",
      "          34       0.63      0.71      0.67      1499\n",
      "          35       0.51      0.54      0.52      1499\n",
      "          36       0.27      0.28      0.27      1499\n",
      "          37       0.18      0.14      0.16      1499\n",
      "          38       0.26      0.21      0.23      1499\n",
      "          39       0.41      0.41      0.41      1499\n",
      "          40       0.40      0.47      0.44      1499\n",
      "          41       0.29      0.39      0.33      1499\n",
      "          42       0.36      0.22      0.28      1499\n",
      "          43       0.29      0.26      0.27      1499\n",
      "          44       0.37      0.50      0.43      1499\n",
      "          45       0.24      0.20      0.22      1499\n",
      "          46       0.28      0.28      0.28      1499\n",
      "          47       0.54      0.63      0.58      1499\n",
      "          48       0.65      0.71      0.68      1499\n",
      "          49       0.25      0.17      0.20      1499\n",
      "\n",
      "    accuracy                           0.37     74950\n",
      "   macro avg       0.36      0.37      0.36     74950\n",
      "weighted avg       0.36      0.37      0.36     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b271d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 13s 669us/step - loss: 2.3515 - val_loss: 2.4842\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 14s 734us/step - loss: 1.8619 - val_loss: 2.4132\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 15s 819us/step - loss: 1.7733 - val_loss: 2.4211\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 16s 865us/step - loss: 1.6820 - val_loss: 2.3772\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 39s 2ms/step - loss: 1.6014 - val_loss: 2.4270\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 17s 892us/step - loss: 1.5581 - val_loss: 2.4179\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 15s 800us/step - loss: 1.5285 - val_loss: 2.3913\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 16s 830us/step - loss: 1.5089 - val_loss: 2.4746\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 16s 879us/step - loss: 1.4928 - val_loss: 2.4768\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 699s 37ms/step - loss: 1.4795 - val_loss: 2.4796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2aac9ef80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 1s 334us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.15      0.14      1499\n",
      "           1       0.11      0.09      0.10      1499\n",
      "           2       0.29      0.24      0.26      1499\n",
      "           3       0.55      0.27      0.36      1499\n",
      "           4       0.67      0.73      0.70      1499\n",
      "           5       0.38      0.42      0.40      1499\n",
      "           6       0.25      0.57      0.35      1499\n",
      "           7       0.47      0.39      0.43      1499\n",
      "           8       0.56      0.58      0.57      1499\n",
      "           9       0.54      0.46      0.50      1499\n",
      "          10       0.31      0.37      0.33      1499\n",
      "          11       0.33      0.39      0.36      1499\n",
      "          12       0.38      0.29      0.33      1499\n",
      "          13       0.20      0.23      0.21      1499\n",
      "          14       0.22      0.07      0.11      1499\n",
      "          15       0.44      0.46      0.45      1499\n",
      "          16       0.15      0.08      0.11      1499\n",
      "          17       0.54      0.20      0.30      1499\n",
      "          18       0.20      0.10      0.13      1499\n",
      "          19       0.20      0.12      0.15      1499\n",
      "          20       0.75      0.43      0.55      1499\n",
      "          21       0.51      0.61      0.55      1499\n",
      "          22       0.41      0.20      0.27      1499\n",
      "          23       0.59      0.72      0.65      1499\n",
      "          24       0.55      0.52      0.54      1499\n",
      "          25       0.34      0.22      0.27      1499\n",
      "          26       0.49      0.37      0.42      1499\n",
      "          27       0.35      0.43      0.38      1499\n",
      "          28       0.31      0.37      0.33      1499\n",
      "          29       0.41      0.63      0.50      1499\n",
      "          30       0.24      0.36      0.29      1499\n",
      "          31       0.39      0.24      0.29      1499\n",
      "          32       0.11      0.18      0.13      1499\n",
      "          33       0.32      0.13      0.19      1499\n",
      "          34       0.77      0.50      0.61      1499\n",
      "          35       0.55      0.60      0.58      1499\n",
      "          36       0.20      0.25      0.22      1499\n",
      "          37       0.15      0.23      0.19      1499\n",
      "          38       0.22      0.19      0.21      1499\n",
      "          39       0.33      0.44      0.38      1499\n",
      "          40       0.36      0.42      0.39      1499\n",
      "          41       0.29      0.24      0.27      1499\n",
      "          42       0.25      0.30      0.27      1499\n",
      "          43       0.31      0.34      0.32      1499\n",
      "          44       0.39      0.42      0.40      1499\n",
      "          45       0.18      0.35      0.24      1499\n",
      "          46       0.28      0.18      0.22      1499\n",
      "          47       0.59      0.63      0.61      1499\n",
      "          48       0.70      0.70      0.70      1499\n",
      "          49       0.17      0.20      0.18      1499\n",
      "\n",
      "    accuracy                           0.35     74950\n",
      "   macro avg       0.37      0.35      0.35     74950\n",
      "weighted avg       0.37      0.35      0.35     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20950/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_20950/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_20950/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000010   0.000020   0.000007  -0.000018   0.000000   0.000004   \n",
       "1        0.000010   0.000050   0.000049   0.000022   0.000038   0.000031   \n",
       "2        0.000017   0.000055   0.000059   0.000030   0.000037   0.000025   \n",
       "3        0.000021   0.000063   0.000066   0.000035   0.000040   0.000029   \n",
       "4        0.000027   0.000072   0.000079   0.000048   0.000052   0.000042   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995  -0.000016  -0.000025  -0.000018  -0.000031  -0.000033  -0.000026   \n",
       "599996  -0.000005  -0.000013  -0.000003  -0.000014  -0.000016  -0.000008   \n",
       "599997  -0.000003  -0.000006  -0.000002  -0.000011  -0.000008  -0.000004   \n",
       "599998  -0.000005  -0.000015  -0.000001  -0.000014  -0.000021  -0.000011   \n",
       "599999   0.000013   0.000016   0.000023   0.000013   0.000010   0.000017   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000060   0.000004   0.000012   0.000006  ...   -0.000006   \n",
       "1        0.000084   0.000025   0.000043   0.000049  ...    0.000001   \n",
       "2        0.000079   0.000030   0.000048   0.000053  ...    0.000006   \n",
       "3        0.000076   0.000035   0.000055   0.000065  ...    0.000029   \n",
       "4        0.000091   0.000043   0.000060   0.000076  ...    0.000035   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995  -0.000028   0.000004  -0.000003   0.000007  ...    0.000021   \n",
       "599996  -0.000009   0.000018   0.000002   0.000016  ...    0.000014   \n",
       "599997  -0.000003   0.000011  -0.000007   0.000011  ...   -0.000003   \n",
       "599998  -0.000008  -0.000008  -0.000010   0.000006  ...    0.000034   \n",
       "599999   0.000016   0.000032   0.000013   0.000029  ...    0.000017   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000030    0.000025    0.000022    0.000013   -0.000009   \n",
       "1         0.000044    0.000031    0.000027    0.000025    0.000012   \n",
       "2         0.000048    0.000036    0.000033    0.000034    0.000034   \n",
       "3         0.000048    0.000041    0.000050    0.000059    0.000063   \n",
       "4         0.000029    0.000022    0.000042    0.000062    0.000071   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995    0.000016    0.000020    0.000014    0.000021    0.000011   \n",
       "599996   -0.000002   -0.000002    0.000001    0.000010    0.000003   \n",
       "599997   -0.000024   -0.000021   -0.000022   -0.000008   -0.000015   \n",
       "599998    0.000008    0.000013    0.000018    0.000034    0.000036   \n",
       "599999    0.000005    0.000002   -0.000004   -0.000001   -0.000015   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000020    0.000024    0.000089    0.000068  \n",
       "1         0.000030    0.000025    0.000083    0.000065  \n",
       "2         0.000041    0.000034    0.000092    0.000078  \n",
       "3         0.000045    0.000047    0.000121    0.000086  \n",
       "4         0.000037    0.000051    0.000143    0.000090  \n",
       "...            ...         ...         ...         ...  \n",
       "599995    0.000018    0.000014    0.000019    0.000027  \n",
       "599996   -0.000001   -0.000005    0.000005    0.000013  \n",
       "599997   -0.000020   -0.000019   -0.000008    0.000004  \n",
       "599998    0.000020    0.000037    0.000054    0.000072  \n",
       "599999    0.000001   -0.000025   -0.000011   -0.000015  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.54984\n",
      "[1]\tvalidation_0-mlogloss:3.39544\n",
      "[2]\tvalidation_0-mlogloss:3.28727\n",
      "[3]\tvalidation_0-mlogloss:3.19921\n",
      "[4]\tvalidation_0-mlogloss:3.13020\n",
      "[5]\tvalidation_0-mlogloss:3.07111\n",
      "[6]\tvalidation_0-mlogloss:3.01865\n",
      "[7]\tvalidation_0-mlogloss:2.97252\n",
      "[8]\tvalidation_0-mlogloss:2.93814\n",
      "[9]\tvalidation_0-mlogloss:2.89854\n",
      "[10]\tvalidation_0-mlogloss:2.86595\n",
      "[11]\tvalidation_0-mlogloss:2.83768\n",
      "[12]\tvalidation_0-mlogloss:2.80821\n",
      "[13]\tvalidation_0-mlogloss:2.78426\n",
      "[14]\tvalidation_0-mlogloss:2.76289\n",
      "[15]\tvalidation_0-mlogloss:2.74434\n",
      "[16]\tvalidation_0-mlogloss:2.72400\n",
      "[17]\tvalidation_0-mlogloss:2.70546\n",
      "[18]\tvalidation_0-mlogloss:2.68571\n",
      "[19]\tvalidation_0-mlogloss:2.66621\n",
      "[20]\tvalidation_0-mlogloss:2.64900\n",
      "[21]\tvalidation_0-mlogloss:2.63569\n",
      "[22]\tvalidation_0-mlogloss:2.61768\n",
      "[23]\tvalidation_0-mlogloss:2.60113\n",
      "[24]\tvalidation_0-mlogloss:2.58854\n",
      "[25]\tvalidation_0-mlogloss:2.57486\n",
      "[26]\tvalidation_0-mlogloss:2.56252\n",
      "[27]\tvalidation_0-mlogloss:2.54736\n",
      "[28]\tvalidation_0-mlogloss:2.53425\n",
      "[29]\tvalidation_0-mlogloss:2.52329\n",
      "[30]\tvalidation_0-mlogloss:2.51156\n",
      "[31]\tvalidation_0-mlogloss:2.50081\n",
      "[32]\tvalidation_0-mlogloss:2.48830\n",
      "[33]\tvalidation_0-mlogloss:2.47708\n",
      "[34]\tvalidation_0-mlogloss:2.46629\n",
      "[35]\tvalidation_0-mlogloss:2.45532\n",
      "[36]\tvalidation_0-mlogloss:2.44253\n",
      "[37]\tvalidation_0-mlogloss:2.43053\n",
      "[38]\tvalidation_0-mlogloss:2.41953\n",
      "[39]\tvalidation_0-mlogloss:2.41081\n",
      "[40]\tvalidation_0-mlogloss:2.40200\n",
      "[41]\tvalidation_0-mlogloss:2.38828\n",
      "[42]\tvalidation_0-mlogloss:2.37656\n",
      "[43]\tvalidation_0-mlogloss:2.36717\n",
      "[44]\tvalidation_0-mlogloss:2.35655\n",
      "[45]\tvalidation_0-mlogloss:2.34645\n",
      "[46]\tvalidation_0-mlogloss:2.33832\n",
      "[47]\tvalidation_0-mlogloss:2.32981\n",
      "[48]\tvalidation_0-mlogloss:2.32052\n",
      "[49]\tvalidation_0-mlogloss:2.31200\n",
      "[50]\tvalidation_0-mlogloss:2.30303\n",
      "[51]\tvalidation_0-mlogloss:2.29639\n",
      "[52]\tvalidation_0-mlogloss:2.28903\n",
      "[53]\tvalidation_0-mlogloss:2.28095\n",
      "[54]\tvalidation_0-mlogloss:2.27222\n",
      "[55]\tvalidation_0-mlogloss:2.26602\n",
      "[56]\tvalidation_0-mlogloss:2.25929\n",
      "[57]\tvalidation_0-mlogloss:2.25168\n",
      "[58]\tvalidation_0-mlogloss:2.24660\n",
      "[59]\tvalidation_0-mlogloss:2.24069\n",
      "[60]\tvalidation_0-mlogloss:2.23332\n",
      "[61]\tvalidation_0-mlogloss:2.22709\n",
      "[62]\tvalidation_0-mlogloss:2.22304\n",
      "[63]\tvalidation_0-mlogloss:2.21715\n",
      "[64]\tvalidation_0-mlogloss:2.21126\n",
      "[65]\tvalidation_0-mlogloss:2.20729\n",
      "[66]\tvalidation_0-mlogloss:2.20132\n",
      "[67]\tvalidation_0-mlogloss:2.19556\n",
      "[68]\tvalidation_0-mlogloss:2.18848\n",
      "[69]\tvalidation_0-mlogloss:2.18350\n",
      "[70]\tvalidation_0-mlogloss:2.17732\n",
      "[71]\tvalidation_0-mlogloss:2.17195\n",
      "[72]\tvalidation_0-mlogloss:2.16762\n",
      "[73]\tvalidation_0-mlogloss:2.16390\n",
      "[74]\tvalidation_0-mlogloss:2.15860\n",
      "[75]\tvalidation_0-mlogloss:2.15283\n",
      "[76]\tvalidation_0-mlogloss:2.14616\n",
      "[77]\tvalidation_0-mlogloss:2.14024\n",
      "[78]\tvalidation_0-mlogloss:2.13397\n",
      "[79]\tvalidation_0-mlogloss:2.12903\n",
      "[80]\tvalidation_0-mlogloss:2.12516\n",
      "[81]\tvalidation_0-mlogloss:2.12131\n",
      "[82]\tvalidation_0-mlogloss:2.11634\n",
      "[83]\tvalidation_0-mlogloss:2.11045\n",
      "[84]\tvalidation_0-mlogloss:2.10602\n",
      "[85]\tvalidation_0-mlogloss:2.10146\n",
      "[86]\tvalidation_0-mlogloss:2.09771\n",
      "[87]\tvalidation_0-mlogloss:2.09313\n",
      "[88]\tvalidation_0-mlogloss:2.09016\n",
      "[89]\tvalidation_0-mlogloss:2.08752\n",
      "[90]\tvalidation_0-mlogloss:2.08333\n",
      "[91]\tvalidation_0-mlogloss:2.08005\n",
      "[92]\tvalidation_0-mlogloss:2.07550\n",
      "[93]\tvalidation_0-mlogloss:2.07182\n",
      "[94]\tvalidation_0-mlogloss:2.06845\n",
      "[95]\tvalidation_0-mlogloss:2.06479\n",
      "[96]\tvalidation_0-mlogloss:2.06098\n",
      "[97]\tvalidation_0-mlogloss:2.05970\n",
      "[98]\tvalidation_0-mlogloss:2.05661\n",
      "[99]\tvalidation_0-mlogloss:2.05515\n",
      "[100]\tvalidation_0-mlogloss:2.05109\n",
      "[101]\tvalidation_0-mlogloss:2.04741\n",
      "[102]\tvalidation_0-mlogloss:2.04496\n",
      "[103]\tvalidation_0-mlogloss:2.04265\n",
      "[104]\tvalidation_0-mlogloss:2.03933\n",
      "[105]\tvalidation_0-mlogloss:2.03616\n",
      "[106]\tvalidation_0-mlogloss:2.03301\n",
      "[107]\tvalidation_0-mlogloss:2.03076\n",
      "[108]\tvalidation_0-mlogloss:2.02844\n",
      "[109]\tvalidation_0-mlogloss:2.02570\n",
      "[110]\tvalidation_0-mlogloss:2.02292\n",
      "[111]\tvalidation_0-mlogloss:2.02028\n",
      "[112]\tvalidation_0-mlogloss:2.01733\n",
      "[113]\tvalidation_0-mlogloss:2.01430\n",
      "[114]\tvalidation_0-mlogloss:2.01254\n",
      "[115]\tvalidation_0-mlogloss:2.01101\n",
      "[116]\tvalidation_0-mlogloss:2.00949\n",
      "[117]\tvalidation_0-mlogloss:2.00717\n",
      "[118]\tvalidation_0-mlogloss:2.00377\n",
      "[119]\tvalidation_0-mlogloss:2.00152\n",
      "[120]\tvalidation_0-mlogloss:1.99855\n",
      "[121]\tvalidation_0-mlogloss:1.99579\n",
      "[122]\tvalidation_0-mlogloss:1.99418\n",
      "[123]\tvalidation_0-mlogloss:1.99121\n",
      "[124]\tvalidation_0-mlogloss:1.99000\n",
      "[125]\tvalidation_0-mlogloss:1.98756\n",
      "[126]\tvalidation_0-mlogloss:1.98429\n",
      "[127]\tvalidation_0-mlogloss:1.98187\n",
      "[128]\tvalidation_0-mlogloss:1.97937\n",
      "[129]\tvalidation_0-mlogloss:1.97666\n",
      "[130]\tvalidation_0-mlogloss:1.97528\n",
      "[131]\tvalidation_0-mlogloss:1.97267\n",
      "[132]\tvalidation_0-mlogloss:1.97080\n",
      "[133]\tvalidation_0-mlogloss:1.96863\n",
      "[134]\tvalidation_0-mlogloss:1.96717\n",
      "[135]\tvalidation_0-mlogloss:1.96565\n",
      "[136]\tvalidation_0-mlogloss:1.96340\n",
      "[137]\tvalidation_0-mlogloss:1.96067\n",
      "[138]\tvalidation_0-mlogloss:1.95912\n",
      "[139]\tvalidation_0-mlogloss:1.95647\n",
      "[140]\tvalidation_0-mlogloss:1.95568\n",
      "[141]\tvalidation_0-mlogloss:1.95422\n",
      "[142]\tvalidation_0-mlogloss:1.95243\n",
      "[143]\tvalidation_0-mlogloss:1.94997\n",
      "[144]\tvalidation_0-mlogloss:1.94897\n",
      "[145]\tvalidation_0-mlogloss:1.94670\n",
      "[146]\tvalidation_0-mlogloss:1.94410\n",
      "[147]\tvalidation_0-mlogloss:1.94226\n",
      "[148]\tvalidation_0-mlogloss:1.93905\n",
      "[149]\tvalidation_0-mlogloss:1.93859\n",
      "[150]\tvalidation_0-mlogloss:1.93747\n",
      "[151]\tvalidation_0-mlogloss:1.93661\n",
      "[152]\tvalidation_0-mlogloss:1.93463\n",
      "[153]\tvalidation_0-mlogloss:1.93218\n",
      "[154]\tvalidation_0-mlogloss:1.93021\n",
      "[155]\tvalidation_0-mlogloss:1.92877\n",
      "[156]\tvalidation_0-mlogloss:1.92737\n",
      "[157]\tvalidation_0-mlogloss:1.92546\n",
      "[158]\tvalidation_0-mlogloss:1.92422\n",
      "[159]\tvalidation_0-mlogloss:1.92334\n",
      "[160]\tvalidation_0-mlogloss:1.92234\n",
      "[161]\tvalidation_0-mlogloss:1.92205\n",
      "[162]\tvalidation_0-mlogloss:1.92070\n",
      "[163]\tvalidation_0-mlogloss:1.91950\n",
      "[164]\tvalidation_0-mlogloss:1.91690\n",
      "[165]\tvalidation_0-mlogloss:1.91591\n",
      "[166]\tvalidation_0-mlogloss:1.91456\n",
      "[167]\tvalidation_0-mlogloss:1.91361\n",
      "[168]\tvalidation_0-mlogloss:1.91201\n",
      "[169]\tvalidation_0-mlogloss:1.91014\n",
      "[170]\tvalidation_0-mlogloss:1.90806\n",
      "[171]\tvalidation_0-mlogloss:1.90632\n",
      "[172]\tvalidation_0-mlogloss:1.90462\n",
      "[173]\tvalidation_0-mlogloss:1.90342\n",
      "[174]\tvalidation_0-mlogloss:1.90202\n",
      "[175]\tvalidation_0-mlogloss:1.90042\n",
      "[176]\tvalidation_0-mlogloss:1.89903\n",
      "[177]\tvalidation_0-mlogloss:1.89906\n",
      "[178]\tvalidation_0-mlogloss:1.89761\n",
      "[179]\tvalidation_0-mlogloss:1.89627\n",
      "[180]\tvalidation_0-mlogloss:1.89464\n",
      "[181]\tvalidation_0-mlogloss:1.89321\n",
      "[182]\tvalidation_0-mlogloss:1.89232\n",
      "[183]\tvalidation_0-mlogloss:1.89105\n",
      "[184]\tvalidation_0-mlogloss:1.88994\n",
      "[185]\tvalidation_0-mlogloss:1.89015\n",
      "[186]\tvalidation_0-mlogloss:1.88917\n",
      "[187]\tvalidation_0-mlogloss:1.88891\n",
      "[188]\tvalidation_0-mlogloss:1.88728\n",
      "[189]\tvalidation_0-mlogloss:1.88605\n",
      "[190]\tvalidation_0-mlogloss:1.88529\n",
      "[191]\tvalidation_0-mlogloss:1.88335\n",
      "[192]\tvalidation_0-mlogloss:1.88166\n",
      "[193]\tvalidation_0-mlogloss:1.88035\n",
      "[194]\tvalidation_0-mlogloss:1.87971\n",
      "[195]\tvalidation_0-mlogloss:1.87792\n",
      "[196]\tvalidation_0-mlogloss:1.87623\n",
      "[197]\tvalidation_0-mlogloss:1.87495\n",
      "[198]\tvalidation_0-mlogloss:1.87389\n",
      "[199]\tvalidation_0-mlogloss:1.87293\n",
      "[200]\tvalidation_0-mlogloss:1.87224\n",
      "[201]\tvalidation_0-mlogloss:1.87207\n",
      "[202]\tvalidation_0-mlogloss:1.87168\n",
      "[203]\tvalidation_0-mlogloss:1.87156\n",
      "[204]\tvalidation_0-mlogloss:1.87088\n",
      "[205]\tvalidation_0-mlogloss:1.87039\n",
      "[206]\tvalidation_0-mlogloss:1.87007\n",
      "[207]\tvalidation_0-mlogloss:1.86902\n",
      "[208]\tvalidation_0-mlogloss:1.86751\n",
      "[209]\tvalidation_0-mlogloss:1.86726\n",
      "[210]\tvalidation_0-mlogloss:1.86603\n",
      "[211]\tvalidation_0-mlogloss:1.86538\n",
      "[212]\tvalidation_0-mlogloss:1.86469\n",
      "[213]\tvalidation_0-mlogloss:1.86474\n",
      "[214]\tvalidation_0-mlogloss:1.86477\n",
      "[215]\tvalidation_0-mlogloss:1.86405\n",
      "[216]\tvalidation_0-mlogloss:1.86318\n",
      "[217]\tvalidation_0-mlogloss:1.86256\n",
      "[218]\tvalidation_0-mlogloss:1.86229\n",
      "[219]\tvalidation_0-mlogloss:1.86191\n",
      "[220]\tvalidation_0-mlogloss:1.86079\n",
      "[221]\tvalidation_0-mlogloss:1.86016\n",
      "[222]\tvalidation_0-mlogloss:1.85973\n",
      "[223]\tvalidation_0-mlogloss:1.85871\n",
      "[224]\tvalidation_0-mlogloss:1.85696\n",
      "[225]\tvalidation_0-mlogloss:1.85588\n",
      "[226]\tvalidation_0-mlogloss:1.85446\n",
      "[227]\tvalidation_0-mlogloss:1.85322\n",
      "[228]\tvalidation_0-mlogloss:1.85280\n",
      "[229]\tvalidation_0-mlogloss:1.85185\n",
      "[230]\tvalidation_0-mlogloss:1.85075\n",
      "[231]\tvalidation_0-mlogloss:1.84969\n",
      "[232]\tvalidation_0-mlogloss:1.84914\n",
      "[233]\tvalidation_0-mlogloss:1.84870\n",
      "[234]\tvalidation_0-mlogloss:1.84764\n",
      "[235]\tvalidation_0-mlogloss:1.84735\n",
      "[236]\tvalidation_0-mlogloss:1.84690\n",
      "[237]\tvalidation_0-mlogloss:1.84663\n",
      "[238]\tvalidation_0-mlogloss:1.84709\n",
      "[239]\tvalidation_0-mlogloss:1.84683\n",
      "[240]\tvalidation_0-mlogloss:1.84610\n",
      "[241]\tvalidation_0-mlogloss:1.84473\n",
      "[242]\tvalidation_0-mlogloss:1.84401\n",
      "[243]\tvalidation_0-mlogloss:1.84320\n",
      "[244]\tvalidation_0-mlogloss:1.84301\n",
      "[245]\tvalidation_0-mlogloss:1.84167\n",
      "[246]\tvalidation_0-mlogloss:1.84095\n",
      "[247]\tvalidation_0-mlogloss:1.83977\n",
      "[248]\tvalidation_0-mlogloss:1.83915\n",
      "[249]\tvalidation_0-mlogloss:1.83871\n",
      "[250]\tvalidation_0-mlogloss:1.83844\n",
      "[251]\tvalidation_0-mlogloss:1.83767\n",
      "[252]\tvalidation_0-mlogloss:1.83721\n",
      "[253]\tvalidation_0-mlogloss:1.83800\n",
      "[254]\tvalidation_0-mlogloss:1.83759\n",
      "[255]\tvalidation_0-mlogloss:1.83653\n",
      "[256]\tvalidation_0-mlogloss:1.83705\n",
      "[257]\tvalidation_0-mlogloss:1.83664\n",
      "[258]\tvalidation_0-mlogloss:1.83611\n",
      "[259]\tvalidation_0-mlogloss:1.83554\n",
      "[260]\tvalidation_0-mlogloss:1.83468\n",
      "[261]\tvalidation_0-mlogloss:1.83395\n",
      "[262]\tvalidation_0-mlogloss:1.83410\n",
      "[263]\tvalidation_0-mlogloss:1.83400\n",
      "[264]\tvalidation_0-mlogloss:1.83412\n",
      "[265]\tvalidation_0-mlogloss:1.83331\n",
      "[266]\tvalidation_0-mlogloss:1.83320\n",
      "[267]\tvalidation_0-mlogloss:1.83306\n",
      "[268]\tvalidation_0-mlogloss:1.83251\n",
      "[269]\tvalidation_0-mlogloss:1.83202\n",
      "[270]\tvalidation_0-mlogloss:1.83213\n",
      "[271]\tvalidation_0-mlogloss:1.83173\n",
      "[272]\tvalidation_0-mlogloss:1.83197\n",
      "[273]\tvalidation_0-mlogloss:1.83122\n",
      "[274]\tvalidation_0-mlogloss:1.83088\n",
      "[275]\tvalidation_0-mlogloss:1.83036\n",
      "[276]\tvalidation_0-mlogloss:1.83068\n",
      "[277]\tvalidation_0-mlogloss:1.82985\n",
      "[278]\tvalidation_0-mlogloss:1.82973\n",
      "[279]\tvalidation_0-mlogloss:1.82941\n",
      "[280]\tvalidation_0-mlogloss:1.82849\n",
      "[281]\tvalidation_0-mlogloss:1.82781\n",
      "[282]\tvalidation_0-mlogloss:1.82735\n",
      "[283]\tvalidation_0-mlogloss:1.82694\n",
      "[284]\tvalidation_0-mlogloss:1.82661\n",
      "[285]\tvalidation_0-mlogloss:1.82584\n",
      "[286]\tvalidation_0-mlogloss:1.82599\n",
      "[287]\tvalidation_0-mlogloss:1.82537\n",
      "[288]\tvalidation_0-mlogloss:1.82449\n",
      "[289]\tvalidation_0-mlogloss:1.82434\n",
      "[290]\tvalidation_0-mlogloss:1.82446\n",
      "[291]\tvalidation_0-mlogloss:1.82483\n",
      "[292]\tvalidation_0-mlogloss:1.82450\n",
      "[293]\tvalidation_0-mlogloss:1.82387\n",
      "[294]\tvalidation_0-mlogloss:1.82358\n",
      "[295]\tvalidation_0-mlogloss:1.82286\n",
      "[296]\tvalidation_0-mlogloss:1.82313\n",
      "[297]\tvalidation_0-mlogloss:1.82275\n",
      "[298]\tvalidation_0-mlogloss:1.82267\n",
      "[299]\tvalidation_0-mlogloss:1.82237\n",
      "[300]\tvalidation_0-mlogloss:1.82165\n",
      "[301]\tvalidation_0-mlogloss:1.82156\n",
      "[302]\tvalidation_0-mlogloss:1.82113\n",
      "[303]\tvalidation_0-mlogloss:1.82124\n",
      "[304]\tvalidation_0-mlogloss:1.82152\n",
      "[305]\tvalidation_0-mlogloss:1.82146\n",
      "[306]\tvalidation_0-mlogloss:1.82188\n",
      "[307]\tvalidation_0-mlogloss:1.82163\n",
      "[308]\tvalidation_0-mlogloss:1.82175\n",
      "[309]\tvalidation_0-mlogloss:1.82217\n",
      "[310]\tvalidation_0-mlogloss:1.82213\n",
      "[311]\tvalidation_0-mlogloss:1.82165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.22      0.25      1499\n",
      "           1       0.25      0.23      0.24      1499\n",
      "           2       0.40      0.39      0.39      1499\n",
      "           3       0.49      0.25      0.33      1499\n",
      "           4       0.87      0.88      0.87      1499\n",
      "           5       0.51      0.55      0.53      1499\n",
      "           6       0.55      0.78      0.65      1499\n",
      "           7       0.84      0.64      0.73      1499\n",
      "           8       0.65      0.75      0.70      1499\n",
      "           9       0.75      0.62      0.68      1499\n",
      "          10       0.65      0.47      0.55      1499\n",
      "          11       0.62      0.62      0.62      1499\n",
      "          12       0.55      0.70      0.62      1499\n",
      "          13       0.35      0.46      0.39      1499\n",
      "          14       0.28      0.31      0.30      1499\n",
      "          15       0.82      0.74      0.78      1499\n",
      "          16       0.68      0.51      0.58      1499\n",
      "          17       0.61      0.53      0.56      1499\n",
      "          18       0.38      0.54      0.44      1499\n",
      "          19       0.43      0.35      0.39      1499\n",
      "          20       0.72      0.82      0.77      1499\n",
      "          21       0.57      0.64      0.60      1499\n",
      "          22       0.39      0.32      0.35      1499\n",
      "          23       0.82      0.83      0.82      1499\n",
      "          24       0.86      0.76      0.81      1499\n",
      "          25       0.78      0.48      0.59      1499\n",
      "          26       0.41      0.34      0.37      1499\n",
      "          27       0.38      0.34      0.36      1499\n",
      "          28       0.50      0.52      0.51      1499\n",
      "          29       0.55      0.74      0.63      1499\n",
      "          30       0.33      0.39      0.36      1499\n",
      "          31       0.48      0.57      0.52      1499\n",
      "          32       0.33      0.61      0.42      1499\n",
      "          33       0.56      0.42      0.48      1499\n",
      "          34       0.75      0.87      0.81      1499\n",
      "          35       0.65      0.68      0.67      1499\n",
      "          36       0.67      0.60      0.63      1499\n",
      "          37       0.31      0.38      0.34      1499\n",
      "          38       0.76      0.61      0.68      1499\n",
      "          39       0.65      0.54      0.59      1499\n",
      "          40       0.78      0.69      0.73      1499\n",
      "          41       0.76      0.68      0.72      1499\n",
      "          42       0.48      0.44      0.46      1499\n",
      "          43       0.43      0.37      0.40      1499\n",
      "          44       0.48      0.62      0.55      1499\n",
      "          45       0.37      0.34      0.35      1499\n",
      "          46       0.47      0.47      0.47      1499\n",
      "          47       0.78      0.69      0.73      1499\n",
      "          48       0.73      0.78      0.75      1499\n",
      "          49       0.49      0.48      0.49      1499\n",
      "\n",
      "    accuracy                           0.55     74950\n",
      "   macro avg       0.56      0.55      0.55     74950\n",
      "weighted avg       0.56      0.55      0.55     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7844baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7980b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 534s 28ms/step - loss: 1.8313 - val_loss: 2.0093\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 20s 1ms/step - loss: 1.0621 - val_loss: 2.1111\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 18s 943us/step - loss: 0.8859 - val_loss: 2.0982\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 15s 820us/step - loss: 0.8101 - val_loss: 2.1486\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 15s 808us/step - loss: 0.7654 - val_loss: 2.1447\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 15s 795us/step - loss: 0.7342 - val_loss: 2.3253\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 15s 777us/step - loss: 0.7100 - val_loss: 2.2375\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 15s 795us/step - loss: 0.6922 - val_loss: 2.2986\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 15s 809us/step - loss: 0.6766 - val_loss: 2.3375\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 15s 803us/step - loss: 0.6638 - val_loss: 2.3145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2f385d8a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 1s 333us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.30      0.28      1499\n",
      "           1       0.30      0.17      0.22      1499\n",
      "           2       0.33      0.38      0.35      1499\n",
      "           3       0.52      0.28      0.37      1499\n",
      "           4       0.79      0.85      0.82      1499\n",
      "           5       0.61      0.43      0.50      1499\n",
      "           6       0.59      0.69      0.64      1499\n",
      "           7       0.90      0.53      0.67      1499\n",
      "           8       0.44      0.58      0.50      1499\n",
      "           9       0.64      0.62      0.63      1499\n",
      "          10       0.55      0.37      0.44      1499\n",
      "          11       0.66      0.68      0.67      1499\n",
      "          12       0.47      0.62      0.54      1499\n",
      "          13       0.33      0.41      0.36      1499\n",
      "          14       0.26      0.27      0.27      1499\n",
      "          15       0.87      0.62      0.72      1499\n",
      "          16       0.68      0.50      0.57      1499\n",
      "          17       0.71      0.30      0.42      1499\n",
      "          18       0.31      0.58      0.41      1499\n",
      "          19       0.26      0.13      0.17      1499\n",
      "          20       0.92      0.71      0.80      1499\n",
      "          21       0.46      0.79      0.58      1499\n",
      "          22       0.38      0.31      0.34      1499\n",
      "          23       0.89      0.80      0.84      1499\n",
      "          24       0.89      0.43      0.58      1499\n",
      "          25       0.82      0.61      0.70      1499\n",
      "          26       0.35      0.38      0.36      1499\n",
      "          27       0.29      0.29      0.29      1499\n",
      "          28       0.78      0.33      0.46      1499\n",
      "          29       0.74      0.50      0.60      1499\n",
      "          30       0.32      0.56      0.41      1499\n",
      "          31       0.54      0.49      0.51      1499\n",
      "          32       0.28      0.67      0.40      1499\n",
      "          33       0.65      0.31      0.42      1499\n",
      "          34       0.93      0.77      0.84      1499\n",
      "          35       0.60      0.52      0.56      1499\n",
      "          36       0.69      0.41      0.51      1499\n",
      "          37       0.30      0.38      0.34      1499\n",
      "          38       0.69      0.45      0.55      1499\n",
      "          39       0.50      0.43      0.46      1499\n",
      "          40       0.71      0.46      0.56      1499\n",
      "          41       0.80      0.65      0.72      1499\n",
      "          42       0.37      0.47      0.41      1499\n",
      "          43       0.34      0.47      0.39      1499\n",
      "          44       0.40      0.59      0.48      1499\n",
      "          45       0.29      0.54      0.38      1499\n",
      "          46       0.36      0.53      0.43      1499\n",
      "          47       0.76      0.47      0.58      1499\n",
      "          48       0.77      0.77      0.77      1499\n",
      "          49       0.44      0.66      0.53      1499\n",
      "\n",
      "    accuracy                           0.50     74950\n",
      "   macro avg       0.55      0.50      0.51     74950\n",
      "weighted avg       0.55      0.50      0.51     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20950/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_20950/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_20950/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.34618\n",
      "[1]\tvalidation_0-mlogloss:3.11471\n",
      "[2]\tvalidation_0-mlogloss:2.95737\n",
      "[3]\tvalidation_0-mlogloss:2.83498\n",
      "[4]\tvalidation_0-mlogloss:2.73736\n",
      "[5]\tvalidation_0-mlogloss:2.65904\n",
      "[6]\tvalidation_0-mlogloss:2.58793\n",
      "[7]\tvalidation_0-mlogloss:2.52424\n",
      "[8]\tvalidation_0-mlogloss:2.46481\n",
      "[9]\tvalidation_0-mlogloss:2.41477\n",
      "[10]\tvalidation_0-mlogloss:2.37046\n",
      "[11]\tvalidation_0-mlogloss:2.33119\n",
      "[12]\tvalidation_0-mlogloss:2.29122\n",
      "[13]\tvalidation_0-mlogloss:2.25572\n",
      "[14]\tvalidation_0-mlogloss:2.22221\n",
      "[15]\tvalidation_0-mlogloss:2.19537\n",
      "[16]\tvalidation_0-mlogloss:2.17056\n",
      "[17]\tvalidation_0-mlogloss:2.14280\n",
      "[18]\tvalidation_0-mlogloss:2.11735\n",
      "[19]\tvalidation_0-mlogloss:2.09372\n",
      "[20]\tvalidation_0-mlogloss:2.06853\n",
      "[21]\tvalidation_0-mlogloss:2.04530\n",
      "[22]\tvalidation_0-mlogloss:2.02768\n",
      "[23]\tvalidation_0-mlogloss:2.00976\n",
      "[24]\tvalidation_0-mlogloss:1.98859\n",
      "[25]\tvalidation_0-mlogloss:1.97224\n",
      "[26]\tvalidation_0-mlogloss:1.95498\n",
      "[27]\tvalidation_0-mlogloss:1.94095\n",
      "[28]\tvalidation_0-mlogloss:1.92631\n",
      "[29]\tvalidation_0-mlogloss:1.90942\n",
      "[30]\tvalidation_0-mlogloss:1.89252\n",
      "[31]\tvalidation_0-mlogloss:1.87805\n",
      "[32]\tvalidation_0-mlogloss:1.86391\n",
      "[33]\tvalidation_0-mlogloss:1.84976\n",
      "[34]\tvalidation_0-mlogloss:1.83653\n",
      "[35]\tvalidation_0-mlogloss:1.82197\n",
      "[36]\tvalidation_0-mlogloss:1.80843\n",
      "[37]\tvalidation_0-mlogloss:1.79303\n",
      "[38]\tvalidation_0-mlogloss:1.78166\n",
      "[39]\tvalidation_0-mlogloss:1.77016\n",
      "[40]\tvalidation_0-mlogloss:1.75603\n",
      "[41]\tvalidation_0-mlogloss:1.74209\n",
      "[42]\tvalidation_0-mlogloss:1.73243\n",
      "[43]\tvalidation_0-mlogloss:1.71925\n",
      "[44]\tvalidation_0-mlogloss:1.70949\n",
      "[45]\tvalidation_0-mlogloss:1.69880\n",
      "[46]\tvalidation_0-mlogloss:1.68808\n",
      "[47]\tvalidation_0-mlogloss:1.67686\n",
      "[48]\tvalidation_0-mlogloss:1.66509\n",
      "[49]\tvalidation_0-mlogloss:1.65364\n",
      "[50]\tvalidation_0-mlogloss:1.64433\n",
      "[51]\tvalidation_0-mlogloss:1.63503\n",
      "[52]\tvalidation_0-mlogloss:1.62555\n",
      "[53]\tvalidation_0-mlogloss:1.61520\n",
      "[54]\tvalidation_0-mlogloss:1.60598\n",
      "[55]\tvalidation_0-mlogloss:1.59561\n",
      "[56]\tvalidation_0-mlogloss:1.58765\n",
      "[57]\tvalidation_0-mlogloss:1.57912\n",
      "[58]\tvalidation_0-mlogloss:1.57011\n",
      "[59]\tvalidation_0-mlogloss:1.56170\n",
      "[60]\tvalidation_0-mlogloss:1.55441\n",
      "[61]\tvalidation_0-mlogloss:1.54790\n",
      "[62]\tvalidation_0-mlogloss:1.53955\n",
      "[63]\tvalidation_0-mlogloss:1.53480\n",
      "[64]\tvalidation_0-mlogloss:1.52614\n",
      "[65]\tvalidation_0-mlogloss:1.51983\n",
      "[66]\tvalidation_0-mlogloss:1.51182\n",
      "[67]\tvalidation_0-mlogloss:1.50519\n",
      "[68]\tvalidation_0-mlogloss:1.49887\n",
      "[69]\tvalidation_0-mlogloss:1.49084\n",
      "[70]\tvalidation_0-mlogloss:1.48385\n",
      "[71]\tvalidation_0-mlogloss:1.47789\n",
      "[72]\tvalidation_0-mlogloss:1.47231\n",
      "[73]\tvalidation_0-mlogloss:1.46655\n",
      "[74]\tvalidation_0-mlogloss:1.46100\n",
      "[75]\tvalidation_0-mlogloss:1.45592\n",
      "[76]\tvalidation_0-mlogloss:1.44848\n",
      "[77]\tvalidation_0-mlogloss:1.44181\n",
      "[78]\tvalidation_0-mlogloss:1.43520\n",
      "[79]\tvalidation_0-mlogloss:1.42788\n",
      "[80]\tvalidation_0-mlogloss:1.42207\n",
      "[81]\tvalidation_0-mlogloss:1.41615\n",
      "[82]\tvalidation_0-mlogloss:1.40741\n",
      "[83]\tvalidation_0-mlogloss:1.40221\n",
      "[84]\tvalidation_0-mlogloss:1.39835\n",
      "[85]\tvalidation_0-mlogloss:1.39310\n",
      "[86]\tvalidation_0-mlogloss:1.38939\n",
      "[87]\tvalidation_0-mlogloss:1.38409\n",
      "[88]\tvalidation_0-mlogloss:1.37862\n",
      "[89]\tvalidation_0-mlogloss:1.37207\n",
      "[90]\tvalidation_0-mlogloss:1.36696\n",
      "[91]\tvalidation_0-mlogloss:1.36082\n",
      "[92]\tvalidation_0-mlogloss:1.35511\n",
      "[93]\tvalidation_0-mlogloss:1.34926\n",
      "[94]\tvalidation_0-mlogloss:1.34462\n",
      "[95]\tvalidation_0-mlogloss:1.34062\n",
      "[96]\tvalidation_0-mlogloss:1.33572\n",
      "[97]\tvalidation_0-mlogloss:1.33184\n",
      "[98]\tvalidation_0-mlogloss:1.32717\n",
      "[99]\tvalidation_0-mlogloss:1.32269\n",
      "[100]\tvalidation_0-mlogloss:1.31880\n",
      "[101]\tvalidation_0-mlogloss:1.31564\n",
      "[102]\tvalidation_0-mlogloss:1.31173\n",
      "[103]\tvalidation_0-mlogloss:1.30655\n",
      "[104]\tvalidation_0-mlogloss:1.30191\n",
      "[105]\tvalidation_0-mlogloss:1.29780\n",
      "[106]\tvalidation_0-mlogloss:1.29398\n",
      "[107]\tvalidation_0-mlogloss:1.28919\n",
      "[108]\tvalidation_0-mlogloss:1.28475\n",
      "[109]\tvalidation_0-mlogloss:1.28055\n",
      "[110]\tvalidation_0-mlogloss:1.27730\n",
      "[111]\tvalidation_0-mlogloss:1.27318\n",
      "[112]\tvalidation_0-mlogloss:1.26857\n",
      "[113]\tvalidation_0-mlogloss:1.26502\n",
      "[114]\tvalidation_0-mlogloss:1.26136\n",
      "[115]\tvalidation_0-mlogloss:1.25811\n",
      "[116]\tvalidation_0-mlogloss:1.25573\n",
      "[117]\tvalidation_0-mlogloss:1.25296\n",
      "[118]\tvalidation_0-mlogloss:1.25010\n",
      "[119]\tvalidation_0-mlogloss:1.24726\n",
      "[120]\tvalidation_0-mlogloss:1.24437\n",
      "[121]\tvalidation_0-mlogloss:1.24044\n",
      "[122]\tvalidation_0-mlogloss:1.23675\n",
      "[123]\tvalidation_0-mlogloss:1.23498\n",
      "[124]\tvalidation_0-mlogloss:1.23030\n",
      "[125]\tvalidation_0-mlogloss:1.22691\n",
      "[126]\tvalidation_0-mlogloss:1.22366\n",
      "[127]\tvalidation_0-mlogloss:1.21950\n",
      "[128]\tvalidation_0-mlogloss:1.21680\n",
      "[129]\tvalidation_0-mlogloss:1.21451\n",
      "[130]\tvalidation_0-mlogloss:1.21178\n",
      "[131]\tvalidation_0-mlogloss:1.20876\n",
      "[132]\tvalidation_0-mlogloss:1.20483\n",
      "[133]\tvalidation_0-mlogloss:1.20266\n",
      "[134]\tvalidation_0-mlogloss:1.19949\n",
      "[135]\tvalidation_0-mlogloss:1.19537\n",
      "[136]\tvalidation_0-mlogloss:1.19304\n",
      "[137]\tvalidation_0-mlogloss:1.19056\n",
      "[138]\tvalidation_0-mlogloss:1.18732\n",
      "[139]\tvalidation_0-mlogloss:1.18505\n",
      "[140]\tvalidation_0-mlogloss:1.18288\n",
      "[141]\tvalidation_0-mlogloss:1.17989\n",
      "[142]\tvalidation_0-mlogloss:1.17837\n",
      "[143]\tvalidation_0-mlogloss:1.17649\n",
      "[144]\tvalidation_0-mlogloss:1.17486\n",
      "[145]\tvalidation_0-mlogloss:1.17270\n",
      "[146]\tvalidation_0-mlogloss:1.17085\n",
      "[147]\tvalidation_0-mlogloss:1.16816\n",
      "[148]\tvalidation_0-mlogloss:1.16581\n",
      "[149]\tvalidation_0-mlogloss:1.16279\n",
      "[150]\tvalidation_0-mlogloss:1.16011\n",
      "[151]\tvalidation_0-mlogloss:1.15917\n",
      "[152]\tvalidation_0-mlogloss:1.15717\n",
      "[153]\tvalidation_0-mlogloss:1.15457\n",
      "[154]\tvalidation_0-mlogloss:1.15125\n",
      "[155]\tvalidation_0-mlogloss:1.14999\n",
      "[156]\tvalidation_0-mlogloss:1.14848\n",
      "[157]\tvalidation_0-mlogloss:1.14644\n",
      "[158]\tvalidation_0-mlogloss:1.14491\n",
      "[159]\tvalidation_0-mlogloss:1.14288\n",
      "[160]\tvalidation_0-mlogloss:1.14055\n",
      "[161]\tvalidation_0-mlogloss:1.13870\n",
      "[162]\tvalidation_0-mlogloss:1.13689\n",
      "[163]\tvalidation_0-mlogloss:1.13459\n",
      "[164]\tvalidation_0-mlogloss:1.13406\n",
      "[165]\tvalidation_0-mlogloss:1.13208\n",
      "[166]\tvalidation_0-mlogloss:1.13005\n",
      "[167]\tvalidation_0-mlogloss:1.12818\n",
      "[168]\tvalidation_0-mlogloss:1.12551\n",
      "[169]\tvalidation_0-mlogloss:1.12439\n",
      "[170]\tvalidation_0-mlogloss:1.12237\n",
      "[171]\tvalidation_0-mlogloss:1.12058\n",
      "[172]\tvalidation_0-mlogloss:1.11906\n",
      "[173]\tvalidation_0-mlogloss:1.11704\n",
      "[174]\tvalidation_0-mlogloss:1.11532\n",
      "[175]\tvalidation_0-mlogloss:1.11384\n",
      "[176]\tvalidation_0-mlogloss:1.11315\n",
      "[177]\tvalidation_0-mlogloss:1.11139\n",
      "[178]\tvalidation_0-mlogloss:1.10953\n",
      "[179]\tvalidation_0-mlogloss:1.10736\n",
      "[180]\tvalidation_0-mlogloss:1.10538\n",
      "[181]\tvalidation_0-mlogloss:1.10373\n",
      "[182]\tvalidation_0-mlogloss:1.10229\n",
      "[183]\tvalidation_0-mlogloss:1.10052\n",
      "[184]\tvalidation_0-mlogloss:1.09877\n",
      "[185]\tvalidation_0-mlogloss:1.09598\n",
      "[186]\tvalidation_0-mlogloss:1.09451\n",
      "[187]\tvalidation_0-mlogloss:1.09328\n",
      "[188]\tvalidation_0-mlogloss:1.09189\n",
      "[189]\tvalidation_0-mlogloss:1.09046\n",
      "[190]\tvalidation_0-mlogloss:1.08832\n",
      "[191]\tvalidation_0-mlogloss:1.08715\n",
      "[192]\tvalidation_0-mlogloss:1.08571\n",
      "[193]\tvalidation_0-mlogloss:1.08467\n",
      "[194]\tvalidation_0-mlogloss:1.08287\n",
      "[195]\tvalidation_0-mlogloss:1.08229\n",
      "[196]\tvalidation_0-mlogloss:1.08051\n",
      "[197]\tvalidation_0-mlogloss:1.08007\n",
      "[198]\tvalidation_0-mlogloss:1.07930\n",
      "[199]\tvalidation_0-mlogloss:1.07892\n",
      "[200]\tvalidation_0-mlogloss:1.07719\n",
      "[201]\tvalidation_0-mlogloss:1.07606\n",
      "[202]\tvalidation_0-mlogloss:1.07475\n",
      "[203]\tvalidation_0-mlogloss:1.07302\n",
      "[204]\tvalidation_0-mlogloss:1.07237\n",
      "[205]\tvalidation_0-mlogloss:1.07046\n",
      "[206]\tvalidation_0-mlogloss:1.06932\n",
      "[207]\tvalidation_0-mlogloss:1.06858\n",
      "[208]\tvalidation_0-mlogloss:1.06744\n",
      "[209]\tvalidation_0-mlogloss:1.06566\n",
      "[210]\tvalidation_0-mlogloss:1.06464\n",
      "[211]\tvalidation_0-mlogloss:1.06357\n",
      "[212]\tvalidation_0-mlogloss:1.06202\n",
      "[213]\tvalidation_0-mlogloss:1.06073\n",
      "[214]\tvalidation_0-mlogloss:1.05981\n",
      "[215]\tvalidation_0-mlogloss:1.05856\n",
      "[216]\tvalidation_0-mlogloss:1.05666\n",
      "[217]\tvalidation_0-mlogloss:1.05642\n",
      "[218]\tvalidation_0-mlogloss:1.05560\n",
      "[219]\tvalidation_0-mlogloss:1.05438\n",
      "[220]\tvalidation_0-mlogloss:1.05227\n",
      "[221]\tvalidation_0-mlogloss:1.05094\n",
      "[222]\tvalidation_0-mlogloss:1.04939\n",
      "[223]\tvalidation_0-mlogloss:1.04832\n",
      "[224]\tvalidation_0-mlogloss:1.04763\n",
      "[225]\tvalidation_0-mlogloss:1.04682\n",
      "[226]\tvalidation_0-mlogloss:1.04608\n",
      "[227]\tvalidation_0-mlogloss:1.04631\n",
      "[228]\tvalidation_0-mlogloss:1.04533\n",
      "[229]\tvalidation_0-mlogloss:1.04412\n",
      "[230]\tvalidation_0-mlogloss:1.04232\n",
      "[231]\tvalidation_0-mlogloss:1.04146\n",
      "[232]\tvalidation_0-mlogloss:1.04003\n",
      "[233]\tvalidation_0-mlogloss:1.03938\n",
      "[234]\tvalidation_0-mlogloss:1.03805\n",
      "[235]\tvalidation_0-mlogloss:1.03723\n",
      "[236]\tvalidation_0-mlogloss:1.03618\n",
      "[237]\tvalidation_0-mlogloss:1.03606\n",
      "[238]\tvalidation_0-mlogloss:1.03485\n",
      "[239]\tvalidation_0-mlogloss:1.03398\n",
      "[240]\tvalidation_0-mlogloss:1.03291\n",
      "[241]\tvalidation_0-mlogloss:1.03232\n",
      "[242]\tvalidation_0-mlogloss:1.03101\n",
      "[243]\tvalidation_0-mlogloss:1.02948\n",
      "[244]\tvalidation_0-mlogloss:1.02841\n",
      "[245]\tvalidation_0-mlogloss:1.02713\n",
      "[246]\tvalidation_0-mlogloss:1.02588\n",
      "[247]\tvalidation_0-mlogloss:1.02434\n",
      "[248]\tvalidation_0-mlogloss:1.02368\n",
      "[249]\tvalidation_0-mlogloss:1.02307\n",
      "[250]\tvalidation_0-mlogloss:1.02249\n",
      "[251]\tvalidation_0-mlogloss:1.02245\n",
      "[252]\tvalidation_0-mlogloss:1.02235\n",
      "[253]\tvalidation_0-mlogloss:1.02180\n",
      "[254]\tvalidation_0-mlogloss:1.02063\n",
      "[255]\tvalidation_0-mlogloss:1.02006\n",
      "[256]\tvalidation_0-mlogloss:1.01955\n",
      "[257]\tvalidation_0-mlogloss:1.01864\n",
      "[258]\tvalidation_0-mlogloss:1.01864\n",
      "[259]\tvalidation_0-mlogloss:1.01773\n",
      "[260]\tvalidation_0-mlogloss:1.01704\n",
      "[261]\tvalidation_0-mlogloss:1.01678\n",
      "[262]\tvalidation_0-mlogloss:1.01754\n",
      "[263]\tvalidation_0-mlogloss:1.01609\n",
      "[264]\tvalidation_0-mlogloss:1.01527\n",
      "[265]\tvalidation_0-mlogloss:1.01371\n",
      "[266]\tvalidation_0-mlogloss:1.01312\n",
      "[267]\tvalidation_0-mlogloss:1.01223\n",
      "[268]\tvalidation_0-mlogloss:1.01195\n",
      "[269]\tvalidation_0-mlogloss:1.01158\n",
      "[270]\tvalidation_0-mlogloss:1.01083\n",
      "[271]\tvalidation_0-mlogloss:1.01009\n",
      "[272]\tvalidation_0-mlogloss:1.00919\n",
      "[273]\tvalidation_0-mlogloss:1.00861\n",
      "[274]\tvalidation_0-mlogloss:1.00786\n",
      "[275]\tvalidation_0-mlogloss:1.00734\n",
      "[276]\tvalidation_0-mlogloss:1.00626\n",
      "[277]\tvalidation_0-mlogloss:1.00631\n",
      "[278]\tvalidation_0-mlogloss:1.00580\n",
      "[279]\tvalidation_0-mlogloss:1.00468\n",
      "[280]\tvalidation_0-mlogloss:1.00434\n",
      "[281]\tvalidation_0-mlogloss:1.00360\n",
      "[282]\tvalidation_0-mlogloss:1.00277\n",
      "[283]\tvalidation_0-mlogloss:1.00215\n",
      "[284]\tvalidation_0-mlogloss:1.00126\n",
      "[285]\tvalidation_0-mlogloss:1.00072\n",
      "[286]\tvalidation_0-mlogloss:1.00048\n",
      "[287]\tvalidation_0-mlogloss:0.99933\n",
      "[288]\tvalidation_0-mlogloss:0.99893\n",
      "[289]\tvalidation_0-mlogloss:0.99798\n",
      "[290]\tvalidation_0-mlogloss:0.99729\n",
      "[291]\tvalidation_0-mlogloss:0.99719\n",
      "[292]\tvalidation_0-mlogloss:0.99651\n",
      "[293]\tvalidation_0-mlogloss:0.99601\n",
      "[294]\tvalidation_0-mlogloss:0.99575\n",
      "[295]\tvalidation_0-mlogloss:0.99505\n",
      "[296]\tvalidation_0-mlogloss:0.99432\n",
      "[297]\tvalidation_0-mlogloss:0.99370\n",
      "[298]\tvalidation_0-mlogloss:0.99311\n",
      "[299]\tvalidation_0-mlogloss:0.99261\n",
      "[300]\tvalidation_0-mlogloss:0.99208\n",
      "[301]\tvalidation_0-mlogloss:0.99100\n",
      "[302]\tvalidation_0-mlogloss:0.99075\n",
      "[303]\tvalidation_0-mlogloss:0.99001\n",
      "[304]\tvalidation_0-mlogloss:0.98992\n",
      "[305]\tvalidation_0-mlogloss:0.98921\n",
      "[306]\tvalidation_0-mlogloss:0.98817\n",
      "[307]\tvalidation_0-mlogloss:0.98737\n",
      "[308]\tvalidation_0-mlogloss:0.98704\n",
      "[309]\tvalidation_0-mlogloss:0.98647\n",
      "[310]\tvalidation_0-mlogloss:0.98601\n",
      "[311]\tvalidation_0-mlogloss:0.98573\n",
      "[312]\tvalidation_0-mlogloss:0.98505\n",
      "[313]\tvalidation_0-mlogloss:0.98477\n",
      "[314]\tvalidation_0-mlogloss:0.98485\n",
      "[315]\tvalidation_0-mlogloss:0.98474\n",
      "[316]\tvalidation_0-mlogloss:0.98480\n",
      "[317]\tvalidation_0-mlogloss:0.98428\n",
      "[318]\tvalidation_0-mlogloss:0.98410\n",
      "[319]\tvalidation_0-mlogloss:0.98296\n",
      "[320]\tvalidation_0-mlogloss:0.98215\n",
      "[321]\tvalidation_0-mlogloss:0.98234\n",
      "[322]\tvalidation_0-mlogloss:0.98222\n",
      "[323]\tvalidation_0-mlogloss:0.98157\n",
      "[324]\tvalidation_0-mlogloss:0.98131\n",
      "[325]\tvalidation_0-mlogloss:0.98071\n",
      "[326]\tvalidation_0-mlogloss:0.98032\n",
      "[327]\tvalidation_0-mlogloss:0.98022\n",
      "[328]\tvalidation_0-mlogloss:0.97986\n",
      "[329]\tvalidation_0-mlogloss:0.97989\n",
      "[330]\tvalidation_0-mlogloss:0.97955\n",
      "[331]\tvalidation_0-mlogloss:0.97924\n",
      "[332]\tvalidation_0-mlogloss:0.97901\n",
      "[333]\tvalidation_0-mlogloss:0.97879\n",
      "[334]\tvalidation_0-mlogloss:0.97846\n",
      "[335]\tvalidation_0-mlogloss:0.97843\n",
      "[336]\tvalidation_0-mlogloss:0.97776\n",
      "[337]\tvalidation_0-mlogloss:0.97721\n",
      "[338]\tvalidation_0-mlogloss:0.97661\n",
      "[339]\tvalidation_0-mlogloss:0.97673\n",
      "[340]\tvalidation_0-mlogloss:0.97632\n",
      "[341]\tvalidation_0-mlogloss:0.97598\n",
      "[342]\tvalidation_0-mlogloss:0.97553\n",
      "[343]\tvalidation_0-mlogloss:0.97492\n",
      "[344]\tvalidation_0-mlogloss:0.97466\n",
      "[345]\tvalidation_0-mlogloss:0.97438\n",
      "[346]\tvalidation_0-mlogloss:0.97357\n",
      "[347]\tvalidation_0-mlogloss:0.97341\n",
      "[348]\tvalidation_0-mlogloss:0.97313\n",
      "[349]\tvalidation_0-mlogloss:0.97267\n",
      "[350]\tvalidation_0-mlogloss:0.97232\n",
      "[351]\tvalidation_0-mlogloss:0.97197\n",
      "[352]\tvalidation_0-mlogloss:0.97149\n",
      "[353]\tvalidation_0-mlogloss:0.97094\n",
      "[354]\tvalidation_0-mlogloss:0.97086\n",
      "[355]\tvalidation_0-mlogloss:0.97113\n",
      "[356]\tvalidation_0-mlogloss:0.97096\n",
      "[357]\tvalidation_0-mlogloss:0.97077\n",
      "[358]\tvalidation_0-mlogloss:0.97049\n",
      "[359]\tvalidation_0-mlogloss:0.97041\n",
      "[360]\tvalidation_0-mlogloss:0.97081\n",
      "[361]\tvalidation_0-mlogloss:0.97103\n",
      "[362]\tvalidation_0-mlogloss:0.97135\n",
      "[363]\tvalidation_0-mlogloss:0.97067\n",
      "[364]\tvalidation_0-mlogloss:0.97084\n",
      "[365]\tvalidation_0-mlogloss:0.97082\n",
      "[366]\tvalidation_0-mlogloss:0.96985\n",
      "[367]\tvalidation_0-mlogloss:0.96946\n",
      "[368]\tvalidation_0-mlogloss:0.96935\n",
      "[369]\tvalidation_0-mlogloss:0.96922\n",
      "[370]\tvalidation_0-mlogloss:0.96858\n",
      "[371]\tvalidation_0-mlogloss:0.96873\n",
      "[372]\tvalidation_0-mlogloss:0.96851\n",
      "[373]\tvalidation_0-mlogloss:0.96796\n",
      "[374]\tvalidation_0-mlogloss:0.96729\n",
      "[375]\tvalidation_0-mlogloss:0.96690\n",
      "[376]\tvalidation_0-mlogloss:0.96636\n",
      "[377]\tvalidation_0-mlogloss:0.96557\n",
      "[378]\tvalidation_0-mlogloss:0.96513\n",
      "[379]\tvalidation_0-mlogloss:0.96508\n",
      "[380]\tvalidation_0-mlogloss:0.96497\n",
      "[381]\tvalidation_0-mlogloss:0.96441\n",
      "[382]\tvalidation_0-mlogloss:0.96404\n",
      "[383]\tvalidation_0-mlogloss:0.96351\n",
      "[384]\tvalidation_0-mlogloss:0.96317\n",
      "[385]\tvalidation_0-mlogloss:0.96294\n",
      "[386]\tvalidation_0-mlogloss:0.96293\n",
      "[387]\tvalidation_0-mlogloss:0.96236\n",
      "[388]\tvalidation_0-mlogloss:0.96253\n",
      "[389]\tvalidation_0-mlogloss:0.96219\n",
      "[390]\tvalidation_0-mlogloss:0.96210\n",
      "[391]\tvalidation_0-mlogloss:0.96179\n",
      "[392]\tvalidation_0-mlogloss:0.96111\n",
      "[393]\tvalidation_0-mlogloss:0.96080\n",
      "[394]\tvalidation_0-mlogloss:0.96077\n",
      "[395]\tvalidation_0-mlogloss:0.96041\n",
      "[396]\tvalidation_0-mlogloss:0.96008\n",
      "[397]\tvalidation_0-mlogloss:0.96018\n",
      "[398]\tvalidation_0-mlogloss:0.96022\n",
      "[399]\tvalidation_0-mlogloss:0.96009\n",
      "[400]\tvalidation_0-mlogloss:0.95999\n",
      "[401]\tvalidation_0-mlogloss:0.95964\n",
      "[402]\tvalidation_0-mlogloss:0.95957\n",
      "[403]\tvalidation_0-mlogloss:0.96003\n",
      "[404]\tvalidation_0-mlogloss:0.95984\n",
      "[405]\tvalidation_0-mlogloss:0.95970\n",
      "[406]\tvalidation_0-mlogloss:0.95966\n",
      "[407]\tvalidation_0-mlogloss:0.95979\n",
      "[408]\tvalidation_0-mlogloss:0.95930\n",
      "[409]\tvalidation_0-mlogloss:0.95941\n",
      "[410]\tvalidation_0-mlogloss:0.95912\n",
      "[411]\tvalidation_0-mlogloss:0.95925\n",
      "[412]\tvalidation_0-mlogloss:0.95913\n",
      "[413]\tvalidation_0-mlogloss:0.95861\n",
      "[414]\tvalidation_0-mlogloss:0.95869\n",
      "[415]\tvalidation_0-mlogloss:0.95848\n",
      "[416]\tvalidation_0-mlogloss:0.95840\n",
      "[417]\tvalidation_0-mlogloss:0.95826\n",
      "[418]\tvalidation_0-mlogloss:0.95796\n",
      "[419]\tvalidation_0-mlogloss:0.95861\n",
      "[420]\tvalidation_0-mlogloss:0.95862\n",
      "[421]\tvalidation_0-mlogloss:0.95919\n",
      "[422]\tvalidation_0-mlogloss:0.95916\n",
      "[423]\tvalidation_0-mlogloss:0.95871\n",
      "[424]\tvalidation_0-mlogloss:0.95807\n",
      "[425]\tvalidation_0-mlogloss:0.95794\n",
      "[426]\tvalidation_0-mlogloss:0.95774\n",
      "[427]\tvalidation_0-mlogloss:0.95755\n",
      "[428]\tvalidation_0-mlogloss:0.95760\n",
      "[429]\tvalidation_0-mlogloss:0.95707\n",
      "[430]\tvalidation_0-mlogloss:0.95680\n",
      "[431]\tvalidation_0-mlogloss:0.95704\n",
      "[432]\tvalidation_0-mlogloss:0.95731\n",
      "[433]\tvalidation_0-mlogloss:0.95748\n",
      "[434]\tvalidation_0-mlogloss:0.95694\n",
      "[435]\tvalidation_0-mlogloss:0.95685\n",
      "[436]\tvalidation_0-mlogloss:0.95646\n",
      "[437]\tvalidation_0-mlogloss:0.95618\n",
      "[438]\tvalidation_0-mlogloss:0.95609\n",
      "[439]\tvalidation_0-mlogloss:0.95603\n",
      "[440]\tvalidation_0-mlogloss:0.95578\n",
      "[441]\tvalidation_0-mlogloss:0.95527\n",
      "[442]\tvalidation_0-mlogloss:0.95469\n",
      "[443]\tvalidation_0-mlogloss:0.95418\n",
      "[444]\tvalidation_0-mlogloss:0.95381\n",
      "[445]\tvalidation_0-mlogloss:0.95349\n",
      "[446]\tvalidation_0-mlogloss:0.95334\n",
      "[447]\tvalidation_0-mlogloss:0.95308\n",
      "[448]\tvalidation_0-mlogloss:0.95264\n",
      "[449]\tvalidation_0-mlogloss:0.95233\n",
      "[450]\tvalidation_0-mlogloss:0.95213\n",
      "[451]\tvalidation_0-mlogloss:0.95161\n",
      "[452]\tvalidation_0-mlogloss:0.95136\n",
      "[453]\tvalidation_0-mlogloss:0.95088\n",
      "[454]\tvalidation_0-mlogloss:0.95075\n",
      "[455]\tvalidation_0-mlogloss:0.95065\n",
      "[456]\tvalidation_0-mlogloss:0.95064\n",
      "[457]\tvalidation_0-mlogloss:0.95091\n",
      "[458]\tvalidation_0-mlogloss:0.95046\n",
      "[459]\tvalidation_0-mlogloss:0.95037\n",
      "[460]\tvalidation_0-mlogloss:0.95010\n",
      "[461]\tvalidation_0-mlogloss:0.94968\n",
      "[462]\tvalidation_0-mlogloss:0.94948\n",
      "[463]\tvalidation_0-mlogloss:0.94937\n",
      "[464]\tvalidation_0-mlogloss:0.94907\n",
      "[465]\tvalidation_0-mlogloss:0.94861\n",
      "[466]\tvalidation_0-mlogloss:0.94834\n",
      "[467]\tvalidation_0-mlogloss:0.94834\n",
      "[468]\tvalidation_0-mlogloss:0.94817\n",
      "[469]\tvalidation_0-mlogloss:0.94764\n",
      "[470]\tvalidation_0-mlogloss:0.94763\n",
      "[471]\tvalidation_0-mlogloss:0.94755\n",
      "[472]\tvalidation_0-mlogloss:0.94743\n",
      "[473]\tvalidation_0-mlogloss:0.94708\n",
      "[474]\tvalidation_0-mlogloss:0.94744\n",
      "[475]\tvalidation_0-mlogloss:0.94744\n",
      "[476]\tvalidation_0-mlogloss:0.94737\n",
      "[477]\tvalidation_0-mlogloss:0.94721\n",
      "[478]\tvalidation_0-mlogloss:0.94704\n",
      "[479]\tvalidation_0-mlogloss:0.94658\n",
      "[480]\tvalidation_0-mlogloss:0.94639\n",
      "[481]\tvalidation_0-mlogloss:0.94608\n",
      "[482]\tvalidation_0-mlogloss:0.94581\n",
      "[483]\tvalidation_0-mlogloss:0.94576\n",
      "[484]\tvalidation_0-mlogloss:0.94557\n",
      "[485]\tvalidation_0-mlogloss:0.94554\n",
      "[486]\tvalidation_0-mlogloss:0.94572\n",
      "[487]\tvalidation_0-mlogloss:0.94532\n",
      "[488]\tvalidation_0-mlogloss:0.94523\n",
      "[489]\tvalidation_0-mlogloss:0.94464\n",
      "[490]\tvalidation_0-mlogloss:0.94441\n",
      "[491]\tvalidation_0-mlogloss:0.94433\n",
      "[492]\tvalidation_0-mlogloss:0.94451\n",
      "[493]\tvalidation_0-mlogloss:0.94380\n",
      "[494]\tvalidation_0-mlogloss:0.94317\n",
      "[495]\tvalidation_0-mlogloss:0.94285\n",
      "[496]\tvalidation_0-mlogloss:0.94291\n",
      "[497]\tvalidation_0-mlogloss:0.94272\n",
      "[498]\tvalidation_0-mlogloss:0.94274\n",
      "[499]\tvalidation_0-mlogloss:0.94288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.64      0.63      1499\n",
      "           1       0.68      0.47      0.56      1499\n",
      "           2       0.80      0.85      0.83      1499\n",
      "           3       0.82      0.38      0.52      1499\n",
      "           4       0.96      0.97      0.96      1499\n",
      "           5       0.93      0.88      0.90      1499\n",
      "           6       0.88      0.95      0.91      1499\n",
      "           7       0.96      0.75      0.84      1499\n",
      "           8       0.77      0.86      0.81      1499\n",
      "           9       0.84      0.77      0.80      1499\n",
      "          10       0.97      0.92      0.94      1499\n",
      "          11       0.96      0.98      0.97      1499\n",
      "          12       0.77      0.93      0.85      1499\n",
      "          13       0.75      0.88      0.81      1499\n",
      "          14       0.55      0.68      0.61      1499\n",
      "          15       0.93      0.97      0.95      1499\n",
      "          16       0.87      0.69      0.77      1499\n",
      "          17       0.87      0.80      0.83      1499\n",
      "          18       0.66      0.92      0.77      1499\n",
      "          19       0.62      0.35      0.45      1499\n",
      "          20       0.91      0.96      0.94      1499\n",
      "          21       0.68      0.83      0.75      1499\n",
      "          22       0.63      0.75      0.69      1499\n",
      "          23       0.96      0.97      0.96      1499\n",
      "          24       0.98      0.94      0.96      1499\n",
      "          25       0.94      0.47      0.63      1499\n",
      "          26       0.85      0.60      0.70      1499\n",
      "          27       0.62      0.51      0.56      1499\n",
      "          28       0.77      0.70      0.73      1499\n",
      "          29       0.84      0.98      0.91      1499\n",
      "          30       0.67      0.77      0.72      1499\n",
      "          31       0.60      0.87      0.71      1499\n",
      "          32       0.74      0.86      0.80      1499\n",
      "          33       0.91      0.67      0.77      1499\n",
      "          34       0.97      0.99      0.98      1499\n",
      "          35       0.78      0.82      0.80      1499\n",
      "          36       0.91      0.95      0.93      1499\n",
      "          37       0.61      0.77      0.68      1499\n",
      "          38       0.75      0.58      0.65      1499\n",
      "          39       0.80      0.76      0.78      1499\n",
      "          40       0.95      0.93      0.94      1499\n",
      "          41       0.91      0.86      0.88      1499\n",
      "          42       0.72      0.53      0.61      1499\n",
      "          43       0.80      0.86      0.83      1499\n",
      "          44       0.75      0.90      0.82      1499\n",
      "          45       0.73      0.67      0.70      1499\n",
      "          46       0.67      0.78      0.72      1499\n",
      "          47       0.90      0.75      0.82      1499\n",
      "          48       0.81      0.94      0.87      1499\n",
      "          49       0.58      0.81      0.68      1499\n",
      "\n",
      "    accuracy                           0.79     74950\n",
      "   macro avg       0.80      0.79      0.78     74950\n",
      "weighted avg       0.80      0.79      0.78     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8f654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "300aa492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 16s 847us/step - loss: 1.1451 - val_loss: 1.6261\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 16s 859us/step - loss: 0.4678 - val_loss: 1.6150\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 17s 881us/step - loss: 0.3352 - val_loss: 1.4612\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 17s 902us/step - loss: 0.2695 - val_loss: 1.4778\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 16s 872us/step - loss: 0.2351 - val_loss: 1.4624\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 17s 885us/step - loss: 0.2118 - val_loss: 1.3247\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 19s 989us/step - loss: 0.1952 - val_loss: 1.3887\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 16s 879us/step - loss: 0.1812 - val_loss: 1.4315\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 17s 893us/step - loss: 0.1709 - val_loss: 1.3067\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 17s 898us/step - loss: 0.1623 - val_loss: 1.3180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x30735a410>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 1s 438us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69      1499\n",
      "           1       0.56      0.64      0.59      1499\n",
      "           2       0.68      0.83      0.75      1499\n",
      "           3       0.73      0.26      0.39      1499\n",
      "           4       0.98      0.95      0.97      1499\n",
      "           5       0.97      0.88      0.92      1499\n",
      "           6       0.95      0.91      0.93      1499\n",
      "           7       0.97      0.79      0.87      1499\n",
      "           8       0.55      0.80      0.65      1499\n",
      "           9       0.84      0.74      0.79      1499\n",
      "          10       0.96      0.88      0.92      1499\n",
      "          11       0.93      0.98      0.95      1499\n",
      "          12       0.71      0.88      0.78      1499\n",
      "          13       0.78      0.80      0.79      1499\n",
      "          14       0.61      0.71      0.66      1499\n",
      "          15       0.94      0.96      0.95      1499\n",
      "          16       0.84      0.36      0.50      1499\n",
      "          17       0.96      0.87      0.91      1499\n",
      "          18       0.68      0.94      0.79      1499\n",
      "          19       0.60      0.34      0.44      1499\n",
      "          20       0.96      0.95      0.95      1499\n",
      "          21       0.68      0.85      0.75      1499\n",
      "          22       0.69      0.68      0.68      1499\n",
      "          23       0.99      0.94      0.97      1499\n",
      "          24       0.95      0.83      0.89      1499\n",
      "          25       0.94      0.53      0.68      1499\n",
      "          26       0.57      0.60      0.59      1499\n",
      "          27       0.51      0.32      0.39      1499\n",
      "          28       0.90      0.91      0.90      1499\n",
      "          29       0.91      1.00      0.95      1499\n",
      "          30       0.82      0.83      0.82      1499\n",
      "          31       0.69      0.84      0.76      1499\n",
      "          32       0.83      0.82      0.82      1499\n",
      "          33       0.94      0.60      0.73      1499\n",
      "          34       0.99      0.99      0.99      1499\n",
      "          35       0.71      0.79      0.75      1499\n",
      "          36       0.89      0.87      0.88      1499\n",
      "          37       0.54      0.85      0.66      1499\n",
      "          38       0.72      0.70      0.71      1499\n",
      "          39       0.49      0.81      0.61      1499\n",
      "          40       0.94      0.88      0.91      1499\n",
      "          41       0.79      0.97      0.87      1499\n",
      "          42       0.72      0.58      0.64      1499\n",
      "          43       0.74      0.76      0.75      1499\n",
      "          44       0.82      0.82      0.82      1499\n",
      "          45       0.73      0.52      0.61      1499\n",
      "          46       0.74      0.83      0.78      1499\n",
      "          47       0.76      0.71      0.73      1499\n",
      "          48       0.79      0.94      0.86      1499\n",
      "          49       0.67      0.78      0.72      1499\n",
      "\n",
      "    accuracy                           0.77     74950\n",
      "   macro avg       0.79      0.77      0.77     74950\n",
      "weighted avg       0.79      0.77      0.77     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.02927\n",
      "[1]\tvalidation_0-mlogloss:2.74625\n",
      "[2]\tvalidation_0-mlogloss:2.55927\n",
      "[3]\tvalidation_0-mlogloss:2.41789\n",
      "[4]\tvalidation_0-mlogloss:2.29925\n",
      "[5]\tvalidation_0-mlogloss:2.20087\n",
      "[6]\tvalidation_0-mlogloss:2.11668\n",
      "[7]\tvalidation_0-mlogloss:2.04918\n",
      "[8]\tvalidation_0-mlogloss:1.98173\n",
      "[9]\tvalidation_0-mlogloss:1.92439\n",
      "[10]\tvalidation_0-mlogloss:1.86914\n",
      "[11]\tvalidation_0-mlogloss:1.82111\n",
      "[12]\tvalidation_0-mlogloss:1.78006\n",
      "[13]\tvalidation_0-mlogloss:1.74138\n",
      "[14]\tvalidation_0-mlogloss:1.70463\n",
      "[15]\tvalidation_0-mlogloss:1.67397\n",
      "[16]\tvalidation_0-mlogloss:1.64273\n",
      "[17]\tvalidation_0-mlogloss:1.61474\n",
      "[18]\tvalidation_0-mlogloss:1.58396\n",
      "[19]\tvalidation_0-mlogloss:1.55625\n",
      "[20]\tvalidation_0-mlogloss:1.53280\n",
      "[21]\tvalidation_0-mlogloss:1.50739\n",
      "[22]\tvalidation_0-mlogloss:1.48700\n",
      "[23]\tvalidation_0-mlogloss:1.46718\n",
      "[24]\tvalidation_0-mlogloss:1.44793\n",
      "[25]\tvalidation_0-mlogloss:1.42842\n",
      "[26]\tvalidation_0-mlogloss:1.40921\n",
      "[27]\tvalidation_0-mlogloss:1.39221\n",
      "[28]\tvalidation_0-mlogloss:1.37468\n",
      "[29]\tvalidation_0-mlogloss:1.35718\n",
      "[30]\tvalidation_0-mlogloss:1.34120\n",
      "[31]\tvalidation_0-mlogloss:1.32474\n",
      "[32]\tvalidation_0-mlogloss:1.30944\n",
      "[33]\tvalidation_0-mlogloss:1.29582\n",
      "[34]\tvalidation_0-mlogloss:1.28208\n",
      "[35]\tvalidation_0-mlogloss:1.27045\n",
      "[36]\tvalidation_0-mlogloss:1.25959\n",
      "[37]\tvalidation_0-mlogloss:1.24735\n",
      "[38]\tvalidation_0-mlogloss:1.23374\n",
      "[39]\tvalidation_0-mlogloss:1.22382\n",
      "[40]\tvalidation_0-mlogloss:1.21511\n",
      "[41]\tvalidation_0-mlogloss:1.20389\n",
      "[42]\tvalidation_0-mlogloss:1.19151\n",
      "[43]\tvalidation_0-mlogloss:1.17916\n",
      "[44]\tvalidation_0-mlogloss:1.16767\n",
      "[45]\tvalidation_0-mlogloss:1.15549\n",
      "[46]\tvalidation_0-mlogloss:1.14574\n",
      "[47]\tvalidation_0-mlogloss:1.13601\n",
      "[48]\tvalidation_0-mlogloss:1.12852\n",
      "[49]\tvalidation_0-mlogloss:1.11997\n",
      "[50]\tvalidation_0-mlogloss:1.11146\n",
      "[51]\tvalidation_0-mlogloss:1.10185\n",
      "[52]\tvalidation_0-mlogloss:1.09109\n",
      "[53]\tvalidation_0-mlogloss:1.08464\n",
      "[54]\tvalidation_0-mlogloss:1.07628\n",
      "[55]\tvalidation_0-mlogloss:1.06842\n",
      "[56]\tvalidation_0-mlogloss:1.05994\n",
      "[57]\tvalidation_0-mlogloss:1.05389\n",
      "[58]\tvalidation_0-mlogloss:1.04717\n",
      "[59]\tvalidation_0-mlogloss:1.03920\n",
      "[60]\tvalidation_0-mlogloss:1.03100\n",
      "[61]\tvalidation_0-mlogloss:1.02377\n",
      "[62]\tvalidation_0-mlogloss:1.01846\n",
      "[63]\tvalidation_0-mlogloss:1.01210\n",
      "[64]\tvalidation_0-mlogloss:1.00544\n",
      "[65]\tvalidation_0-mlogloss:0.99750\n",
      "[66]\tvalidation_0-mlogloss:0.99301\n",
      "[67]\tvalidation_0-mlogloss:0.98790\n",
      "[68]\tvalidation_0-mlogloss:0.98285\n",
      "[69]\tvalidation_0-mlogloss:0.97710\n",
      "[70]\tvalidation_0-mlogloss:0.97154\n",
      "[71]\tvalidation_0-mlogloss:0.96539\n",
      "[72]\tvalidation_0-mlogloss:0.95938\n",
      "[73]\tvalidation_0-mlogloss:0.95566\n",
      "[74]\tvalidation_0-mlogloss:0.95148\n",
      "[75]\tvalidation_0-mlogloss:0.94599\n",
      "[76]\tvalidation_0-mlogloss:0.94061\n",
      "[77]\tvalidation_0-mlogloss:0.93725\n",
      "[78]\tvalidation_0-mlogloss:0.93189\n",
      "[79]\tvalidation_0-mlogloss:0.92642\n",
      "[80]\tvalidation_0-mlogloss:0.92231\n",
      "[81]\tvalidation_0-mlogloss:0.91723\n",
      "[82]\tvalidation_0-mlogloss:0.91201\n",
      "[83]\tvalidation_0-mlogloss:0.90774\n",
      "[84]\tvalidation_0-mlogloss:0.90301\n",
      "[85]\tvalidation_0-mlogloss:0.89928\n",
      "[86]\tvalidation_0-mlogloss:0.89468\n",
      "[87]\tvalidation_0-mlogloss:0.89004\n",
      "[88]\tvalidation_0-mlogloss:0.88499\n",
      "[89]\tvalidation_0-mlogloss:0.88093\n",
      "[90]\tvalidation_0-mlogloss:0.87729\n",
      "[91]\tvalidation_0-mlogloss:0.87316\n",
      "[92]\tvalidation_0-mlogloss:0.86985\n",
      "[93]\tvalidation_0-mlogloss:0.86559\n",
      "[94]\tvalidation_0-mlogloss:0.86111\n",
      "[95]\tvalidation_0-mlogloss:0.85798\n",
      "[96]\tvalidation_0-mlogloss:0.85397\n",
      "[97]\tvalidation_0-mlogloss:0.85087\n",
      "[98]\tvalidation_0-mlogloss:0.84785\n",
      "[99]\tvalidation_0-mlogloss:0.84467\n",
      "[100]\tvalidation_0-mlogloss:0.84151\n",
      "[101]\tvalidation_0-mlogloss:0.83727\n",
      "[102]\tvalidation_0-mlogloss:0.83421\n",
      "[103]\tvalidation_0-mlogloss:0.83188\n",
      "[104]\tvalidation_0-mlogloss:0.82928\n",
      "[105]\tvalidation_0-mlogloss:0.82718\n",
      "[106]\tvalidation_0-mlogloss:0.82477\n",
      "[107]\tvalidation_0-mlogloss:0.82200\n",
      "[108]\tvalidation_0-mlogloss:0.81905\n",
      "[109]\tvalidation_0-mlogloss:0.81672\n",
      "[110]\tvalidation_0-mlogloss:0.81347\n",
      "[111]\tvalidation_0-mlogloss:0.81104\n",
      "[112]\tvalidation_0-mlogloss:0.80891\n",
      "[113]\tvalidation_0-mlogloss:0.80646\n",
      "[114]\tvalidation_0-mlogloss:0.80484\n",
      "[115]\tvalidation_0-mlogloss:0.80253\n",
      "[116]\tvalidation_0-mlogloss:0.80061\n",
      "[117]\tvalidation_0-mlogloss:0.79798\n",
      "[118]\tvalidation_0-mlogloss:0.79611\n",
      "[119]\tvalidation_0-mlogloss:0.79404\n",
      "[120]\tvalidation_0-mlogloss:0.79240\n",
      "[121]\tvalidation_0-mlogloss:0.79071\n",
      "[122]\tvalidation_0-mlogloss:0.78794\n",
      "[123]\tvalidation_0-mlogloss:0.78544\n",
      "[124]\tvalidation_0-mlogloss:0.78273\n",
      "[125]\tvalidation_0-mlogloss:0.78058\n",
      "[126]\tvalidation_0-mlogloss:0.77820\n",
      "[127]\tvalidation_0-mlogloss:0.77588\n",
      "[128]\tvalidation_0-mlogloss:0.77343\n",
      "[129]\tvalidation_0-mlogloss:0.77074\n",
      "[130]\tvalidation_0-mlogloss:0.76924\n",
      "[131]\tvalidation_0-mlogloss:0.76730\n",
      "[132]\tvalidation_0-mlogloss:0.76577\n",
      "[133]\tvalidation_0-mlogloss:0.76461\n",
      "[134]\tvalidation_0-mlogloss:0.76269\n",
      "[135]\tvalidation_0-mlogloss:0.76103\n",
      "[136]\tvalidation_0-mlogloss:0.75979\n",
      "[137]\tvalidation_0-mlogloss:0.75915\n",
      "[138]\tvalidation_0-mlogloss:0.75726\n",
      "[139]\tvalidation_0-mlogloss:0.75452\n",
      "[140]\tvalidation_0-mlogloss:0.75251\n",
      "[141]\tvalidation_0-mlogloss:0.75167\n",
      "[142]\tvalidation_0-mlogloss:0.75005\n",
      "[143]\tvalidation_0-mlogloss:0.74780\n",
      "[144]\tvalidation_0-mlogloss:0.74590\n",
      "[145]\tvalidation_0-mlogloss:0.74389\n",
      "[146]\tvalidation_0-mlogloss:0.74141\n",
      "[147]\tvalidation_0-mlogloss:0.74030\n",
      "[148]\tvalidation_0-mlogloss:0.73785\n",
      "[149]\tvalidation_0-mlogloss:0.73519\n",
      "[150]\tvalidation_0-mlogloss:0.73346\n",
      "[151]\tvalidation_0-mlogloss:0.73223\n",
      "[152]\tvalidation_0-mlogloss:0.73116\n",
      "[153]\tvalidation_0-mlogloss:0.72952\n",
      "[154]\tvalidation_0-mlogloss:0.72774\n",
      "[155]\tvalidation_0-mlogloss:0.72686\n",
      "[156]\tvalidation_0-mlogloss:0.72584\n",
      "[157]\tvalidation_0-mlogloss:0.72383\n",
      "[158]\tvalidation_0-mlogloss:0.72322\n",
      "[159]\tvalidation_0-mlogloss:0.72277\n",
      "[160]\tvalidation_0-mlogloss:0.72217\n",
      "[161]\tvalidation_0-mlogloss:0.72074\n",
      "[162]\tvalidation_0-mlogloss:0.72011\n",
      "[163]\tvalidation_0-mlogloss:0.71843\n",
      "[164]\tvalidation_0-mlogloss:0.71713\n",
      "[165]\tvalidation_0-mlogloss:0.71605\n",
      "[166]\tvalidation_0-mlogloss:0.71395\n",
      "[167]\tvalidation_0-mlogloss:0.71189\n",
      "[168]\tvalidation_0-mlogloss:0.71130\n",
      "[169]\tvalidation_0-mlogloss:0.70958\n",
      "[170]\tvalidation_0-mlogloss:0.70850\n",
      "[171]\tvalidation_0-mlogloss:0.70776\n",
      "[172]\tvalidation_0-mlogloss:0.70675\n",
      "[173]\tvalidation_0-mlogloss:0.70558\n",
      "[174]\tvalidation_0-mlogloss:0.70380\n",
      "[175]\tvalidation_0-mlogloss:0.70205\n",
      "[176]\tvalidation_0-mlogloss:0.70129\n",
      "[177]\tvalidation_0-mlogloss:0.70105\n",
      "[178]\tvalidation_0-mlogloss:0.70017\n",
      "[179]\tvalidation_0-mlogloss:0.69926\n",
      "[180]\tvalidation_0-mlogloss:0.69757\n",
      "[181]\tvalidation_0-mlogloss:0.69555\n",
      "[182]\tvalidation_0-mlogloss:0.69343\n",
      "[183]\tvalidation_0-mlogloss:0.69186\n",
      "[184]\tvalidation_0-mlogloss:0.69111\n",
      "[185]\tvalidation_0-mlogloss:0.68966\n",
      "[186]\tvalidation_0-mlogloss:0.68828\n",
      "[187]\tvalidation_0-mlogloss:0.68713\n",
      "[188]\tvalidation_0-mlogloss:0.68622\n",
      "[189]\tvalidation_0-mlogloss:0.68548\n",
      "[190]\tvalidation_0-mlogloss:0.68481\n",
      "[191]\tvalidation_0-mlogloss:0.68358\n",
      "[192]\tvalidation_0-mlogloss:0.68275\n",
      "[193]\tvalidation_0-mlogloss:0.68205\n",
      "[194]\tvalidation_0-mlogloss:0.68172\n",
      "[195]\tvalidation_0-mlogloss:0.68068\n",
      "[196]\tvalidation_0-mlogloss:0.67970\n",
      "[197]\tvalidation_0-mlogloss:0.67897\n",
      "[198]\tvalidation_0-mlogloss:0.67790\n",
      "[199]\tvalidation_0-mlogloss:0.67662\n",
      "[200]\tvalidation_0-mlogloss:0.67547\n",
      "[201]\tvalidation_0-mlogloss:0.67488\n",
      "[202]\tvalidation_0-mlogloss:0.67427\n",
      "[203]\tvalidation_0-mlogloss:0.67323\n",
      "[204]\tvalidation_0-mlogloss:0.67222\n",
      "[205]\tvalidation_0-mlogloss:0.67142\n",
      "[206]\tvalidation_0-mlogloss:0.67095\n",
      "[207]\tvalidation_0-mlogloss:0.67000\n",
      "[208]\tvalidation_0-mlogloss:0.66936\n",
      "[209]\tvalidation_0-mlogloss:0.66850\n",
      "[210]\tvalidation_0-mlogloss:0.66782\n",
      "[211]\tvalidation_0-mlogloss:0.66680\n",
      "[212]\tvalidation_0-mlogloss:0.66627\n",
      "[213]\tvalidation_0-mlogloss:0.66526\n",
      "[214]\tvalidation_0-mlogloss:0.66429\n",
      "[215]\tvalidation_0-mlogloss:0.66357\n",
      "[216]\tvalidation_0-mlogloss:0.66297\n",
      "[217]\tvalidation_0-mlogloss:0.66207\n",
      "[218]\tvalidation_0-mlogloss:0.66079\n",
      "[219]\tvalidation_0-mlogloss:0.66032\n",
      "[220]\tvalidation_0-mlogloss:0.65925\n",
      "[221]\tvalidation_0-mlogloss:0.65809\n",
      "[222]\tvalidation_0-mlogloss:0.65725\n",
      "[223]\tvalidation_0-mlogloss:0.65699\n",
      "[224]\tvalidation_0-mlogloss:0.65587\n",
      "[225]\tvalidation_0-mlogloss:0.65576\n",
      "[226]\tvalidation_0-mlogloss:0.65581\n",
      "[227]\tvalidation_0-mlogloss:0.65547\n",
      "[228]\tvalidation_0-mlogloss:0.65434\n",
      "[229]\tvalidation_0-mlogloss:0.65395\n",
      "[230]\tvalidation_0-mlogloss:0.65311\n",
      "[231]\tvalidation_0-mlogloss:0.65244\n",
      "[232]\tvalidation_0-mlogloss:0.65216\n",
      "[233]\tvalidation_0-mlogloss:0.65137\n",
      "[234]\tvalidation_0-mlogloss:0.65085\n",
      "[235]\tvalidation_0-mlogloss:0.65069\n",
      "[236]\tvalidation_0-mlogloss:0.65051\n",
      "[237]\tvalidation_0-mlogloss:0.65042\n",
      "[238]\tvalidation_0-mlogloss:0.65013\n",
      "[239]\tvalidation_0-mlogloss:0.64923\n",
      "[240]\tvalidation_0-mlogloss:0.64892\n",
      "[241]\tvalidation_0-mlogloss:0.64773\n",
      "[242]\tvalidation_0-mlogloss:0.64688\n",
      "[243]\tvalidation_0-mlogloss:0.64620\n",
      "[244]\tvalidation_0-mlogloss:0.64654\n",
      "[245]\tvalidation_0-mlogloss:0.64596\n",
      "[246]\tvalidation_0-mlogloss:0.64541\n",
      "[247]\tvalidation_0-mlogloss:0.64517\n",
      "[248]\tvalidation_0-mlogloss:0.64532\n",
      "[249]\tvalidation_0-mlogloss:0.64471\n",
      "[250]\tvalidation_0-mlogloss:0.64376\n",
      "[251]\tvalidation_0-mlogloss:0.64328\n",
      "[252]\tvalidation_0-mlogloss:0.64363\n",
      "[253]\tvalidation_0-mlogloss:0.64356\n",
      "[254]\tvalidation_0-mlogloss:0.64318\n",
      "[255]\tvalidation_0-mlogloss:0.64223\n",
      "[256]\tvalidation_0-mlogloss:0.64137\n",
      "[257]\tvalidation_0-mlogloss:0.64075\n",
      "[258]\tvalidation_0-mlogloss:0.63983\n",
      "[259]\tvalidation_0-mlogloss:0.63866\n",
      "[260]\tvalidation_0-mlogloss:0.63828\n",
      "[261]\tvalidation_0-mlogloss:0.63772\n",
      "[262]\tvalidation_0-mlogloss:0.63704\n",
      "[263]\tvalidation_0-mlogloss:0.63611\n",
      "[264]\tvalidation_0-mlogloss:0.63591\n",
      "[265]\tvalidation_0-mlogloss:0.63558\n",
      "[266]\tvalidation_0-mlogloss:0.63533\n",
      "[267]\tvalidation_0-mlogloss:0.63504\n",
      "[268]\tvalidation_0-mlogloss:0.63490\n",
      "[269]\tvalidation_0-mlogloss:0.63446\n",
      "[270]\tvalidation_0-mlogloss:0.63394\n",
      "[271]\tvalidation_0-mlogloss:0.63351\n",
      "[272]\tvalidation_0-mlogloss:0.63300\n",
      "[273]\tvalidation_0-mlogloss:0.63236\n",
      "[274]\tvalidation_0-mlogloss:0.63162\n",
      "[275]\tvalidation_0-mlogloss:0.63082\n",
      "[276]\tvalidation_0-mlogloss:0.63025\n",
      "[277]\tvalidation_0-mlogloss:0.62926\n",
      "[278]\tvalidation_0-mlogloss:0.62899\n",
      "[279]\tvalidation_0-mlogloss:0.62855\n",
      "[280]\tvalidation_0-mlogloss:0.62840\n",
      "[281]\tvalidation_0-mlogloss:0.62765\n",
      "[282]\tvalidation_0-mlogloss:0.62765\n",
      "[283]\tvalidation_0-mlogloss:0.62685\n",
      "[284]\tvalidation_0-mlogloss:0.62617\n",
      "[285]\tvalidation_0-mlogloss:0.62569\n",
      "[286]\tvalidation_0-mlogloss:0.62551\n",
      "[287]\tvalidation_0-mlogloss:0.62492\n",
      "[288]\tvalidation_0-mlogloss:0.62437\n",
      "[289]\tvalidation_0-mlogloss:0.62362\n",
      "[290]\tvalidation_0-mlogloss:0.62309\n",
      "[291]\tvalidation_0-mlogloss:0.62251\n",
      "[292]\tvalidation_0-mlogloss:0.62208\n",
      "[293]\tvalidation_0-mlogloss:0.62178\n",
      "[294]\tvalidation_0-mlogloss:0.62180\n",
      "[295]\tvalidation_0-mlogloss:0.62124\n",
      "[296]\tvalidation_0-mlogloss:0.62094\n",
      "[297]\tvalidation_0-mlogloss:0.62066\n",
      "[298]\tvalidation_0-mlogloss:0.62029\n",
      "[299]\tvalidation_0-mlogloss:0.61992\n",
      "[300]\tvalidation_0-mlogloss:0.61963\n",
      "[301]\tvalidation_0-mlogloss:0.61946\n",
      "[302]\tvalidation_0-mlogloss:0.61907\n",
      "[303]\tvalidation_0-mlogloss:0.61885\n",
      "[304]\tvalidation_0-mlogloss:0.61852\n",
      "[305]\tvalidation_0-mlogloss:0.61802\n",
      "[306]\tvalidation_0-mlogloss:0.61769\n",
      "[307]\tvalidation_0-mlogloss:0.61733\n",
      "[308]\tvalidation_0-mlogloss:0.61723\n",
      "[309]\tvalidation_0-mlogloss:0.61704\n",
      "[310]\tvalidation_0-mlogloss:0.61692\n",
      "[311]\tvalidation_0-mlogloss:0.61698\n",
      "[312]\tvalidation_0-mlogloss:0.61670\n",
      "[313]\tvalidation_0-mlogloss:0.61646\n",
      "[314]\tvalidation_0-mlogloss:0.61584\n",
      "[315]\tvalidation_0-mlogloss:0.61564\n",
      "[316]\tvalidation_0-mlogloss:0.61500\n",
      "[317]\tvalidation_0-mlogloss:0.61458\n",
      "[318]\tvalidation_0-mlogloss:0.61461\n",
      "[319]\tvalidation_0-mlogloss:0.61439\n",
      "[320]\tvalidation_0-mlogloss:0.61376\n",
      "[321]\tvalidation_0-mlogloss:0.61322\n",
      "[322]\tvalidation_0-mlogloss:0.61283\n",
      "[323]\tvalidation_0-mlogloss:0.61283\n",
      "[324]\tvalidation_0-mlogloss:0.61277\n",
      "[325]\tvalidation_0-mlogloss:0.61282\n",
      "[326]\tvalidation_0-mlogloss:0.61284\n",
      "[327]\tvalidation_0-mlogloss:0.61297\n",
      "[328]\tvalidation_0-mlogloss:0.61240\n",
      "[329]\tvalidation_0-mlogloss:0.61169\n",
      "[330]\tvalidation_0-mlogloss:0.61118\n",
      "[331]\tvalidation_0-mlogloss:0.61062\n",
      "[332]\tvalidation_0-mlogloss:0.61021\n",
      "[333]\tvalidation_0-mlogloss:0.60997\n",
      "[334]\tvalidation_0-mlogloss:0.60943\n",
      "[335]\tvalidation_0-mlogloss:0.60915\n",
      "[336]\tvalidation_0-mlogloss:0.60880\n",
      "[337]\tvalidation_0-mlogloss:0.60870\n",
      "[338]\tvalidation_0-mlogloss:0.60872\n",
      "[339]\tvalidation_0-mlogloss:0.60831\n",
      "[340]\tvalidation_0-mlogloss:0.60842\n",
      "[341]\tvalidation_0-mlogloss:0.60809\n",
      "[342]\tvalidation_0-mlogloss:0.60792\n",
      "[343]\tvalidation_0-mlogloss:0.60786\n",
      "[344]\tvalidation_0-mlogloss:0.60759\n",
      "[345]\tvalidation_0-mlogloss:0.60743\n",
      "[346]\tvalidation_0-mlogloss:0.60703\n",
      "[347]\tvalidation_0-mlogloss:0.60695\n",
      "[348]\tvalidation_0-mlogloss:0.60672\n",
      "[349]\tvalidation_0-mlogloss:0.60668\n",
      "[350]\tvalidation_0-mlogloss:0.60609\n",
      "[351]\tvalidation_0-mlogloss:0.60592\n",
      "[352]\tvalidation_0-mlogloss:0.60551\n",
      "[353]\tvalidation_0-mlogloss:0.60507\n",
      "[354]\tvalidation_0-mlogloss:0.60503\n",
      "[355]\tvalidation_0-mlogloss:0.60450\n",
      "[356]\tvalidation_0-mlogloss:0.60419\n",
      "[357]\tvalidation_0-mlogloss:0.60381\n",
      "[358]\tvalidation_0-mlogloss:0.60392\n",
      "[359]\tvalidation_0-mlogloss:0.60352\n",
      "[360]\tvalidation_0-mlogloss:0.60316\n",
      "[361]\tvalidation_0-mlogloss:0.60272\n",
      "[362]\tvalidation_0-mlogloss:0.60260\n",
      "[363]\tvalidation_0-mlogloss:0.60236\n",
      "[364]\tvalidation_0-mlogloss:0.60194\n",
      "[365]\tvalidation_0-mlogloss:0.60136\n",
      "[366]\tvalidation_0-mlogloss:0.60119\n",
      "[367]\tvalidation_0-mlogloss:0.60101\n",
      "[368]\tvalidation_0-mlogloss:0.60099\n",
      "[369]\tvalidation_0-mlogloss:0.60040\n",
      "[370]\tvalidation_0-mlogloss:0.60065\n",
      "[371]\tvalidation_0-mlogloss:0.60061\n",
      "[372]\tvalidation_0-mlogloss:0.60013\n",
      "[373]\tvalidation_0-mlogloss:0.59998\n",
      "[374]\tvalidation_0-mlogloss:0.59966\n",
      "[375]\tvalidation_0-mlogloss:0.59963\n",
      "[376]\tvalidation_0-mlogloss:0.59980\n",
      "[377]\tvalidation_0-mlogloss:0.59956\n",
      "[378]\tvalidation_0-mlogloss:0.59935\n",
      "[379]\tvalidation_0-mlogloss:0.59916\n",
      "[380]\tvalidation_0-mlogloss:0.59932\n",
      "[381]\tvalidation_0-mlogloss:0.59905\n",
      "[382]\tvalidation_0-mlogloss:0.59887\n",
      "[383]\tvalidation_0-mlogloss:0.59877\n",
      "[384]\tvalidation_0-mlogloss:0.59861\n",
      "[385]\tvalidation_0-mlogloss:0.59822\n",
      "[386]\tvalidation_0-mlogloss:0.59803\n",
      "[387]\tvalidation_0-mlogloss:0.59779\n",
      "[388]\tvalidation_0-mlogloss:0.59765\n",
      "[389]\tvalidation_0-mlogloss:0.59738\n",
      "[390]\tvalidation_0-mlogloss:0.59712\n",
      "[391]\tvalidation_0-mlogloss:0.59708\n",
      "[392]\tvalidation_0-mlogloss:0.59710\n",
      "[393]\tvalidation_0-mlogloss:0.59712\n",
      "[394]\tvalidation_0-mlogloss:0.59698\n",
      "[395]\tvalidation_0-mlogloss:0.59696\n",
      "[396]\tvalidation_0-mlogloss:0.59681\n",
      "[397]\tvalidation_0-mlogloss:0.59629\n",
      "[398]\tvalidation_0-mlogloss:0.59671\n",
      "[399]\tvalidation_0-mlogloss:0.59655\n",
      "[400]\tvalidation_0-mlogloss:0.59649\n",
      "[401]\tvalidation_0-mlogloss:0.59632\n",
      "[402]\tvalidation_0-mlogloss:0.59573\n",
      "[403]\tvalidation_0-mlogloss:0.59517\n",
      "[404]\tvalidation_0-mlogloss:0.59455\n",
      "[405]\tvalidation_0-mlogloss:0.59452\n",
      "[406]\tvalidation_0-mlogloss:0.59450\n",
      "[407]\tvalidation_0-mlogloss:0.59474\n",
      "[408]\tvalidation_0-mlogloss:0.59498\n",
      "[409]\tvalidation_0-mlogloss:0.59504\n",
      "[410]\tvalidation_0-mlogloss:0.59479\n",
      "[411]\tvalidation_0-mlogloss:0.59451\n",
      "[412]\tvalidation_0-mlogloss:0.59438\n",
      "[413]\tvalidation_0-mlogloss:0.59434\n",
      "[414]\tvalidation_0-mlogloss:0.59412\n",
      "[415]\tvalidation_0-mlogloss:0.59410\n",
      "[416]\tvalidation_0-mlogloss:0.59382\n",
      "[417]\tvalidation_0-mlogloss:0.59346\n",
      "[418]\tvalidation_0-mlogloss:0.59334\n",
      "[419]\tvalidation_0-mlogloss:0.59317\n",
      "[420]\tvalidation_0-mlogloss:0.59294\n",
      "[421]\tvalidation_0-mlogloss:0.59316\n",
      "[422]\tvalidation_0-mlogloss:0.59260\n",
      "[423]\tvalidation_0-mlogloss:0.59238\n",
      "[424]\tvalidation_0-mlogloss:0.59204\n",
      "[425]\tvalidation_0-mlogloss:0.59172\n",
      "[426]\tvalidation_0-mlogloss:0.59132\n",
      "[427]\tvalidation_0-mlogloss:0.59127\n",
      "[428]\tvalidation_0-mlogloss:0.59114\n",
      "[429]\tvalidation_0-mlogloss:0.59049\n",
      "[430]\tvalidation_0-mlogloss:0.59044\n",
      "[431]\tvalidation_0-mlogloss:0.59045\n",
      "[432]\tvalidation_0-mlogloss:0.59050\n",
      "[433]\tvalidation_0-mlogloss:0.59049\n",
      "[434]\tvalidation_0-mlogloss:0.59036\n",
      "[435]\tvalidation_0-mlogloss:0.59022\n",
      "[436]\tvalidation_0-mlogloss:0.59036\n",
      "[437]\tvalidation_0-mlogloss:0.59040\n",
      "[438]\tvalidation_0-mlogloss:0.59029\n",
      "[439]\tvalidation_0-mlogloss:0.59034\n",
      "[440]\tvalidation_0-mlogloss:0.59017\n",
      "[441]\tvalidation_0-mlogloss:0.58982\n",
      "[442]\tvalidation_0-mlogloss:0.58951\n",
      "[443]\tvalidation_0-mlogloss:0.58937\n",
      "[444]\tvalidation_0-mlogloss:0.58917\n",
      "[445]\tvalidation_0-mlogloss:0.58904\n",
      "[446]\tvalidation_0-mlogloss:0.58897\n",
      "[447]\tvalidation_0-mlogloss:0.58892\n",
      "[448]\tvalidation_0-mlogloss:0.58865\n",
      "[449]\tvalidation_0-mlogloss:0.58867\n",
      "[450]\tvalidation_0-mlogloss:0.58855\n",
      "[451]\tvalidation_0-mlogloss:0.58838\n",
      "[452]\tvalidation_0-mlogloss:0.58828\n",
      "[453]\tvalidation_0-mlogloss:0.58818\n",
      "[454]\tvalidation_0-mlogloss:0.58808\n",
      "[455]\tvalidation_0-mlogloss:0.58798\n",
      "[456]\tvalidation_0-mlogloss:0.58772\n",
      "[457]\tvalidation_0-mlogloss:0.58750\n",
      "[458]\tvalidation_0-mlogloss:0.58748\n",
      "[459]\tvalidation_0-mlogloss:0.58746\n",
      "[460]\tvalidation_0-mlogloss:0.58749\n",
      "[461]\tvalidation_0-mlogloss:0.58731\n",
      "[462]\tvalidation_0-mlogloss:0.58747\n",
      "[463]\tvalidation_0-mlogloss:0.58718\n",
      "[464]\tvalidation_0-mlogloss:0.58704\n",
      "[465]\tvalidation_0-mlogloss:0.58690\n",
      "[466]\tvalidation_0-mlogloss:0.58689\n",
      "[467]\tvalidation_0-mlogloss:0.58681\n",
      "[468]\tvalidation_0-mlogloss:0.58661\n",
      "[469]\tvalidation_0-mlogloss:0.58653\n",
      "[470]\tvalidation_0-mlogloss:0.58651\n",
      "[471]\tvalidation_0-mlogloss:0.58630\n",
      "[472]\tvalidation_0-mlogloss:0.58616\n",
      "[473]\tvalidation_0-mlogloss:0.58587\n",
      "[474]\tvalidation_0-mlogloss:0.58546\n",
      "[475]\tvalidation_0-mlogloss:0.58534\n",
      "[476]\tvalidation_0-mlogloss:0.58502\n",
      "[477]\tvalidation_0-mlogloss:0.58484\n",
      "[478]\tvalidation_0-mlogloss:0.58494\n",
      "[479]\tvalidation_0-mlogloss:0.58474\n",
      "[480]\tvalidation_0-mlogloss:0.58466\n",
      "[481]\tvalidation_0-mlogloss:0.58431\n",
      "[482]\tvalidation_0-mlogloss:0.58426\n",
      "[483]\tvalidation_0-mlogloss:0.58405\n",
      "[484]\tvalidation_0-mlogloss:0.58407\n",
      "[485]\tvalidation_0-mlogloss:0.58411\n",
      "[486]\tvalidation_0-mlogloss:0.58395\n",
      "[487]\tvalidation_0-mlogloss:0.58387\n",
      "[488]\tvalidation_0-mlogloss:0.58399\n",
      "[489]\tvalidation_0-mlogloss:0.58381\n",
      "[490]\tvalidation_0-mlogloss:0.58367\n",
      "[491]\tvalidation_0-mlogloss:0.58355\n",
      "[492]\tvalidation_0-mlogloss:0.58329\n",
      "[493]\tvalidation_0-mlogloss:0.58306\n",
      "[494]\tvalidation_0-mlogloss:0.58293\n",
      "[495]\tvalidation_0-mlogloss:0.58265\n",
      "[496]\tvalidation_0-mlogloss:0.58258\n",
      "[497]\tvalidation_0-mlogloss:0.58236\n",
      "[498]\tvalidation_0-mlogloss:0.58240\n",
      "[499]\tvalidation_0-mlogloss:0.58219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85      1499\n",
      "           1       0.93      0.88      0.90      1499\n",
      "           2       0.90      0.92      0.91      1499\n",
      "           3       0.98      0.90      0.94      1499\n",
      "           4       0.99      1.00      0.99      1499\n",
      "           5       0.97      0.97      0.97      1499\n",
      "           6       0.95      0.98      0.97      1499\n",
      "           7       0.97      0.91      0.94      1499\n",
      "           8       0.83      0.90      0.87      1499\n",
      "           9       0.86      0.73      0.79      1499\n",
      "          10       0.97      0.94      0.96      1499\n",
      "          11       0.98      0.99      0.99      1499\n",
      "          12       0.91      0.97      0.94      1499\n",
      "          13       0.89      0.94      0.91      1499\n",
      "          14       0.83      0.91      0.87      1499\n",
      "          15       0.97      0.98      0.97      1499\n",
      "          16       0.82      0.89      0.85      1499\n",
      "          17       0.81      0.91      0.86      1499\n",
      "          18       0.89      0.96      0.92      1499\n",
      "          19       0.88      0.48      0.63      1499\n",
      "          20       0.96      0.99      0.98      1499\n",
      "          21       0.93      0.95      0.94      1499\n",
      "          22       0.80      0.82      0.81      1499\n",
      "          23       0.99      0.99      0.99      1499\n",
      "          24       0.98      0.98      0.98      1499\n",
      "          25       0.94      0.58      0.72      1499\n",
      "          26       0.88      0.57      0.69      1499\n",
      "          27       0.68      0.67      0.68      1499\n",
      "          28       0.79      0.84      0.81      1499\n",
      "          29       0.91      1.00      0.95      1499\n",
      "          30       0.86      0.95      0.90      1499\n",
      "          31       0.78      0.96      0.86      1499\n",
      "          32       0.83      0.95      0.89      1499\n",
      "          33       0.94      0.84      0.89      1499\n",
      "          34       0.98      1.00      0.99      1499\n",
      "          35       0.86      0.91      0.89      1499\n",
      "          36       0.76      0.98      0.86      1499\n",
      "          37       0.87      0.86      0.87      1499\n",
      "          38       0.88      0.55      0.68      1499\n",
      "          39       0.86      0.82      0.84      1499\n",
      "          40       0.97      0.91      0.94      1499\n",
      "          41       0.95      0.96      0.95      1499\n",
      "          42       0.88      0.90      0.89      1499\n",
      "          43       0.90      0.92      0.91      1499\n",
      "          44       0.95      0.98      0.97      1499\n",
      "          45       0.77      0.87      0.82      1499\n",
      "          46       0.88      0.67      0.76      1499\n",
      "          47       0.96      0.76      0.85      1499\n",
      "          48       0.80      0.96      0.87      1499\n",
      "          49       0.74      0.95      0.83      1499\n",
      "\n",
      "    accuracy                           0.88     74950\n",
      "   macro avg       0.89      0.88      0.88     74950\n",
      "weighted avg       0.89      0.88      0.88     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1dad4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3361defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 18s 951us/step - loss: 0.7520 - val_loss: 1.6733\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 18s 950us/step - loss: 0.2907 - val_loss: 1.7687\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 17s 931us/step - loss: 0.2129 - val_loss: 1.9209\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 287s 15ms/step - loss: 0.1717 - val_loss: 1.7423\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 23s 1ms/step - loss: 0.1481 - val_loss: 1.7941\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 18s 953us/step - loss: 0.1294 - val_loss: 1.9329\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 16s 862us/step - loss: 0.1156 - val_loss: 1.8122\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 16s 869us/step - loss: 0.1056 - val_loss: 1.5906\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 16s 861us/step - loss: 0.0973 - val_loss: 1.5869\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 16s 869us/step - loss: 0.0905 - val_loss: 1.4476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x30e6b72b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 1s 334us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.81      1499\n",
      "           1       0.83      0.90      0.86      1499\n",
      "           2       0.89      0.79      0.84      1499\n",
      "           3       0.93      0.82      0.87      1499\n",
      "           4       0.95      1.00      0.97      1499\n",
      "           5       0.94      0.94      0.94      1499\n",
      "           6       0.93      0.99      0.96      1499\n",
      "           7       0.99      0.81      0.89      1499\n",
      "           8       0.72      0.89      0.80      1499\n",
      "           9       0.82      0.74      0.78      1499\n",
      "          10       0.77      0.90      0.83      1499\n",
      "          11       0.94      0.96      0.95      1499\n",
      "          12       0.91      0.94      0.92      1499\n",
      "          13       0.65      0.92      0.76      1499\n",
      "          14       0.65      0.74      0.69      1499\n",
      "          15       0.96      0.94      0.95      1499\n",
      "          16       0.70      0.75      0.73      1499\n",
      "          17       0.85      0.71      0.77      1499\n",
      "          18       0.71      0.80      0.75      1499\n",
      "          19       0.85      0.39      0.54      1499\n",
      "          20       0.99      0.98      0.98      1499\n",
      "          21       0.88      0.93      0.90      1499\n",
      "          22       0.65      0.75      0.70      1499\n",
      "          23       0.95      0.98      0.96      1499\n",
      "          24       0.76      0.98      0.85      1499\n",
      "          25       0.90      0.52      0.66      1499\n",
      "          26       0.68      0.63      0.65      1499\n",
      "          27       0.62      0.44      0.52      1499\n",
      "          28       0.66      0.79      0.72      1499\n",
      "          29       0.93      0.92      0.93      1499\n",
      "          30       0.95      0.91      0.93      1499\n",
      "          31       0.88      0.93      0.90      1499\n",
      "          32       0.80      0.92      0.86      1499\n",
      "          33       0.88      0.61      0.72      1499\n",
      "          34       0.99      0.99      0.99      1499\n",
      "          35       0.85      0.72      0.78      1499\n",
      "          36       0.79      0.77      0.78      1499\n",
      "          37       0.72      0.76      0.74      1499\n",
      "          38       0.64      0.21      0.32      1499\n",
      "          39       0.72      0.76      0.74      1499\n",
      "          40       0.92      0.83      0.87      1499\n",
      "          41       0.90      0.94      0.92      1499\n",
      "          42       0.71      0.79      0.75      1499\n",
      "          43       0.73      0.82      0.77      1499\n",
      "          44       0.87      0.88      0.87      1499\n",
      "          45       0.64      0.78      0.70      1499\n",
      "          46       0.57      0.32      0.41      1499\n",
      "          47       0.77      0.89      0.83      1499\n",
      "          48       0.74      0.91      0.81      1499\n",
      "          49       0.65      0.83      0.73      1499\n",
      "\n",
      "    accuracy                           0.81     74950\n",
      "   macro avg       0.81      0.81      0.80     74950\n",
      "weighted avg       0.81      0.81      0.80     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e2629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a125457",
   "metadata": {},
   "source": [
    "# training-test data 60-40, 70-30,80-20,90-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/aditya/Research_Intership/eeg_files/files/S001\\\\S001R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S002\\\\S002R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S003\\\\S003R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S004\\\\S004R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S005\\\\S005R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S006\\\\S006R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S007\\\\S007R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S008\\\\S008R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S009\\\\S009R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S010\\\\S010R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S011\\\\S011R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S012\\\\S012R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S013\\\\S013R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S014\\\\S014R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S015\\\\S015R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S016\\\\S016R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S017\\\\S017R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S018\\\\S018R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S019\\\\S019R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S020\\\\S020R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S021\\\\S021R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S022\\\\S022R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S023\\\\S023R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S024\\\\S024R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S025\\\\S025R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S026\\\\S026R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S027\\\\S027R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S028\\\\S028R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S029\\\\S029R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S030\\\\S030R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S031\\\\S031R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S032\\\\S032R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S033\\\\S033R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S034\\\\S034R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S035\\\\S035R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S036\\\\S036R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S037\\\\S037R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S038\\\\S038R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S039\\\\S039R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S040\\\\S040R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S041\\\\S041R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S042\\\\S042R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S043\\\\S043R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S044\\\\S044R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S045\\\\S045R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S046\\\\S046R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S047\\\\S047R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S048\\\\S048R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S049\\\\S049R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S050\\\\S050R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S051\\\\S051R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S052\\\\S052R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S053\\\\S053R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S054\\\\S054R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S055\\\\S055R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S056\\\\S056R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S057\\\\S057R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S058\\\\S058R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S059\\\\S059R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S060\\\\S060R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S061\\\\S061R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S062\\\\S062R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S063\\\\S063R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S064\\\\S064R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S065\\\\S065R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S066\\\\S066R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S067\\\\S067R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S068\\\\S068R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S069\\\\S069R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S070\\\\S070R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S071\\\\S071R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S072\\\\S072R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S073\\\\S073R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S074\\\\S074R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S075\\\\S075R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S076\\\\S076R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S077\\\\S077R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S078\\\\S078R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S079\\\\S079R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S080\\\\S080R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S081\\\\S081R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S082\\\\S082R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S083\\\\S083R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S084\\\\S084R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S085\\\\S085R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S086\\\\S086R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S087\\\\S087R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S088\\\\S088R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S089\\\\S089R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S090\\\\S090R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S091\\\\S091R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S092\\\\S092R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S093\\\\S093R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S094\\\\S094R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S095\\\\S095R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S096\\\\S096R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S097\\\\S097R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S098\\\\S098R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S099\\\\S099R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S100\\\\S100R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S101\\\\S101R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S102\\\\S102R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S103\\\\S103R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S104\\\\S104R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S105\\\\S105R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S106\\\\S106R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S107\\\\S107R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S108\\\\S108R03.edf',\n",
       " 'C:/aditya/Research_Intership/eeg_files/files/S109\\\\S109R03.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=[]\n",
    "test=[]\n",
    "for i in range(1,no_of_patients+1):\n",
    "    files=glob('C:/aditya/Research_Intership/eeg_files/files/S'+str(str(i).zfill(3))+'/*.edf')\n",
    "    files=files[2:3]\n",
    "    train+=files\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "#     display(eeg_df)\n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample()\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "        \n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    \n",
    "    return eeg_df1,eeg_df2, len(eeg_df1),len(eeg_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe[dataframe.columns[:-1]].values\n",
    "    y=dataframe[dataframe.columns[-1]].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746b37e",
   "metadata": {},
   "source": [
    "## 60-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,ytr,yte=read_data(train[i],0.6) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "\n",
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "\n",
    "ytrain=[]\n",
    "for i in range(1,len(ytemp1)+1):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(1,len(ytemp2)+1):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "\n",
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "# display(xtrain)\n",
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "\n",
    "lr_model=LogisticRegression()\n",
    "lr_model.fit(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c60877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.12      0.06      0.08      5999\n",
      "           2       0.13      0.09      0.11      5999\n",
      "           3       0.19      0.24      0.21      5999\n",
      "           4       0.78      0.76      0.77      5999\n",
      "           5       0.09      0.08      0.09      5999\n",
      "           6       0.20      0.37      0.26      5999\n",
      "           7       0.44      0.70      0.54      5999\n",
      "           8       0.07      0.03      0.04      5999\n",
      "           9       0.10      0.11      0.10      5999\n",
      "          10       0.07      0.07      0.07      5999\n",
      "          11       0.07      0.07      0.07      5999\n",
      "          12       0.27      0.29      0.28      5999\n",
      "          13       0.10      0.12      0.11      5999\n",
      "          14       0.11      0.11      0.11      5999\n",
      "          15       0.06      0.03      0.04      5999\n",
      "          16       0.36      0.29      0.32      5999\n",
      "          17       0.00      0.00      0.00      5999\n",
      "          18       0.49      0.57      0.53      5999\n",
      "          19       0.16      0.14      0.15      5999\n",
      "          20       0.08      0.07      0.08      5999\n",
      "          21       0.48      0.61      0.54      5999\n",
      "          22       0.41      0.42      0.42      5999\n",
      "          23       0.11      0.06      0.08      5999\n",
      "          24       0.28      0.36      0.31      5999\n",
      "          25       0.30      0.34      0.32      5999\n",
      "          26       0.17      0.12      0.15      5999\n",
      "          27       0.09      0.07      0.08      5999\n",
      "          28       0.05      0.06      0.06      5999\n",
      "          29       0.17      0.12      0.15      5999\n",
      "          30       0.20      0.19      0.20      5999\n",
      "          31       0.24      0.27      0.26      5999\n",
      "          32       0.05      0.05      0.05      5999\n",
      "          33       0.38      0.67      0.48      5999\n",
      "          34       0.01      0.02      0.02      5999\n",
      "          35       0.23      0.28      0.25      5999\n",
      "          36       0.18      0.13      0.15      5999\n",
      "          37       0.08      0.09      0.08      5999\n",
      "          38       0.06      0.05      0.05      5999\n",
      "          39       0.01      0.01      0.01      5999\n",
      "          40       0.05      0.03      0.04      5999\n",
      "          41       0.26      0.19      0.22      5999\n",
      "          42       0.04      0.02      0.03      5999\n",
      "          43       0.03      0.03      0.03      5999\n",
      "          44       0.44      0.48      0.46      5999\n",
      "          45       0.15      0.15      0.15      5999\n",
      "          46       0.07      0.06      0.06      5999\n",
      "          47       0.16      0.12      0.14      5999\n",
      "          48       0.12      0.11      0.12      5999\n",
      "          49       0.13      0.16      0.14      5999\n",
      "          50       0.18      0.14      0.15      5999\n",
      "          51       0.11      0.06      0.08      5999\n",
      "          52       0.03      0.02      0.02      5999\n",
      "          53       0.13      0.15      0.14      5999\n",
      "          54       0.15      0.13      0.14      5999\n",
      "          55       0.35      0.38      0.37      5999\n",
      "          56       0.11      0.09      0.10      5999\n",
      "          57       0.10      0.12      0.11      5999\n",
      "          58       0.17      0.16      0.16      5999\n",
      "          59       0.04      0.04      0.04      5999\n",
      "          60       0.06      0.06      0.06      5999\n",
      "          61       0.23      0.19      0.21      5999\n",
      "          62       0.18      0.15      0.16      5999\n",
      "          63       0.15      0.27      0.19      5999\n",
      "          64       0.29      0.42      0.34      5999\n",
      "          65       0.05      0.03      0.04      5999\n",
      "          66       0.18      0.21      0.19      5999\n",
      "          67       0.42      0.70      0.53      5999\n",
      "          68       0.03      0.03      0.03      5999\n",
      "          69       0.16      0.19      0.17      5999\n",
      "          70       0.18      0.15      0.16      5999\n",
      "          71       0.01      0.01      0.01      5999\n",
      "          72       0.08      0.06      0.07      5999\n",
      "          73       0.16      0.16      0.16      5999\n",
      "          74       0.13      0.10      0.11      5999\n",
      "          75       0.08      0.09      0.08      5999\n",
      "          76       0.11      0.21      0.15      5999\n",
      "          77       0.01      0.01      0.01      5999\n",
      "          78       0.18      0.21      0.19      5999\n",
      "          79       0.01      0.01      0.01      5999\n",
      "          80       0.11      0.06      0.07      5999\n",
      "          81       0.31      0.59      0.41      5999\n",
      "          82       0.09      0.06      0.07      5999\n",
      "          83       0.07      0.08      0.07      5999\n",
      "          84       0.03      0.04      0.03      5999\n",
      "          85       0.17      0.23      0.20      5999\n",
      "          86       0.14      0.14      0.14      5999\n",
      "          87       0.10      0.05      0.07      5999\n",
      "          88       0.07      0.04      0.05      5999\n",
      "          89       0.51      0.56      0.53      5999\n",
      "          90       0.15      0.24      0.18      5999\n",
      "          91       0.42      0.63      0.50      5999\n",
      "          92       0.17      0.11      0.13      5999\n",
      "          93       0.10      0.05      0.07      5999\n",
      "          94       0.93      0.99      0.96      5999\n",
      "          95       0.01      0.01      0.01      5999\n",
      "          96       0.37      0.43      0.40      5999\n",
      "          97       0.26      0.40      0.31      5999\n",
      "          98       0.11      0.05      0.07      5999\n",
      "          99       0.09      0.07      0.08      5999\n",
      "         100       0.20      0.26      0.22      5999\n",
      "         101       0.05      0.03      0.04      5999\n",
      "         102       0.11      0.05      0.07      5999\n",
      "         103       0.06      0.06      0.06      5999\n",
      "         104       0.19      0.18      0.18      5999\n",
      "         105       0.22      0.26      0.24      5999\n",
      "         106       0.01      0.01      0.01      5999\n",
      "         107       0.05      0.04      0.04      5999\n",
      "         108       0.15      0.16      0.15      5999\n",
      "         109       0.42      0.45      0.43      5999\n",
      "\n",
      "    accuracy                           0.19    653891\n",
      "   macro avg       0.17      0.19      0.17    653891\n",
      "weighted avg       0.17      0.19      0.17    653891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=lr_model.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42fe31",
   "metadata": {},
   "source": [
    "## 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a100dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,ytr,yte=read_data(train[i],0.7) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "\n",
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "\n",
    "ytrain=[]\n",
    "for i in range(1,len(ytemp1)+1):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(1,len(ytemp2)+1):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "\n",
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "\n",
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "\n",
    "lr_model=LogisticRegression()\n",
    "lr_model.fit(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541e3970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.13      0.05      0.07      4499\n",
      "           2       0.17      0.10      0.13      4499\n",
      "           3       0.15      0.20      0.17      4499\n",
      "           4       0.78      0.76      0.77      4499\n",
      "           5       0.09      0.09      0.09      4499\n",
      "           6       0.21      0.41      0.28      4499\n",
      "           7       0.42      0.69      0.53      4499\n",
      "           8       0.04      0.02      0.03      4499\n",
      "           9       0.13      0.16      0.14      4499\n",
      "          10       0.05      0.06      0.05      4499\n",
      "          11       0.04      0.04      0.04      4499\n",
      "          12       0.29      0.31      0.30      4499\n",
      "          13       0.10      0.12      0.11      4499\n",
      "          14       0.14      0.15      0.14      4499\n",
      "          15       0.06      0.03      0.04      4499\n",
      "          16       0.37      0.27      0.31      4499\n",
      "          17       0.00      0.00      0.00      4499\n",
      "          18       0.48      0.57      0.52      4499\n",
      "          19       0.17      0.16      0.17      4499\n",
      "          20       0.05      0.05      0.05      4499\n",
      "          21       0.47      0.59      0.52      4499\n",
      "          22       0.47      0.44      0.45      4499\n",
      "          23       0.11      0.05      0.07      4499\n",
      "          24       0.29      0.39      0.33      4499\n",
      "          25       0.30      0.35      0.33      4499\n",
      "          26       0.17      0.12      0.14      4499\n",
      "          27       0.19      0.19      0.19      4499\n",
      "          28       0.05      0.06      0.05      4499\n",
      "          29       0.16      0.13      0.14      4499\n",
      "          30       0.22      0.21      0.21      4499\n",
      "          31       0.26      0.32      0.29      4499\n",
      "          32       0.04      0.04      0.04      4499\n",
      "          33       0.37      0.63      0.47      4499\n",
      "          34       0.02      0.04      0.03      4499\n",
      "          35       0.19      0.26      0.22      4499\n",
      "          36       0.19      0.10      0.13      4499\n",
      "          37       0.12      0.16      0.14      4499\n",
      "          38       0.07      0.06      0.06      4499\n",
      "          39       0.01      0.01      0.01      4499\n",
      "          40       0.08      0.05      0.06      4499\n",
      "          41       0.23      0.17      0.19      4499\n",
      "          42       0.05      0.03      0.04      4499\n",
      "          43       0.04      0.03      0.03      4499\n",
      "          44       0.39      0.45      0.42      4499\n",
      "          45       0.15      0.15      0.15      4499\n",
      "          46       0.08      0.07      0.07      4499\n",
      "          47       0.19      0.08      0.11      4499\n",
      "          48       0.09      0.09      0.09      4499\n",
      "          49       0.13      0.15      0.14      4499\n",
      "          50       0.18      0.12      0.15      4499\n",
      "          51       0.08      0.05      0.06      4499\n",
      "          52       0.04      0.02      0.02      4499\n",
      "          53       0.11      0.13      0.12      4499\n",
      "          54       0.14      0.13      0.14      4499\n",
      "          55       0.26      0.36      0.30      4499\n",
      "          56       0.10      0.09      0.09      4499\n",
      "          57       0.15      0.19      0.17      4499\n",
      "          58       0.15      0.16      0.16      4499\n",
      "          59       0.00      0.00      0.00      4499\n",
      "          60       0.12      0.15      0.13      4499\n",
      "          61       0.22      0.20      0.21      4499\n",
      "          62       0.17      0.15      0.16      4499\n",
      "          63       0.14      0.26      0.19      4499\n",
      "          64       0.26      0.35      0.30      4499\n",
      "          65       0.06      0.03      0.04      4499\n",
      "          66       0.20      0.23      0.21      4499\n",
      "          67       0.43      0.69      0.53      4499\n",
      "          68       0.04      0.03      0.03      4499\n",
      "          69       0.16      0.20      0.18      4499\n",
      "          70       0.17      0.14      0.15      4499\n",
      "          71       0.02      0.01      0.02      4499\n",
      "          72       0.11      0.07      0.09      4499\n",
      "          73       0.17      0.19      0.18      4499\n",
      "          74       0.12      0.07      0.09      4499\n",
      "          75       0.10      0.12      0.11      4499\n",
      "          76       0.15      0.21      0.18      4499\n",
      "          77       0.01      0.01      0.01      4499\n",
      "          78       0.08      0.08      0.08      4499\n",
      "          79       0.03      0.02      0.02      4499\n",
      "          80       0.10      0.04      0.05      4499\n",
      "          81       0.36      0.60      0.45      4499\n",
      "          82       0.13      0.09      0.10      4499\n",
      "          83       0.06      0.06      0.06      4499\n",
      "          84       0.04      0.06      0.05      4499\n",
      "          85       0.17      0.21      0.19      4499\n",
      "          86       0.17      0.17      0.17      4499\n",
      "          87       0.10      0.08      0.09      4499\n",
      "          88       0.06      0.04      0.05      4499\n",
      "          89       0.59      0.67      0.63      4499\n",
      "          90       0.16      0.26      0.20      4499\n",
      "          91       0.39      0.63      0.48      4499\n",
      "          92       0.19      0.11      0.14      4499\n",
      "          93       0.13      0.08      0.10      4499\n",
      "          94       0.93      0.99      0.96      4499\n",
      "          95       0.00      0.00      0.00      4499\n",
      "          96       0.38      0.41      0.40      4499\n",
      "          97       0.25      0.37      0.30      4499\n",
      "          98       0.07      0.03      0.04      4499\n",
      "          99       0.11      0.07      0.08      4499\n",
      "         100       0.19      0.26      0.22      4499\n",
      "         101       0.02      0.01      0.01      4499\n",
      "         102       0.11      0.04      0.06      4499\n",
      "         103       0.05      0.05      0.05      4499\n",
      "         104       0.17      0.17      0.17      4499\n",
      "         105       0.27      0.33      0.30      4499\n",
      "         106       0.03      0.02      0.03      4499\n",
      "         107       0.03      0.02      0.02      4499\n",
      "         108       0.14      0.14      0.14      4499\n",
      "         109       0.45      0.50      0.48      4499\n",
      "\n",
      "    accuracy                           0.19    490391\n",
      "   macro avg       0.17      0.19      0.18    490391\n",
      "weighted avg       0.17      0.19      0.18    490391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=lr_model.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306baa6",
   "metadata": {},
   "source": [
    "## 80-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c89803",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,ytr,yte=read_data(train[i],0.8) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "\n",
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "\n",
    "ytrain=[]\n",
    "for i in range(1,len(ytemp1)+1):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(1,len(ytemp2)+1):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "\n",
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "\n",
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "\n",
    "lr_model=LogisticRegression()\n",
    "lr_model.fit(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a59f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.14      0.05      0.07      2999\n",
      "           2       0.18      0.11      0.13      2999\n",
      "           3       0.19      0.28      0.22      2999\n",
      "           4       0.74      0.84      0.79      2999\n",
      "           5       0.06      0.07      0.07      2999\n",
      "           6       0.23      0.43      0.30      2999\n",
      "           7       0.42      0.71      0.53      2999\n",
      "           8       0.12      0.05      0.07      2999\n",
      "           9       0.16      0.21      0.18      2999\n",
      "          10       0.06      0.05      0.06      2999\n",
      "          11       0.13      0.09      0.10      2999\n",
      "          12       0.29      0.31      0.30      2999\n",
      "          13       0.09      0.10      0.10      2999\n",
      "          14       0.19      0.19      0.19      2999\n",
      "          15       0.07      0.03      0.04      2999\n",
      "          16       0.34      0.23      0.27      2999\n",
      "          17       0.00      0.00      0.00      2999\n",
      "          18       0.53      0.60      0.56      2999\n",
      "          19       0.14      0.16      0.15      2999\n",
      "          20       0.10      0.07      0.08      2999\n",
      "          21       0.51      0.63      0.56      2999\n",
      "          22       0.44      0.43      0.43      2999\n",
      "          23       0.11      0.06      0.08      2999\n",
      "          24       0.28      0.45      0.35      2999\n",
      "          25       0.31      0.41      0.35      2999\n",
      "          26       0.31      0.19      0.24      2999\n",
      "          27       0.10      0.10      0.10      2999\n",
      "          28       0.07      0.08      0.07      2999\n",
      "          29       0.17      0.15      0.16      2999\n",
      "          30       0.20      0.21      0.20      2999\n",
      "          31       0.32      0.36      0.34      2999\n",
      "          32       0.05      0.04      0.05      2999\n",
      "          33       0.38      0.66      0.48      2999\n",
      "          34       0.01      0.02      0.02      2999\n",
      "          35       0.18      0.23      0.21      2999\n",
      "          36       0.19      0.10      0.13      2999\n",
      "          37       0.10      0.15      0.12      2999\n",
      "          38       0.08      0.07      0.07      2999\n",
      "          39       0.00      0.00      0.00      2999\n",
      "          40       0.03      0.02      0.02      2999\n",
      "          41       0.29      0.17      0.21      2999\n",
      "          42       0.03      0.02      0.02      2999\n",
      "          43       0.02      0.02      0.02      2999\n",
      "          44       0.29      0.38      0.33      2999\n",
      "          45       0.19      0.19      0.19      2999\n",
      "          46       0.10      0.07      0.08      2999\n",
      "          47       0.08      0.04      0.05      2999\n",
      "          48       0.11      0.11      0.11      2999\n",
      "          49       0.13      0.16      0.14      2999\n",
      "          50       0.15      0.10      0.12      2999\n",
      "          51       0.19      0.09      0.13      2999\n",
      "          52       0.14      0.08      0.10      2999\n",
      "          53       0.09      0.12      0.10      2999\n",
      "          54       0.14      0.13      0.13      2999\n",
      "          55       0.51      0.43      0.47      2999\n",
      "          56       0.10      0.08      0.09      2999\n",
      "          57       0.17      0.21      0.19      2999\n",
      "          58       0.16      0.17      0.17      2999\n",
      "          59       0.01      0.01      0.01      2999\n",
      "          60       0.15      0.16      0.16      2999\n",
      "          61       0.18      0.19      0.18      2999\n",
      "          62       0.12      0.08      0.10      2999\n",
      "          63       0.15      0.28      0.20      2999\n",
      "          64       0.22      0.41      0.29      2999\n",
      "          65       0.03      0.02      0.02      2999\n",
      "          66       0.22      0.23      0.22      2999\n",
      "          67       0.48      0.76      0.59      2999\n",
      "          68       0.06      0.06      0.06      2999\n",
      "          69       0.19      0.21      0.20      2999\n",
      "          70       0.21      0.18      0.19      2999\n",
      "          71       0.05      0.04      0.05      2999\n",
      "          72       0.10      0.07      0.08      2999\n",
      "          73       0.18      0.19      0.18      2999\n",
      "          74       0.11      0.08      0.09      2999\n",
      "          75       0.11      0.13      0.12      2999\n",
      "          76       0.19      0.25      0.21      2999\n",
      "          77       0.00      0.00      0.00      2999\n",
      "          78       0.10      0.12      0.11      2999\n",
      "          79       0.01      0.01      0.01      2999\n",
      "          80       0.08      0.03      0.04      2999\n",
      "          81       0.37      0.59      0.45      2999\n",
      "          82       0.16      0.13      0.14      2999\n",
      "          83       0.08      0.06      0.07      2999\n",
      "          84       0.08      0.12      0.09      2999\n",
      "          85       0.20      0.21      0.20      2999\n",
      "          86       0.18      0.18      0.18      2999\n",
      "          87       0.10      0.09      0.10      2999\n",
      "          88       0.07      0.04      0.05      2999\n",
      "          89       0.57      0.73      0.64      2999\n",
      "          90       0.13      0.20      0.16      2999\n",
      "          91       0.40      0.65      0.50      2999\n",
      "          92       0.21      0.11      0.14      2999\n",
      "          93       0.11      0.08      0.09      2999\n",
      "          94       0.91      0.99      0.95      2999\n",
      "          95       0.02      0.02      0.02      2999\n",
      "          96       0.38      0.42      0.40      2999\n",
      "          97       0.26      0.38      0.31      2999\n",
      "          98       0.04      0.02      0.03      2999\n",
      "          99       0.09      0.06      0.07      2999\n",
      "         100       0.17      0.28      0.21      2999\n",
      "         101       0.02      0.01      0.01      2999\n",
      "         102       0.11      0.04      0.05      2999\n",
      "         103       0.06      0.08      0.07      2999\n",
      "         104       0.15      0.15      0.15      2999\n",
      "         105       0.17      0.22      0.19      2999\n",
      "         106       0.01      0.01      0.01      2999\n",
      "         107       0.05      0.05      0.05      2999\n",
      "         108       0.16      0.16      0.16      2999\n",
      "         109       0.50      0.60      0.54      2999\n",
      "\n",
      "    accuracy                           0.20    326891\n",
      "   macro avg       0.18      0.20      0.18    326891\n",
      "weighted avg       0.18      0.20      0.18    326891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=lr_model.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ffcfb",
   "metadata": {},
   "source": [
    "## 90-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "183124c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,ytr,yte=read_data(train[i],0.9) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "\n",
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "\n",
    "ytrain=[]\n",
    "for i in range(1,len(ytemp1)+1):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(1,len(ytemp2)+1):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "\n",
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "\n",
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "\n",
    "lr_model=LogisticRegression()\n",
    "lr_model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ccc7ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.06      0.02      0.03      1499\n",
      "           2       0.17      0.10      0.13      1499\n",
      "           3       0.13      0.20      0.16      1499\n",
      "           4       0.70      0.83      0.76      1499\n",
      "           5       0.07      0.06      0.07      1499\n",
      "           6       0.28      0.42      0.34      1499\n",
      "           7       0.38      0.67      0.49      1499\n",
      "           8       0.27      0.11      0.16      1499\n",
      "           9       0.24      0.24      0.24      1499\n",
      "          10       0.02      0.01      0.02      1499\n",
      "          11       0.05      0.03      0.04      1499\n",
      "          12       0.25      0.23      0.24      1499\n",
      "          13       0.08      0.09      0.08      1499\n",
      "          14       0.19      0.21      0.20      1499\n",
      "          15       0.07      0.04      0.05      1499\n",
      "          16       0.09      0.06      0.07      1499\n",
      "          17       0.00      0.00      0.00      1499\n",
      "          18       0.49      0.59      0.53      1499\n",
      "          19       0.14      0.13      0.14      1499\n",
      "          20       0.22      0.19      0.20      1499\n",
      "          21       0.51      0.65      0.57      1499\n",
      "          22       0.50      0.50      0.50      1499\n",
      "          23       0.10      0.05      0.07      1499\n",
      "          24       0.24      0.43      0.31      1499\n",
      "          25       0.27      0.36      0.31      1499\n",
      "          26       0.23      0.17      0.19      1499\n",
      "          27       0.35      0.29      0.32      1499\n",
      "          28       0.05      0.05      0.05      1499\n",
      "          29       0.17      0.14      0.15      1499\n",
      "          30       0.33      0.28      0.31      1499\n",
      "          31       0.34      0.46      0.39      1499\n",
      "          32       0.02      0.02      0.02      1499\n",
      "          33       0.42      0.75      0.54      1499\n",
      "          34       0.01      0.03      0.02      1499\n",
      "          35       0.22      0.25      0.23      1499\n",
      "          36       0.16      0.08      0.11      1499\n",
      "          37       0.02      0.03      0.03      1499\n",
      "          38       0.08      0.07      0.07      1499\n",
      "          39       0.00      0.00      0.00      1499\n",
      "          40       0.01      0.01      0.01      1499\n",
      "          41       0.17      0.23      0.20      1499\n",
      "          42       0.02      0.01      0.01      1499\n",
      "          43       0.05      0.04      0.04      1499\n",
      "          44       0.38      0.30      0.34      1499\n",
      "          45       0.17      0.16      0.17      1499\n",
      "          46       0.17      0.12      0.14      1499\n",
      "          47       0.09      0.05      0.06      1499\n",
      "          48       0.17      0.16      0.16      1499\n",
      "          49       0.11      0.14      0.12      1499\n",
      "          50       0.15      0.09      0.11      1499\n",
      "          51       0.33      0.28      0.30      1499\n",
      "          52       0.04      0.02      0.03      1499\n",
      "          53       0.06      0.07      0.07      1499\n",
      "          54       0.08      0.07      0.08      1499\n",
      "          55       0.51      0.44      0.47      1499\n",
      "          56       0.09      0.07      0.08      1499\n",
      "          57       0.13      0.14      0.13      1499\n",
      "          58       0.22      0.24      0.23      1499\n",
      "          59       0.00      0.00      0.00      1499\n",
      "          60       0.23      0.27      0.25      1499\n",
      "          61       0.14      0.16      0.15      1499\n",
      "          62       0.47      0.55      0.51      1499\n",
      "          63       0.14      0.26      0.18      1499\n",
      "          64       0.28      0.47      0.35      1499\n",
      "          65       0.02      0.01      0.01      1499\n",
      "          66       0.29      0.30      0.30      1499\n",
      "          67       0.40      0.79      0.53      1499\n",
      "          68       0.16      0.18      0.17      1499\n",
      "          69       0.26      0.27      0.26      1499\n",
      "          70       0.24      0.21      0.23      1499\n",
      "          71       0.00      0.00      0.00      1499\n",
      "          72       0.06      0.02      0.03      1499\n",
      "          73       0.16      0.23      0.19      1499\n",
      "          74       0.10      0.10      0.10      1499\n",
      "          75       0.10      0.12      0.11      1499\n",
      "          76       0.20      0.30      0.24      1499\n",
      "          77       0.00      0.00      0.00      1499\n",
      "          78       0.15      0.15      0.15      1499\n",
      "          79       0.01      0.01      0.01      1499\n",
      "          80       0.14      0.04      0.06      1499\n",
      "          81       0.37      0.58      0.45      1499\n",
      "          82       0.13      0.08      0.10      1499\n",
      "          83       0.06      0.05      0.05      1499\n",
      "          84       0.00      0.00      0.00      1499\n",
      "          85       0.20      0.22      0.21      1499\n",
      "          86       0.13      0.15      0.14      1499\n",
      "          87       0.13      0.12      0.12      1499\n",
      "          88       0.08      0.04      0.05      1499\n",
      "          89       0.60      0.61      0.60      1499\n",
      "          90       0.12      0.19      0.15      1499\n",
      "          91       0.36      0.68      0.48      1499\n",
      "          92       0.14      0.07      0.09      1499\n",
      "          93       0.11      0.05      0.07      1499\n",
      "          94       0.96      0.98      0.97      1499\n",
      "          95       0.00      0.00      0.00      1499\n",
      "          96       0.32      0.42      0.36      1499\n",
      "          97       0.16      0.22      0.18      1499\n",
      "          98       0.05      0.02      0.03      1499\n",
      "          99       0.08      0.06      0.07      1499\n",
      "         100       0.18      0.27      0.22      1499\n",
      "         101       0.00      0.00      0.00      1499\n",
      "         102       0.21      0.06      0.09      1499\n",
      "         103       0.05      0.04      0.05      1499\n",
      "         104       0.11      0.10      0.10      1499\n",
      "         105       0.10      0.10      0.10      1499\n",
      "         106       0.00      0.00      0.00      1499\n",
      "         107       0.31      0.27      0.29      1499\n",
      "         108       0.16      0.19      0.18      1499\n",
      "         109       0.50      0.55      0.52      1499\n",
      "\n",
      "    accuracy                           0.20    163391\n",
      "   macro avg       0.18      0.20      0.19    163391\n",
      "weighted avg       0.18      0.20      0.19    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=lr_model.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

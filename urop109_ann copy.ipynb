{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 109 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 14:39:26.713347: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-04 14:39:26.740469: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-04 14:39:26.740500: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-04 14:39:26.740520: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-04 14:39:26.745637: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-04 14:39:26.746524: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-04 14:39:27.689178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081a3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /usr/local/python/3.10.8/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.3 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (1.11.3)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (3.8.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.8/lib/python3.10/site-packages (from mne) (4.66.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /usr/local/python/3.10.8/lib/python3.10/site-packages (from mne) (1.7.0)\n",
      "Requirement already satisfied: decorator in /home/codespace/.local/lib/python3.10/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from mne) (23.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (4.43.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from pooch>=1.5->mne) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from pooch>=1.5->mne) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->mne) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files2/S001R04.edf',\n",
       " 'files2/S002R04.edf',\n",
       " 'files2/S003R04.edf',\n",
       " 'files2/S004R04.edf',\n",
       " 'files2/S005R04.edf',\n",
       " 'files2/S006R04.edf',\n",
       " 'files2/S007R04.edf',\n",
       " 'files2/S008R04.edf',\n",
       " 'files2/S009R04.edf',\n",
       " 'files2/S010R04.edf',\n",
       " 'files2/S011R04.edf',\n",
       " 'files2/S012R04.edf',\n",
       " 'files2/S013R04.edf',\n",
       " 'files2/S014R04.edf',\n",
       " 'files2/S015R04.edf',\n",
       " 'files2/S016R04.edf',\n",
       " 'files2/S017R04.edf',\n",
       " 'files2/S018R04.edf',\n",
       " 'files2/S019R04.edf',\n",
       " 'files2/S020R04.edf',\n",
       " 'files2/S021R04.edf',\n",
       " 'files2/S022R04.edf',\n",
       " 'files2/S023R04.edf',\n",
       " 'files2/S024R04.edf',\n",
       " 'files2/S025R04.edf',\n",
       " 'files2/S026R04.edf',\n",
       " 'files2/S027R04.edf',\n",
       " 'files2/S028R04.edf',\n",
       " 'files2/S029R04.edf',\n",
       " 'files2/S030R04.edf',\n",
       " 'files2/S031R04.edf',\n",
       " 'files2/S032R04.edf',\n",
       " 'files2/S033R04.edf',\n",
       " 'files2/S034R04.edf',\n",
       " 'files2/S035R04.edf',\n",
       " 'files2/S036R04.edf',\n",
       " 'files2/S037R04.edf',\n",
       " 'files2/S038R04.edf',\n",
       " 'files2/S039R04.edf',\n",
       " 'files2/S040R04.edf',\n",
       " 'files2/S041R04.edf',\n",
       " 'files2/S042R04.edf',\n",
       " 'files2/S043R04.edf',\n",
       " 'files2/S044R04.edf',\n",
       " 'files2/S045R04.edf',\n",
       " 'files2/S046R04.edf',\n",
       " 'files2/S047R04.edf',\n",
       " 'files2/S048R04.edf',\n",
       " 'files2/S049R04.edf',\n",
       " 'files2/S050R04.edf',\n",
       " 'files2/S051R04.edf',\n",
       " 'files2/S052R04.edf',\n",
       " 'files2/S053R04.edf',\n",
       " 'files2/S054R04.edf',\n",
       " 'files2/S055R04.edf',\n",
       " 'files2/S056R04.edf',\n",
       " 'files2/S057R04.edf',\n",
       " 'files2/S058R04.edf',\n",
       " 'files2/S059R04.edf',\n",
       " 'files2/S060R04.edf',\n",
       " 'files2/S061R04.edf',\n",
       " 'files2/S062R04.edf',\n",
       " 'files2/S063R04.edf',\n",
       " 'files2/S064R04.edf',\n",
       " 'files2/S065R04.edf',\n",
       " 'files2/S066R04.edf',\n",
       " 'files2/S067R04.edf',\n",
       " 'files2/S068R04.edf',\n",
       " 'files2/S069R04.edf',\n",
       " 'files2/S070R04.edf',\n",
       " 'files2/S071R04.edf',\n",
       " 'files2/S072R04.edf',\n",
       " 'files2/S073R04.edf',\n",
       " 'files2/S074R04.edf',\n",
       " 'files2/S075R04.edf',\n",
       " 'files2/S076R04.edf',\n",
       " 'files2/S077R04.edf',\n",
       " 'files2/S078R04.edf',\n",
       " 'files2/S079R04.edf',\n",
       " 'files2/S080R04.edf',\n",
       " 'files2/S081R04.edf',\n",
       " 'files2/S082R04.edf',\n",
       " 'files2/S083R04.edf',\n",
       " 'files2/S084R04.edf',\n",
       " 'files2/S085R04.edf',\n",
       " 'files2/S086R04.edf',\n",
       " 'files2/S087R04.edf',\n",
       " 'files2/S088R04.edf',\n",
       " 'files2/S089R04.edf',\n",
       " 'files2/S090R04.edf',\n",
       " 'files2/S091R04.edf',\n",
       " 'files2/S092R04.edf',\n",
       " 'files2/S093R04.edf',\n",
       " 'files2/S094R04.edf',\n",
       " 'files2/S095R04.edf',\n",
       " 'files2/S096R04.edf',\n",
       " 'files2/S097R04.edf',\n",
       " 'files2/S098R04.edf',\n",
       " 'files2/S099R04.edf',\n",
       " 'files2/S100R04.edf',\n",
       " 'files2/S101R04.edf',\n",
       " 'files2/S102R04.edf',\n",
       " 'files2/S103R04.edf',\n",
       " 'files2/S104R04.edf',\n",
       " 'files2/S105R04.edf',\n",
       " 'files2/S106R04.edf',\n",
       " 'files2/S107R04.edf',\n",
       " 'files2/S108R04.edf',\n",
       " 'files2/S109R04.edf']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files2/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1308000, 163391, 1308000, 163391)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0e-05  1.5e-05  6.0e-06 ...  4.0e-06  1.2e-05 -3.0e-06]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307995</th>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307996</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307997</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307998</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307999</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        -0.000005   0.000002   0.000037   0.000039   0.000030   0.000026   \n",
       "1        -0.000012  -0.000024   0.000001  -0.000002  -0.000015  -0.000022   \n",
       "2        -0.000077  -0.000078  -0.000059  -0.000065  -0.000063  -0.000055   \n",
       "3        -0.000066  -0.000067  -0.000050  -0.000065  -0.000060  -0.000055   \n",
       "4        -0.000045  -0.000055  -0.000033  -0.000053  -0.000054  -0.000063   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "1307995   0.000077   0.000051   0.000020  -0.000052  -0.000064   0.000143   \n",
       "1307996   0.000071   0.000063   0.000092  -0.000048  -0.000063   0.000181   \n",
       "1307997   0.000062   0.000065   0.000119  -0.000045  -0.000058   0.000166   \n",
       "1307998   0.000052   0.000061   0.000111  -0.000049  -0.000056   0.000152   \n",
       "1307999   0.000044   0.000081   0.000118  -0.000035  -0.000041   0.000179   \n",
       "\n",
       "         Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        -0.000016  -0.000014   0.000004   0.000018  ...   -0.000021   \n",
       "1        -0.000055  -0.000036  -0.000027  -0.000025  ...   -0.000050   \n",
       "2        -0.000067  -0.000088  -0.000071  -0.000065  ...   -0.000017   \n",
       "3        -0.000068  -0.000062  -0.000053  -0.000054  ...   -0.000039   \n",
       "4        -0.000083  -0.000052  -0.000050  -0.000053  ...   -0.000044   \n",
       "...            ...        ...        ...        ...  ...         ...   \n",
       "1307995  -0.000131   0.000119  -0.000013  -0.000012  ...    0.000050   \n",
       "1307996  -0.000130   0.000136  -0.000006   0.000016  ...    0.000052   \n",
       "1307997  -0.000122   0.000145  -0.000007   0.000020  ...    0.000042   \n",
       "1307998  -0.000108   0.000123  -0.000017   0.000011  ...    0.000038   \n",
       "1307999  -0.000093   0.000122  -0.000010   0.000032  ...    0.000048   \n",
       "\n",
       "         Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         -0.000008   -0.000035   -0.000045   -0.000066   -0.000039   \n",
       "1         -0.000040   -0.000068   -0.000065   -0.000084   -0.000052   \n",
       "2         -0.000022   -0.000050   -0.000035   -0.000048   -0.000018   \n",
       "3         -0.000060   -0.000078   -0.000064   -0.000068   -0.000041   \n",
       "4         -0.000055   -0.000070   -0.000054   -0.000063   -0.000037   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1307995   -0.000005   -0.000011   -0.000044    0.000001    0.000042   \n",
       "1307996    0.000007    0.000006   -0.000023    0.000016    0.000049   \n",
       "1307997   -0.000002    0.000001   -0.000022    0.000010    0.000039   \n",
       "1307998   -0.000011   -0.000006   -0.000029    0.000004    0.000036   \n",
       "1307999    0.000002    0.000004   -0.000027    0.000011    0.000049   \n",
       "\n",
       "         Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         -0.000033   -0.000048   -0.000039   -0.000039  \n",
       "1         -0.000021   -0.000042   -0.000031   -0.000034  \n",
       "2         -0.000020   -0.000042   -0.000029   -0.000027  \n",
       "3         -0.000044   -0.000062   -0.000034   -0.000043  \n",
       "4         -0.000060   -0.000070   -0.000034   -0.000045  \n",
       "...             ...         ...         ...         ...  \n",
       "1307995    0.000003    0.000048   -0.000040   -0.000022  \n",
       "1307996    0.000020    0.000069   -0.000026   -0.000004  \n",
       "1307997    0.000013    0.000067   -0.000029   -0.000012  \n",
       "1307998    0.000008    0.000057   -0.000035   -0.000014  \n",
       "1307999    0.000013    0.000057   -0.000039   -0.000005  \n",
       "\n",
       "[1308000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28746/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_28746/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_28746/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307995</th>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307996</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307997</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307998</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307999</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        -0.000005   0.000002   0.000037   0.000039   0.000030   0.000026   \n",
       "1        -0.000012  -0.000024   0.000001  -0.000002  -0.000015  -0.000022   \n",
       "2        -0.000077  -0.000078  -0.000059  -0.000065  -0.000063  -0.000055   \n",
       "3        -0.000066  -0.000067  -0.000050  -0.000065  -0.000060  -0.000055   \n",
       "4        -0.000045  -0.000055  -0.000033  -0.000053  -0.000054  -0.000063   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "1307995   0.000077   0.000051   0.000020  -0.000052  -0.000064   0.000143   \n",
       "1307996   0.000071   0.000063   0.000092  -0.000048  -0.000063   0.000181   \n",
       "1307997   0.000062   0.000065   0.000119  -0.000045  -0.000058   0.000166   \n",
       "1307998   0.000052   0.000061   0.000111  -0.000049  -0.000056   0.000152   \n",
       "1307999   0.000044   0.000081   0.000118  -0.000035  -0.000041   0.000179   \n",
       "\n",
       "         Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        -0.000016  -0.000014   0.000004   0.000018  ...   -0.000021   \n",
       "1        -0.000055  -0.000036  -0.000027  -0.000025  ...   -0.000050   \n",
       "2        -0.000067  -0.000088  -0.000071  -0.000065  ...   -0.000017   \n",
       "3        -0.000068  -0.000062  -0.000053  -0.000054  ...   -0.000039   \n",
       "4        -0.000083  -0.000052  -0.000050  -0.000053  ...   -0.000044   \n",
       "...            ...        ...        ...        ...  ...         ...   \n",
       "1307995  -0.000131   0.000119  -0.000013  -0.000012  ...    0.000050   \n",
       "1307996  -0.000130   0.000136  -0.000006   0.000016  ...    0.000052   \n",
       "1307997  -0.000122   0.000145  -0.000007   0.000020  ...    0.000042   \n",
       "1307998  -0.000108   0.000123  -0.000017   0.000011  ...    0.000038   \n",
       "1307999  -0.000093   0.000122  -0.000010   0.000032  ...    0.000048   \n",
       "\n",
       "         Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         -0.000008   -0.000035   -0.000045   -0.000066   -0.000039   \n",
       "1         -0.000040   -0.000068   -0.000065   -0.000084   -0.000052   \n",
       "2         -0.000022   -0.000050   -0.000035   -0.000048   -0.000018   \n",
       "3         -0.000060   -0.000078   -0.000064   -0.000068   -0.000041   \n",
       "4         -0.000055   -0.000070   -0.000054   -0.000063   -0.000037   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1307995   -0.000005   -0.000011   -0.000044    0.000001    0.000042   \n",
       "1307996    0.000007    0.000006   -0.000023    0.000016    0.000049   \n",
       "1307997   -0.000002    0.000001   -0.000022    0.000010    0.000039   \n",
       "1307998   -0.000011   -0.000006   -0.000029    0.000004    0.000036   \n",
       "1307999    0.000002    0.000004   -0.000027    0.000011    0.000049   \n",
       "\n",
       "         Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         -0.000033   -0.000048   -0.000039   -0.000039  \n",
       "1         -0.000021   -0.000042   -0.000031   -0.000034  \n",
       "2         -0.000020   -0.000042   -0.000029   -0.000027  \n",
       "3         -0.000044   -0.000062   -0.000034   -0.000043  \n",
       "4         -0.000060   -0.000070   -0.000034   -0.000045  \n",
       "...             ...         ...         ...         ...  \n",
       "1307995    0.000003    0.000048   -0.000040   -0.000022  \n",
       "1307996    0.000020    0.000069   -0.000026   -0.000004  \n",
       "1307997    0.000013    0.000067   -0.000029   -0.000012  \n",
       "1307998    0.000008    0.000057   -0.000035   -0.000014  \n",
       "1307999    0.000013    0.000057   -0.000039   -0.000005  \n",
       "\n",
       "[1308000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b271d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(109, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40875/40875 [==============================] - 56s 1ms/step - loss: 2.9747 - val_loss: 3.2391\n",
      "Epoch 2/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 2.5099 - val_loss: 3.3174\n",
      "Epoch 3/10\n",
      "40875/40875 [==============================] - 55s 1ms/step - loss: 2.4206 - val_loss: 3.2931\n",
      "Epoch 4/10\n",
      "40875/40875 [==============================] - 60s 1ms/step - loss: 2.3748 - val_loss: 3.3589\n",
      "Epoch 5/10\n",
      "40875/40875 [==============================] - 61s 1ms/step - loss: 2.3458 - val_loss: 3.3275\n",
      "Epoch 6/10\n",
      "40875/40875 [==============================] - 56s 1ms/step - loss: 2.3254 - val_loss: 3.3775\n",
      "Epoch 7/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 2.3097 - val_loss: 3.4692\n",
      "Epoch 8/10\n",
      "40875/40875 [==============================] - 55s 1ms/step - loss: 2.2977 - val_loss: 3.2967\n",
      "Epoch 9/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 2.2875 - val_loss: 3.3769\n",
      "Epoch 10/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 2.2785 - val_loss: 3.3882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb401c68b20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106/5106 [==============================] - 4s 824us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.13      0.13      1499\n",
      "           1       0.20      0.20      0.20      1499\n",
      "           2       0.17      0.20      0.19      1499\n",
      "           3       0.34      0.38      0.36      1499\n",
      "           4       0.67      0.64      0.66      1499\n",
      "           5       0.07      0.04      0.05      1499\n",
      "           6       0.17      0.47      0.25      1499\n",
      "           7       0.28      0.28      0.28      1499\n",
      "           8       0.36      0.19      0.25      1499\n",
      "           9       0.29      0.42      0.35      1499\n",
      "          10       0.13      0.12      0.12      1499\n",
      "          11       0.07      0.08      0.07      1499\n",
      "          12       0.06      0.00      0.01      1499\n",
      "          13       0.10      0.08      0.09      1499\n",
      "          14       0.12      0.03      0.04      1499\n",
      "          15       0.24      0.18      0.21      1499\n",
      "          16       0.16      0.07      0.10      1499\n",
      "          17       0.42      0.54      0.48      1499\n",
      "          18       0.11      0.02      0.04      1499\n",
      "          19       0.12      0.08      0.09      1499\n",
      "          20       0.31      0.76      0.44      1499\n",
      "          21       0.15      0.28      0.19      1499\n",
      "          22       0.14      0.11      0.12      1499\n",
      "          23       0.31      0.52      0.39      1499\n",
      "          24       0.17      0.22      0.19      1499\n",
      "          25       0.32      0.18      0.23      1499\n",
      "          26       0.16      0.07      0.09      1499\n",
      "          27       0.08      0.07      0.07      1499\n",
      "          28       0.16      0.10      0.12      1499\n",
      "          29       0.26      0.26      0.26      1499\n",
      "          30       0.13      0.21      0.16      1499\n",
      "          31       0.11      0.34      0.17      1499\n",
      "          32       0.09      0.08      0.09      1499\n",
      "          33       0.08      0.03      0.04      1499\n",
      "          34       0.23      0.25      0.24      1499\n",
      "          35       0.33      0.59      0.43      1499\n",
      "          36       0.33      0.40      0.36      1499\n",
      "          37       0.12      0.08      0.10      1499\n",
      "          38       0.04      0.06      0.05      1499\n",
      "          39       0.18      0.23      0.20      1499\n",
      "          40       0.25      0.37      0.30      1499\n",
      "          41       0.34      0.24      0.28      1499\n",
      "          42       0.15      0.15      0.15      1499\n",
      "          43       0.18      0.13      0.15      1499\n",
      "          44       0.25      0.43      0.32      1499\n",
      "          45       0.11      0.04      0.06      1499\n",
      "          46       0.25      0.45      0.32      1499\n",
      "          47       0.42      0.23      0.30      1499\n",
      "          48       0.34      0.60      0.43      1499\n",
      "          49       0.17      0.09      0.12      1499\n",
      "          50       0.29      0.33      0.31      1499\n",
      "          51       0.18      0.06      0.09      1499\n",
      "          52       0.10      0.06      0.07      1499\n",
      "          53       0.12      0.09      0.10      1499\n",
      "          54       0.18      0.13      0.15      1499\n",
      "          55       0.17      0.30      0.22      1499\n",
      "          56       0.08      0.07      0.07      1499\n",
      "          57       0.33      0.31      0.32      1499\n",
      "          58       0.81      0.62      0.70      1499\n",
      "          59       0.04      0.03      0.03      1499\n",
      "          60       0.25      0.20      0.22      1499\n",
      "          61       0.15      0.11      0.13      1499\n",
      "          62       0.48      0.43      0.46      1499\n",
      "          63       0.20      0.09      0.12      1499\n",
      "          64       0.10      0.06      0.07      1499\n",
      "          65       0.10      0.09      0.10      1499\n",
      "          66       0.46      0.50      0.48      1499\n",
      "          67       0.05      0.03      0.04      1499\n",
      "          68       0.27      0.71      0.40      1499\n",
      "          69       0.06      0.06      0.06      1499\n",
      "          70       0.14      0.15      0.15      1499\n",
      "          71       0.12      0.06      0.08      1499\n",
      "          72       0.06      0.08      0.07      1499\n",
      "          73       0.27      0.31      0.28      1499\n",
      "          74       0.11      0.18      0.14      1499\n",
      "          75       0.22      0.22      0.22      1499\n",
      "          76       0.14      0.12      0.13      1499\n",
      "          77       0.11      0.10      0.10      1499\n",
      "          78       0.08      0.09      0.08      1499\n",
      "          79       0.17      0.18      0.18      1499\n",
      "          80       0.42      0.25      0.31      1499\n",
      "          81       0.13      0.06      0.08      1499\n",
      "          82       0.25      0.13      0.17      1499\n",
      "          83       0.09      0.12      0.10      1499\n",
      "          84       0.30      0.23      0.26      1499\n",
      "          85       0.31      0.43      0.36      1499\n",
      "          86       0.20      0.38      0.27      1499\n",
      "          87       0.06      0.02      0.03      1499\n",
      "          88       0.22      0.22      0.22      1499\n",
      "          89       0.19      0.12      0.14      1499\n",
      "          90       0.21      0.26      0.23      1499\n",
      "          91       0.16      0.15      0.15      1499\n",
      "          92       0.04      0.03      0.04      1499\n",
      "          93       0.61      0.31      0.41      1499\n",
      "          94       0.09      0.09      0.09      1499\n",
      "          95       0.12      0.05      0.07      1499\n",
      "          96       0.19      0.18      0.18      1499\n",
      "          97       0.16      0.13      0.14      1499\n",
      "          98       0.12      0.07      0.09      1499\n",
      "          99       0.36      0.41      0.39      1499\n",
      "         100       0.39      0.30      0.34      1499\n",
      "         101       0.14      0.10      0.12      1499\n",
      "         102       0.40      0.52      0.45      1499\n",
      "         103       0.19      0.14      0.16      1499\n",
      "         104       0.76      0.72      0.74      1499\n",
      "         105       0.05      0.05      0.05      1499\n",
      "         106       0.06      0.07      0.07      1499\n",
      "         107       0.12      0.10      0.11      1499\n",
      "         108       0.35      0.50      0.41      1499\n",
      "\n",
      "    accuracy                           0.22    163391\n",
      "   macro avg       0.21      0.22      0.20    163391\n",
      "weighted avg       0.21      0.22      0.20    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28746/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_28746/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_28746/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307995</th>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307996</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307997</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307998</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307999</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0        -0.000005   0.000002   0.000037   0.000039   0.000030   0.000026   \n",
       "1        -0.000012  -0.000024   0.000001  -0.000002  -0.000015  -0.000022   \n",
       "2        -0.000077  -0.000078  -0.000059  -0.000065  -0.000063  -0.000055   \n",
       "3        -0.000066  -0.000067  -0.000050  -0.000065  -0.000060  -0.000055   \n",
       "4        -0.000045  -0.000055  -0.000033  -0.000053  -0.000054  -0.000063   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "1307995   0.000077   0.000051   0.000020  -0.000052  -0.000064   0.000143   \n",
       "1307996   0.000071   0.000063   0.000092  -0.000048  -0.000063   0.000181   \n",
       "1307997   0.000062   0.000065   0.000119  -0.000045  -0.000058   0.000166   \n",
       "1307998   0.000052   0.000061   0.000111  -0.000049  -0.000056   0.000152   \n",
       "1307999   0.000044   0.000081   0.000118  -0.000035  -0.000041   0.000179   \n",
       "\n",
       "         Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        -0.000016  -0.000014   0.000004   0.000018  ...   -0.000021   \n",
       "1        -0.000055  -0.000036  -0.000027  -0.000025  ...   -0.000050   \n",
       "2        -0.000067  -0.000088  -0.000071  -0.000065  ...   -0.000017   \n",
       "3        -0.000068  -0.000062  -0.000053  -0.000054  ...   -0.000039   \n",
       "4        -0.000083  -0.000052  -0.000050  -0.000053  ...   -0.000044   \n",
       "...            ...        ...        ...        ...  ...         ...   \n",
       "1307995  -0.000131   0.000119  -0.000013  -0.000012  ...    0.000050   \n",
       "1307996  -0.000130   0.000136  -0.000006   0.000016  ...    0.000052   \n",
       "1307997  -0.000122   0.000145  -0.000007   0.000020  ...    0.000042   \n",
       "1307998  -0.000108   0.000123  -0.000017   0.000011  ...    0.000038   \n",
       "1307999  -0.000093   0.000122  -0.000010   0.000032  ...    0.000048   \n",
       "\n",
       "         Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         -0.000008   -0.000035   -0.000045   -0.000066   -0.000039   \n",
       "1         -0.000040   -0.000068   -0.000065   -0.000084   -0.000052   \n",
       "2         -0.000022   -0.000050   -0.000035   -0.000048   -0.000018   \n",
       "3         -0.000060   -0.000078   -0.000064   -0.000068   -0.000041   \n",
       "4         -0.000055   -0.000070   -0.000054   -0.000063   -0.000037   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1307995   -0.000005   -0.000011   -0.000044    0.000001    0.000042   \n",
       "1307996    0.000007    0.000006   -0.000023    0.000016    0.000049   \n",
       "1307997   -0.000002    0.000001   -0.000022    0.000010    0.000039   \n",
       "1307998   -0.000011   -0.000006   -0.000029    0.000004    0.000036   \n",
       "1307999    0.000002    0.000004   -0.000027    0.000011    0.000049   \n",
       "\n",
       "         Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         -0.000033   -0.000048   -0.000039   -0.000039  \n",
       "1         -0.000021   -0.000042   -0.000031   -0.000034  \n",
       "2         -0.000020   -0.000042   -0.000029   -0.000027  \n",
       "3         -0.000044   -0.000062   -0.000034   -0.000043  \n",
       "4         -0.000060   -0.000070   -0.000034   -0.000045  \n",
       "...             ...         ...         ...         ...  \n",
       "1307995    0.000003    0.000048   -0.000040   -0.000022  \n",
       "1307996    0.000020    0.000069   -0.000026   -0.000004  \n",
       "1307997    0.000013    0.000067   -0.000029   -0.000012  \n",
       "1307998    0.000008    0.000057   -0.000035   -0.000014  \n",
       "1307999    0.000013    0.000057   -0.000039   -0.000005  \n",
       "\n",
       "[1308000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7844baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(109, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7980b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32119/40875 [======================>.......] - ETA: 10s - loss: 2.3834"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40875/40875 [==============================] - 55s 1ms/step - loss: 2.2650 - val_loss: 2.9046\n",
      "Epoch 2/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 1.6664 - val_loss: 2.9964\n",
      "Epoch 3/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 1.5095 - val_loss: 2.9952\n",
      "Epoch 4/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 1.4317 - val_loss: 3.1108\n",
      "Epoch 5/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 1.3864 - val_loss: 3.0446\n",
      "Epoch 6/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 1.3539 - val_loss: 3.0394\n",
      "Epoch 7/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 1.3219 - val_loss: 3.1125\n",
      "Epoch 8/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 1.2889 - val_loss: 3.0289\n",
      "Epoch 9/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 1.2662 - val_loss: 3.1235\n",
      "Epoch 10/10\n",
      "40875/40875 [==============================] - 54s 1ms/step - loss: 1.2511 - val_loss: 3.0581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb38892d6c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106/5106 [==============================] - 4s 795us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.34      0.28      1499\n",
      "           1       0.27      0.31      0.29      1499\n",
      "           2       0.18      0.32      0.23      1499\n",
      "           3       0.47      0.26      0.34      1499\n",
      "           4       0.82      0.81      0.81      1499\n",
      "           5       0.30      0.44      0.35      1499\n",
      "           6       0.60      0.52      0.56      1499\n",
      "           7       0.76      0.36      0.49      1499\n",
      "           8       0.29      0.21      0.24      1499\n",
      "           9       0.37      0.53      0.44      1499\n",
      "          10       0.29      0.25      0.27      1499\n",
      "          11       0.28      0.36      0.32      1499\n",
      "          12       0.32      0.12      0.17      1499\n",
      "          13       0.18      0.34      0.24      1499\n",
      "          14       0.17      0.11      0.13      1499\n",
      "          15       0.56      0.36      0.44      1499\n",
      "          16       0.29      0.14      0.19      1499\n",
      "          17       0.74      0.37      0.49      1499\n",
      "          18       0.35      0.37      0.36      1499\n",
      "          19       0.20      0.29      0.24      1499\n",
      "          20       0.73      0.83      0.77      1499\n",
      "          21       0.20      0.34      0.25      1499\n",
      "          22       0.30      0.24      0.27      1499\n",
      "          23       0.57      0.76      0.65      1499\n",
      "          24       0.42      0.28      0.33      1499\n",
      "          25       0.68      0.52      0.59      1499\n",
      "          26       0.20      0.13      0.16      1499\n",
      "          27       0.23      0.24      0.23      1499\n",
      "          28       0.39      0.18      0.25      1499\n",
      "          29       0.67      0.37      0.48      1499\n",
      "          30       0.41      0.48      0.44      1499\n",
      "          31       0.25      0.50      0.34      1499\n",
      "          32       0.23      0.29      0.26      1499\n",
      "          33       0.20      0.20      0.20      1499\n",
      "          34       0.86      0.56      0.67      1499\n",
      "          35       0.54      0.75      0.63      1499\n",
      "          36       0.79      0.47      0.59      1499\n",
      "          37       0.31      0.16      0.21      1499\n",
      "          38       0.15      0.15      0.15      1499\n",
      "          39       0.20      0.26      0.23      1499\n",
      "          40       0.71      0.51      0.59      1499\n",
      "          41       0.75      0.58      0.65      1499\n",
      "          42       0.18      0.28      0.22      1499\n",
      "          43       0.21      0.21      0.21      1499\n",
      "          44       0.60      0.54      0.57      1499\n",
      "          45       0.19      0.18      0.19      1499\n",
      "          46       0.45      0.61      0.52      1499\n",
      "          47       0.45      0.33      0.38      1499\n",
      "          48       0.46      0.74      0.57      1499\n",
      "          49       0.38      0.23      0.29      1499\n",
      "          50       0.49      0.33      0.39      1499\n",
      "          51       0.12      0.11      0.12      1499\n",
      "          52       0.21      0.20      0.20      1499\n",
      "          53       0.24      0.18      0.21      1499\n",
      "          54       0.30      0.37      0.33      1499\n",
      "          55       0.32      0.44      0.37      1499\n",
      "          56       0.09      0.09      0.09      1499\n",
      "          57       0.51      0.38      0.44      1499\n",
      "          58       0.84      0.71      0.77      1499\n",
      "          59       0.27      0.17      0.21      1499\n",
      "          60       0.28      0.28      0.28      1499\n",
      "          61       0.30      0.33      0.31      1499\n",
      "          62       0.72      0.61      0.66      1499\n",
      "          63       0.36      0.24      0.29      1499\n",
      "          64       0.11      0.17      0.13      1499\n",
      "          65       0.42      0.25      0.31      1499\n",
      "          66       0.70      0.52      0.59      1499\n",
      "          67       0.11      0.13      0.12      1499\n",
      "          68       0.36      0.70      0.48      1499\n",
      "          69       0.09      0.15      0.11      1499\n",
      "          70       0.35      0.25      0.29      1499\n",
      "          71       0.25      0.11      0.15      1499\n",
      "          72       0.23      0.25      0.24      1499\n",
      "          73       0.42      0.50      0.46      1499\n",
      "          74       0.17      0.25      0.20      1499\n",
      "          75       0.92      0.76      0.83      1499\n",
      "          76       0.24      0.22      0.22      1499\n",
      "          77       0.19      0.25      0.21      1499\n",
      "          78       0.20      0.25      0.23      1499\n",
      "          79       0.40      0.36      0.38      1499\n",
      "          80       0.68      0.43      0.53      1499\n",
      "          81       0.31      0.22      0.26      1499\n",
      "          82       0.39      0.35      0.37      1499\n",
      "          83       0.18      0.24      0.20      1499\n",
      "          84       0.39      0.46      0.42      1499\n",
      "          85       0.27      0.21      0.24      1499\n",
      "          86       0.29      0.44      0.35      1499\n",
      "          87       0.16      0.07      0.10      1499\n",
      "          88       0.76      0.81      0.79      1499\n",
      "          89       0.29      0.22      0.25      1499\n",
      "          90       0.69      0.52      0.59      1499\n",
      "          91       0.35      0.34      0.34      1499\n",
      "          92       0.13      0.16      0.15      1499\n",
      "          93       0.87      0.74      0.80      1499\n",
      "          94       0.31      0.40      0.35      1499\n",
      "          95       0.31      0.25      0.28      1499\n",
      "          96       0.29      0.43      0.35      1499\n",
      "          97       0.18      0.21      0.19      1499\n",
      "          98       0.53      0.36      0.43      1499\n",
      "          99       0.52      0.59      0.55      1499\n",
      "         100       0.54      0.43      0.48      1499\n",
      "         101       0.18      0.16      0.17      1499\n",
      "         102       0.50      0.76      0.60      1499\n",
      "         103       0.18      0.03      0.05      1499\n",
      "         104       0.89      0.58      0.70      1499\n",
      "         105       0.08      0.07      0.07      1499\n",
      "         106       0.16      0.11      0.13      1499\n",
      "         107       0.24      0.32      0.27      1499\n",
      "         108       0.59      0.79      0.68      1499\n",
      "\n",
      "    accuracy                           0.36    163391\n",
      "   macro avg       0.38      0.36      0.36    163391\n",
      "weighted avg       0.38      0.36      0.36    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28746/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_28746/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_28746/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(109, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300aa492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1816/40875 [>.............................] - ETA: 1:08 - loss: 3.6163"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40875/40875 [==============================] - 73s 2ms/step - loss: 1.6391 - val_loss: 2.2969\n",
      "Epoch 2/10\n",
      "40875/40875 [==============================] - 72s 2ms/step - loss: 0.8422 - val_loss: 2.0763\n",
      "Epoch 3/10\n",
      "40875/40875 [==============================] - 71s 2ms/step - loss: 0.6961 - val_loss: 2.0895\n",
      "Epoch 4/10\n",
      "40875/40875 [==============================] - 77s 2ms/step - loss: 0.6233 - val_loss: 1.9373\n",
      "Epoch 5/10\n",
      "40875/40875 [==============================] - 72s 2ms/step - loss: 0.5804 - val_loss: 2.1325\n",
      "Epoch 6/10\n",
      "40875/40875 [==============================] - 72s 2ms/step - loss: 0.5495 - val_loss: 2.0038\n",
      "Epoch 7/10\n",
      "40875/40875 [==============================] - 71s 2ms/step - loss: 0.5240 - val_loss: 2.1145\n",
      "Epoch 8/10\n",
      "40875/40875 [==============================] - 71s 2ms/step - loss: 0.5012 - val_loss: 2.0442\n",
      "Epoch 9/10\n",
      "40875/40875 [==============================] - 78s 2ms/step - loss: 0.4831 - val_loss: 2.0025\n",
      "Epoch 10/10\n",
      "40875/40875 [==============================] - 74s 2ms/step - loss: 0.4677 - val_loss: 2.1240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2ecd957af0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106/5106 [==============================] - 6s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.80      0.63      1499\n",
      "           1       0.63      0.80      0.70      1499\n",
      "           2       0.47      0.75      0.58      1499\n",
      "           3       0.81      0.38      0.52      1499\n",
      "           4       0.95      0.91      0.93      1499\n",
      "           5       0.85      0.89      0.87      1499\n",
      "           6       0.86      0.83      0.84      1499\n",
      "           7       0.87      0.57      0.69      1499\n",
      "           8       0.49      0.74      0.59      1499\n",
      "           9       0.60      0.75      0.67      1499\n",
      "          10       0.64      0.63      0.64      1499\n",
      "          11       0.97      0.86      0.91      1499\n",
      "          12       0.63      0.64      0.63      1499\n",
      "          13       0.52      0.48      0.50      1499\n",
      "          14       0.56      0.54      0.55      1499\n",
      "          15       0.85      0.32      0.47      1499\n",
      "          16       0.70      0.41      0.52      1499\n",
      "          17       0.88      0.50      0.64      1499\n",
      "          18       0.63      0.75      0.68      1499\n",
      "          19       0.19      0.31      0.24      1499\n",
      "          20       0.88      0.91      0.89      1499\n",
      "          21       0.62      0.72      0.67      1499\n",
      "          22       0.72      0.75      0.73      1499\n",
      "          23       0.90      0.87      0.88      1499\n",
      "          24       0.53      0.40      0.45      1499\n",
      "          25       0.83      0.54      0.65      1499\n",
      "          26       0.42      0.16      0.23      1499\n",
      "          27       0.59      0.57      0.58      1499\n",
      "          28       0.74      0.66      0.70      1499\n",
      "          29       0.90      0.50      0.65      1499\n",
      "          30       0.79      0.75      0.77      1499\n",
      "          31       0.59      0.68      0.63      1499\n",
      "          32       0.88      0.77      0.82      1499\n",
      "          33       0.74      0.72      0.73      1499\n",
      "          34       0.98      0.91      0.94      1499\n",
      "          35       0.76      0.65      0.70      1499\n",
      "          36       0.95      0.87      0.91      1499\n",
      "          37       0.53      0.56      0.54      1499\n",
      "          38       0.31      0.38      0.34      1499\n",
      "          39       0.36      0.45      0.40      1499\n",
      "          40       0.99      0.37      0.53      1499\n",
      "          41       0.85      0.44      0.58      1499\n",
      "          42       0.32      0.31      0.32      1499\n",
      "          43       0.58      0.53      0.56      1499\n",
      "          44       0.89      0.71      0.79      1499\n",
      "          45       0.47      0.47      0.47      1499\n",
      "          46       0.78      0.53      0.63      1499\n",
      "          47       0.83      0.39      0.53      1499\n",
      "          48       0.74      0.84      0.79      1499\n",
      "          49       0.65      0.62      0.63      1499\n",
      "          50       0.54      0.65      0.59      1499\n",
      "          51       0.39      0.54      0.45      1499\n",
      "          52       0.63      0.78      0.70      1499\n",
      "          53       0.58      0.86      0.69      1499\n",
      "          54       0.77      0.52      0.62      1499\n",
      "          55       0.59      0.71      0.65      1499\n",
      "          56       0.38      0.38      0.38      1499\n",
      "          57       0.91      0.60      0.72      1499\n",
      "          58       0.95      0.66      0.78      1499\n",
      "          59       0.39      0.53      0.45      1499\n",
      "          60       0.77      0.70      0.74      1499\n",
      "          61       0.76      0.79      0.78      1499\n",
      "          62       0.95      0.76      0.84      1499\n",
      "          63       0.72      0.65      0.69      1499\n",
      "          64       0.37      0.25      0.30      1499\n",
      "          65       0.66      0.84      0.74      1499\n",
      "          66       0.71      0.91      0.80      1499\n",
      "          67       0.48      0.59      0.53      1499\n",
      "          68       0.88      0.82      0.85      1499\n",
      "          69       0.36      0.53      0.43      1499\n",
      "          70       0.70      0.30      0.42      1499\n",
      "          71       0.56      0.51      0.53      1499\n",
      "          72       0.24      0.29      0.26      1499\n",
      "          73       0.60      0.84      0.70      1499\n",
      "          74       0.21      0.75      0.33      1499\n",
      "          75       0.82      0.92      0.87      1499\n",
      "          76       0.41      0.64      0.49      1499\n",
      "          77       0.61      0.67      0.64      1499\n",
      "          78       0.42      0.58      0.48      1499\n",
      "          79       0.61      0.71      0.65      1499\n",
      "          80       0.80      0.73      0.76      1499\n",
      "          81       0.53      0.49      0.51      1499\n",
      "          82       0.82      0.60      0.69      1499\n",
      "          83       0.78      0.58      0.67      1499\n",
      "          84       0.48      0.67      0.56      1499\n",
      "          85       0.70      0.73      0.71      1499\n",
      "          86       0.52      0.60      0.55      1499\n",
      "          87       0.29      0.37      0.32      1499\n",
      "          88       0.86      0.77      0.81      1499\n",
      "          89       0.64      0.74      0.69      1499\n",
      "          90       0.83      0.79      0.81      1499\n",
      "          91       0.75      0.58      0.65      1499\n",
      "          92       0.43      0.31      0.36      1499\n",
      "          93       0.99      0.97      0.98      1499\n",
      "          94       0.46      0.56      0.51      1499\n",
      "          95       0.67      0.56      0.61      1499\n",
      "          96       0.80      0.76      0.78      1499\n",
      "          97       0.57      0.44      0.49      1499\n",
      "          98       0.67      0.59      0.63      1499\n",
      "          99       0.56      0.77      0.65      1499\n",
      "         100       0.84      0.58      0.69      1499\n",
      "         101       0.41      0.35      0.38      1499\n",
      "         102       0.69      0.54      0.61      1499\n",
      "         103       0.40      0.06      0.11      1499\n",
      "         104       0.93      0.56      0.70      1499\n",
      "         105       0.19      0.19      0.19      1499\n",
      "         106       0.55      0.48      0.52      1499\n",
      "         107       0.65      0.64      0.64      1499\n",
      "         108       0.64      0.89      0.74      1499\n",
      "\n",
      "    accuracy                           0.62    163391\n",
      "   macro avg       0.65      0.62      0.62    163391\n",
      "weighted avg       0.65      0.62      0.62    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dad4a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/ML_Project/urop109_ann copy.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bsturdy-eureka-74g545x6xx4hp9wp/workspaces/ML_Project/urop109_ann%20copy.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mset_seed(\u001b[39m1234\u001b[39m)  \u001b[39m# applied to achieve consistent results\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsturdy-eureka-74g545x6xx4hp9wp/workspaces/ML_Project/urop109_ann%20copy.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model3 \u001b[39m=\u001b[39m Sequential(\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsturdy-eureka-74g545x6xx4hp9wp/workspaces/ML_Project/urop109_ann%20copy.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     [\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsturdy-eureka-74g545x6xx4hp9wp/workspaces/ML_Project/urop109_ann%20copy.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         Dense(\u001b[39m64\u001b[39m, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m,   name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mL1\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsturdy-eureka-74g545x6xx4hp9wp/workspaces/ML_Project/urop109_ann%20copy.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsturdy-eureka-74g545x6xx4hp9wp/workspaces/ML_Project/urop109_ann%20copy.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(109, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361defb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  60/5106 [..............................] - ETA: 4s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106/5106 [==============================] - 5s 919us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.95      0.84      1499\n",
      "           1       0.89      0.89      0.89      1499\n",
      "           2       0.65      0.63      0.64      1499\n",
      "           3       0.93      0.97      0.95      1499\n",
      "           4       0.98      1.00      0.99      1499\n",
      "           5       0.95      0.97      0.96      1499\n",
      "           6       0.95      0.95      0.95      1499\n",
      "           7       0.97      0.60      0.74      1499\n",
      "           8       0.60      0.60      0.60      1499\n",
      "           9       0.47      0.84      0.60      1499\n",
      "          10       0.83      0.91      0.86      1499\n",
      "          11       0.96      0.91      0.93      1499\n",
      "          12       0.80      0.76      0.78      1499\n",
      "          13       0.83      0.81      0.82      1499\n",
      "          14       0.76      0.68      0.72      1499\n",
      "          15       0.98      0.41      0.58      1499\n",
      "          16       0.60      0.36      0.45      1499\n",
      "          17       0.76      0.72      0.74      1499\n",
      "          18       0.69      0.82      0.75      1499\n",
      "          19       0.78      0.57      0.66      1499\n",
      "          20       0.99      0.98      0.99      1499\n",
      "          21       0.88      0.76      0.82      1499\n",
      "          22       0.84      0.70      0.77      1499\n",
      "          23       0.95      0.97      0.96      1499\n",
      "          24       0.98      0.94      0.96      1499\n",
      "          25       0.64      0.27      0.38      1499\n",
      "          26       0.69      0.49      0.57      1499\n",
      "          27       0.85      0.76      0.80      1499\n",
      "          28       0.76      0.58      0.66      1499\n",
      "          29       0.69      0.98      0.81      1499\n",
      "          30       0.90      0.89      0.90      1499\n",
      "          31       0.78      0.86      0.82      1499\n",
      "          32       0.89      0.93      0.91      1499\n",
      "          33       0.68      0.72      0.70      1499\n",
      "          34       0.98      0.98      0.98      1499\n",
      "          35       0.85      0.87      0.86      1499\n",
      "          36       0.66      0.63      0.64      1499\n",
      "          37       0.81      0.75      0.78      1499\n",
      "          38       0.46      0.20      0.27      1499\n",
      "          39       0.57      0.63      0.60      1499\n",
      "          40       0.93      0.60      0.73      1499\n",
      "          41       0.81      0.33      0.47      1499\n",
      "          42       0.46      0.80      0.59      1499\n",
      "          43       0.77      0.74      0.75      1499\n",
      "          44       0.97      0.93      0.95      1499\n",
      "          45       0.82      0.70      0.75      1499\n",
      "          46       0.37      0.79      0.51      1499\n",
      "          47       0.61      0.65      0.63      1499\n",
      "          48       0.75      0.84      0.79      1499\n",
      "          49       0.67      0.77      0.72      1499\n",
      "          50       0.76      0.67      0.71      1499\n",
      "          51       0.46      0.77      0.58      1499\n",
      "          52       0.77      0.89      0.82      1499\n",
      "          53       0.91      0.79      0.85      1499\n",
      "          54       0.87      0.44      0.58      1499\n",
      "          55       0.76      0.86      0.81      1499\n",
      "          56       0.85      0.77      0.81      1499\n",
      "          57       0.93      0.90      0.91      1499\n",
      "          58       0.87      0.43      0.57      1499\n",
      "          59       0.49      0.49      0.49      1499\n",
      "          60       0.86      0.78      0.82      1499\n",
      "          61       0.60      0.15      0.24      1499\n",
      "          62       0.93      0.97      0.95      1499\n",
      "          63       0.51      0.85      0.64      1499\n",
      "          64       0.64      0.73      0.68      1499\n",
      "          65       0.88      0.93      0.90      1499\n",
      "          66       0.94      0.90      0.92      1499\n",
      "          67       0.40      0.32      0.36      1499\n",
      "          68       0.94      0.88      0.91      1499\n",
      "          69       0.50      0.79      0.61      1499\n",
      "          70       0.88      0.34      0.49      1499\n",
      "          71       0.41      0.35      0.38      1499\n",
      "          72       0.69      0.74      0.71      1499\n",
      "          73       0.84      0.65      0.73      1499\n",
      "          74       0.31      0.91      0.46      1499\n",
      "          75       0.82      0.73      0.78      1499\n",
      "          76       0.65      0.86      0.74      1499\n",
      "          77       0.86      0.79      0.82      1499\n",
      "          78       0.80      0.71      0.75      1499\n",
      "          79       0.86      0.87      0.87      1499\n",
      "          80       0.85      0.77      0.81      1499\n",
      "          81       0.91      0.66      0.76      1499\n",
      "          82       0.93      0.96      0.94      1499\n",
      "          83       0.57      0.67      0.62      1499\n",
      "          84       0.72      0.92      0.81      1499\n",
      "          85       0.72      0.70      0.71      1499\n",
      "          86       0.58      0.87      0.70      1499\n",
      "          87       0.68      0.63      0.65      1499\n",
      "          88       0.95      0.94      0.95      1499\n",
      "          89       0.82      0.90      0.86      1499\n",
      "          90       0.92      0.97      0.95      1499\n",
      "          91       0.78      0.86      0.82      1499\n",
      "          92       0.83      0.45      0.58      1499\n",
      "          93       1.00      0.99      0.99      1499\n",
      "          94       0.75      0.84      0.79      1499\n",
      "          95       0.89      0.71      0.79      1499\n",
      "          96       0.87      0.91      0.89      1499\n",
      "          97       0.71      0.86      0.78      1499\n",
      "          98       0.87      0.88      0.88      1499\n",
      "          99       0.72      0.87      0.79      1499\n",
      "         100       0.72      0.76      0.74      1499\n",
      "         101       0.72      0.59      0.65      1499\n",
      "         102       0.68      0.79      0.73      1499\n",
      "         103       0.94      0.70      0.80      1499\n",
      "         104       0.85      0.64      0.73      1499\n",
      "         105       0.82      0.75      0.78      1499\n",
      "         106       0.63      0.55      0.59      1499\n",
      "         107       0.86      0.80      0.82      1499\n",
      "         108       0.91      0.92      0.91      1499\n",
      "\n",
      "    accuracy                           0.75    163391\n",
      "   macro avg       0.77      0.75      0.75    163391\n",
      "weighted avg       0.77      0.75      0.75    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2156e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:4.43284\n",
      "[1]\tvalidation_0-mlogloss:4.35325\n",
      "[2]\tvalidation_0-mlogloss:4.20869\n",
      "[3]\tvalidation_0-mlogloss:4.06689\n",
      "[4]\tvalidation_0-mlogloss:3.88122\n",
      "[5]\tvalidation_0-mlogloss:3.77287\n",
      "[6]\tvalidation_0-mlogloss:3.69161\n",
      "[7]\tvalidation_0-mlogloss:3.57765\n",
      "[8]\tvalidation_0-mlogloss:3.51041\n",
      "[9]\tvalidation_0-mlogloss:3.43149\n",
      "[10]\tvalidation_0-mlogloss:3.38192\n",
      "[11]\tvalidation_0-mlogloss:3.32379\n",
      "[12]\tvalidation_0-mlogloss:3.28331\n",
      "[13]\tvalidation_0-mlogloss:3.21093\n",
      "[14]\tvalidation_0-mlogloss:3.18555\n",
      "[15]\tvalidation_0-mlogloss:3.12841\n",
      "[16]\tvalidation_0-mlogloss:3.09847\n",
      "[17]\tvalidation_0-mlogloss:3.05379\n",
      "[18]\tvalidation_0-mlogloss:3.01431\n",
      "[19]\tvalidation_0-mlogloss:2.98414\n",
      "[20]\tvalidation_0-mlogloss:2.95681\n",
      "[21]\tvalidation_0-mlogloss:2.93154\n",
      "[22]\tvalidation_0-mlogloss:2.91297\n",
      "[23]\tvalidation_0-mlogloss:2.88990\n",
      "[24]\tvalidation_0-mlogloss:2.86761\n",
      "[25]\tvalidation_0-mlogloss:2.84620\n",
      "[26]\tvalidation_0-mlogloss:2.82552\n",
      "[27]\tvalidation_0-mlogloss:2.79681\n",
      "[28]\tvalidation_0-mlogloss:2.77844\n",
      "[29]\tvalidation_0-mlogloss:2.75984\n",
      "[30]\tvalidation_0-mlogloss:2.74190\n",
      "[31]\tvalidation_0-mlogloss:2.71738\n",
      "[32]\tvalidation_0-mlogloss:2.70343\n",
      "[33]\tvalidation_0-mlogloss:2.68793\n",
      "[34]\tvalidation_0-mlogloss:2.67286\n",
      "[35]\tvalidation_0-mlogloss:2.66147\n",
      "[36]\tvalidation_0-mlogloss:2.64818\n",
      "[37]\tvalidation_0-mlogloss:2.63361\n",
      "[38]\tvalidation_0-mlogloss:2.61769\n",
      "[39]\tvalidation_0-mlogloss:2.60665\n",
      "[40]\tvalidation_0-mlogloss:2.59090\n",
      "[41]\tvalidation_0-mlogloss:2.57838\n",
      "[42]\tvalidation_0-mlogloss:2.56534\n",
      "[43]\tvalidation_0-mlogloss:2.55366\n",
      "[44]\tvalidation_0-mlogloss:2.54425\n",
      "[45]\tvalidation_0-mlogloss:2.52948\n",
      "[46]\tvalidation_0-mlogloss:2.51230\n",
      "[47]\tvalidation_0-mlogloss:2.50068\n",
      "[48]\tvalidation_0-mlogloss:2.49284\n",
      "[49]\tvalidation_0-mlogloss:2.48380\n",
      "[50]\tvalidation_0-mlogloss:2.47688\n",
      "[51]\tvalidation_0-mlogloss:2.46179\n",
      "[52]\tvalidation_0-mlogloss:2.45143\n",
      "[53]\tvalidation_0-mlogloss:2.44534\n",
      "[54]\tvalidation_0-mlogloss:2.43339\n",
      "[55]\tvalidation_0-mlogloss:2.42594\n",
      "[56]\tvalidation_0-mlogloss:2.41656\n",
      "[57]\tvalidation_0-mlogloss:2.40941\n",
      "[58]\tvalidation_0-mlogloss:2.40252\n",
      "[59]\tvalidation_0-mlogloss:2.39455\n",
      "[60]\tvalidation_0-mlogloss:2.38811\n",
      "[61]\tvalidation_0-mlogloss:2.37817\n",
      "[62]\tvalidation_0-mlogloss:2.37186\n",
      "[63]\tvalidation_0-mlogloss:2.36259\n",
      "[64]\tvalidation_0-mlogloss:2.35727\n",
      "[65]\tvalidation_0-mlogloss:2.34999\n",
      "[66]\tvalidation_0-mlogloss:2.34342\n",
      "[67]\tvalidation_0-mlogloss:2.33844\n",
      "[68]\tvalidation_0-mlogloss:2.33661\n",
      "[69]\tvalidation_0-mlogloss:2.33224\n",
      "[70]\tvalidation_0-mlogloss:2.32886\n",
      "[71]\tvalidation_0-mlogloss:2.32185\n",
      "[72]\tvalidation_0-mlogloss:2.32082\n",
      "[73]\tvalidation_0-mlogloss:2.31699\n",
      "[74]\tvalidation_0-mlogloss:2.31279\n",
      "[75]\tvalidation_0-mlogloss:2.30609\n",
      "[76]\tvalidation_0-mlogloss:2.30192\n",
      "[77]\tvalidation_0-mlogloss:2.29286\n",
      "[78]\tvalidation_0-mlogloss:2.29118\n",
      "[79]\tvalidation_0-mlogloss:2.29116\n",
      "[80]\tvalidation_0-mlogloss:2.28592\n",
      "[81]\tvalidation_0-mlogloss:2.28185\n",
      "[82]\tvalidation_0-mlogloss:2.27781\n",
      "[83]\tvalidation_0-mlogloss:2.27203\n",
      "[84]\tvalidation_0-mlogloss:2.26999\n",
      "[85]\tvalidation_0-mlogloss:2.26887\n",
      "[86]\tvalidation_0-mlogloss:2.26219\n",
      "[87]\tvalidation_0-mlogloss:2.25958\n",
      "[88]\tvalidation_0-mlogloss:2.25414\n",
      "[89]\tvalidation_0-mlogloss:2.24794\n",
      "[90]\tvalidation_0-mlogloss:2.24555\n",
      "[91]\tvalidation_0-mlogloss:2.24491\n",
      "[92]\tvalidation_0-mlogloss:2.24383\n",
      "[93]\tvalidation_0-mlogloss:2.24102\n",
      "[94]\tvalidation_0-mlogloss:2.24043\n",
      "[95]\tvalidation_0-mlogloss:2.23418\n",
      "[96]\tvalidation_0-mlogloss:2.22986\n",
      "[97]\tvalidation_0-mlogloss:2.22612\n",
      "[98]\tvalidation_0-mlogloss:2.22047\n",
      "[99]\tvalidation_0-mlogloss:2.22015\n",
      "[100]\tvalidation_0-mlogloss:2.21447\n",
      "[101]\tvalidation_0-mlogloss:2.21213\n",
      "[102]\tvalidation_0-mlogloss:2.21049\n",
      "[103]\tvalidation_0-mlogloss:2.20595\n",
      "[104]\tvalidation_0-mlogloss:2.20056\n",
      "[105]\tvalidation_0-mlogloss:2.19866\n",
      "[106]\tvalidation_0-mlogloss:2.19408\n",
      "[107]\tvalidation_0-mlogloss:2.19281\n",
      "[108]\tvalidation_0-mlogloss:2.19093\n",
      "[109]\tvalidation_0-mlogloss:2.18978\n",
      "[110]\tvalidation_0-mlogloss:2.18120\n",
      "[111]\tvalidation_0-mlogloss:2.18283\n",
      "[112]\tvalidation_0-mlogloss:2.17989\n",
      "[113]\tvalidation_0-mlogloss:2.17950\n",
      "[114]\tvalidation_0-mlogloss:2.17748\n",
      "[115]\tvalidation_0-mlogloss:2.17543\n",
      "[116]\tvalidation_0-mlogloss:2.17138\n",
      "[117]\tvalidation_0-mlogloss:2.16608\n",
      "[118]\tvalidation_0-mlogloss:2.16482\n",
      "[119]\tvalidation_0-mlogloss:2.15790\n",
      "[120]\tvalidation_0-mlogloss:2.15644\n",
      "[121]\tvalidation_0-mlogloss:2.15231\n",
      "[122]\tvalidation_0-mlogloss:2.15060\n",
      "[123]\tvalidation_0-mlogloss:2.14850\n",
      "[124]\tvalidation_0-mlogloss:2.14879\n",
      "[125]\tvalidation_0-mlogloss:2.14591\n",
      "[126]\tvalidation_0-mlogloss:2.14449\n",
      "[127]\tvalidation_0-mlogloss:2.14220\n",
      "[128]\tvalidation_0-mlogloss:2.14006\n",
      "[129]\tvalidation_0-mlogloss:2.13864\n",
      "[130]\tvalidation_0-mlogloss:2.13836\n",
      "[131]\tvalidation_0-mlogloss:2.13880\n",
      "[132]\tvalidation_0-mlogloss:2.13601\n",
      "[133]\tvalidation_0-mlogloss:2.13454\n",
      "[134]\tvalidation_0-mlogloss:2.13377\n",
      "[135]\tvalidation_0-mlogloss:2.13152\n",
      "[136]\tvalidation_0-mlogloss:2.13307\n",
      "[137]\tvalidation_0-mlogloss:2.13120\n",
      "[138]\tvalidation_0-mlogloss:2.13033\n",
      "[139]\tvalidation_0-mlogloss:2.12748\n",
      "[140]\tvalidation_0-mlogloss:2.12612\n",
      "[141]\tvalidation_0-mlogloss:2.12434\n",
      "[142]\tvalidation_0-mlogloss:2.12299\n",
      "[143]\tvalidation_0-mlogloss:2.12113\n",
      "[144]\tvalidation_0-mlogloss:2.12047\n",
      "[145]\tvalidation_0-mlogloss:2.12040\n",
      "[146]\tvalidation_0-mlogloss:2.11811\n",
      "[147]\tvalidation_0-mlogloss:2.11673\n",
      "[148]\tvalidation_0-mlogloss:2.11499\n",
      "[149]\tvalidation_0-mlogloss:2.11326\n",
      "[150]\tvalidation_0-mlogloss:2.11387\n",
      "[151]\tvalidation_0-mlogloss:2.11300\n",
      "[152]\tvalidation_0-mlogloss:2.11125\n",
      "[153]\tvalidation_0-mlogloss:2.10859\n",
      "[154]\tvalidation_0-mlogloss:2.10511\n",
      "[155]\tvalidation_0-mlogloss:2.10549\n",
      "[156]\tvalidation_0-mlogloss:2.10636\n",
      "[157]\tvalidation_0-mlogloss:2.10493\n",
      "[158]\tvalidation_0-mlogloss:2.10206\n",
      "[159]\tvalidation_0-mlogloss:2.09989\n",
      "[160]\tvalidation_0-mlogloss:2.09918\n",
      "[161]\tvalidation_0-mlogloss:2.09762\n",
      "[162]\tvalidation_0-mlogloss:2.09598\n",
      "[163]\tvalidation_0-mlogloss:2.09356\n",
      "[164]\tvalidation_0-mlogloss:2.09152\n",
      "[165]\tvalidation_0-mlogloss:2.08849\n",
      "[166]\tvalidation_0-mlogloss:2.08915\n",
      "[167]\tvalidation_0-mlogloss:2.08527\n",
      "[168]\tvalidation_0-mlogloss:2.08365\n",
      "[169]\tvalidation_0-mlogloss:2.08330\n",
      "[170]\tvalidation_0-mlogloss:2.08262\n",
      "[171]\tvalidation_0-mlogloss:2.08219\n",
      "[172]\tvalidation_0-mlogloss:2.07967\n",
      "[173]\tvalidation_0-mlogloss:2.07836\n",
      "[174]\tvalidation_0-mlogloss:2.07640\n",
      "[175]\tvalidation_0-mlogloss:2.07586\n",
      "[176]\tvalidation_0-mlogloss:2.07432\n",
      "[177]\tvalidation_0-mlogloss:2.07404\n",
      "[178]\tvalidation_0-mlogloss:2.07233\n",
      "[179]\tvalidation_0-mlogloss:2.07188\n",
      "[180]\tvalidation_0-mlogloss:2.07154\n",
      "[181]\tvalidation_0-mlogloss:2.07090\n",
      "[182]\tvalidation_0-mlogloss:2.07105\n",
      "[183]\tvalidation_0-mlogloss:2.06960\n",
      "[184]\tvalidation_0-mlogloss:2.06933\n",
      "[185]\tvalidation_0-mlogloss:2.06715\n",
      "[186]\tvalidation_0-mlogloss:2.06609\n",
      "[187]\tvalidation_0-mlogloss:2.06354\n",
      "[188]\tvalidation_0-mlogloss:2.06564\n",
      "[189]\tvalidation_0-mlogloss:2.06458\n",
      "[190]\tvalidation_0-mlogloss:2.06414\n",
      "[191]\tvalidation_0-mlogloss:2.06074\n",
      "[192]\tvalidation_0-mlogloss:2.06069\n",
      "[193]\tvalidation_0-mlogloss:2.05983\n",
      "[194]\tvalidation_0-mlogloss:2.05893\n",
      "[195]\tvalidation_0-mlogloss:2.05925\n",
      "[196]\tvalidation_0-mlogloss:2.05908\n",
      "[197]\tvalidation_0-mlogloss:2.05616\n",
      "[198]\tvalidation_0-mlogloss:2.05567\n",
      "[199]\tvalidation_0-mlogloss:2.05467\n",
      "[200]\tvalidation_0-mlogloss:2.05426\n",
      "[201]\tvalidation_0-mlogloss:2.05485\n",
      "[202]\tvalidation_0-mlogloss:2.05336\n",
      "[203]\tvalidation_0-mlogloss:2.05163\n",
      "[204]\tvalidation_0-mlogloss:2.05118\n",
      "[205]\tvalidation_0-mlogloss:2.05066\n",
      "[206]\tvalidation_0-mlogloss:2.05025\n",
      "[207]\tvalidation_0-mlogloss:2.04962\n",
      "[208]\tvalidation_0-mlogloss:2.04650\n",
      "[209]\tvalidation_0-mlogloss:2.04583\n",
      "[210]\tvalidation_0-mlogloss:2.04346\n",
      "[211]\tvalidation_0-mlogloss:2.04201\n",
      "[212]\tvalidation_0-mlogloss:2.04173\n",
      "[213]\tvalidation_0-mlogloss:2.03992\n",
      "[214]\tvalidation_0-mlogloss:2.03823\n",
      "[215]\tvalidation_0-mlogloss:2.03783\n",
      "[216]\tvalidation_0-mlogloss:2.03645\n",
      "[217]\tvalidation_0-mlogloss:2.03585\n",
      "[218]\tvalidation_0-mlogloss:2.03538\n",
      "[219]\tvalidation_0-mlogloss:2.03461\n",
      "[220]\tvalidation_0-mlogloss:2.03325\n",
      "[221]\tvalidation_0-mlogloss:2.03264\n",
      "[222]\tvalidation_0-mlogloss:2.03337\n",
      "[223]\tvalidation_0-mlogloss:2.03111\n",
      "[224]\tvalidation_0-mlogloss:2.03018\n",
      "[225]\tvalidation_0-mlogloss:2.03041\n",
      "[226]\tvalidation_0-mlogloss:2.02982\n",
      "[227]\tvalidation_0-mlogloss:2.03035\n",
      "[228]\tvalidation_0-mlogloss:2.02864\n",
      "[229]\tvalidation_0-mlogloss:2.02789\n",
      "[230]\tvalidation_0-mlogloss:2.02780\n",
      "[231]\tvalidation_0-mlogloss:2.02633\n",
      "[232]\tvalidation_0-mlogloss:2.02515\n",
      "[233]\tvalidation_0-mlogloss:2.02404\n",
      "[234]\tvalidation_0-mlogloss:2.02235\n",
      "[235]\tvalidation_0-mlogloss:2.02162\n",
      "[236]\tvalidation_0-mlogloss:2.02008\n",
      "[237]\tvalidation_0-mlogloss:2.01889\n",
      "[238]\tvalidation_0-mlogloss:2.01777\n",
      "[239]\tvalidation_0-mlogloss:2.01717\n",
      "[240]\tvalidation_0-mlogloss:2.01579\n",
      "[241]\tvalidation_0-mlogloss:2.01486\n",
      "[242]\tvalidation_0-mlogloss:2.01356\n",
      "[243]\tvalidation_0-mlogloss:2.01250\n",
      "[244]\tvalidation_0-mlogloss:2.01079\n",
      "[245]\tvalidation_0-mlogloss:2.01140\n",
      "[246]\tvalidation_0-mlogloss:2.01012\n",
      "[247]\tvalidation_0-mlogloss:2.00973\n",
      "[248]\tvalidation_0-mlogloss:2.01040\n",
      "[249]\tvalidation_0-mlogloss:2.00972\n",
      "[250]\tvalidation_0-mlogloss:2.00921\n",
      "[251]\tvalidation_0-mlogloss:2.00842\n",
      "[252]\tvalidation_0-mlogloss:2.00873\n",
      "[253]\tvalidation_0-mlogloss:2.00616\n",
      "[254]\tvalidation_0-mlogloss:2.00574\n",
      "[255]\tvalidation_0-mlogloss:2.00502\n",
      "[256]\tvalidation_0-mlogloss:2.00397\n",
      "[257]\tvalidation_0-mlogloss:2.00427\n",
      "[258]\tvalidation_0-mlogloss:2.00341\n",
      "[259]\tvalidation_0-mlogloss:2.00280\n",
      "[260]\tvalidation_0-mlogloss:2.00311\n",
      "[261]\tvalidation_0-mlogloss:2.00172\n",
      "[262]\tvalidation_0-mlogloss:2.00131\n",
      "[263]\tvalidation_0-mlogloss:2.00011\n",
      "[264]\tvalidation_0-mlogloss:1.99881\n",
      "[265]\tvalidation_0-mlogloss:1.99830\n",
      "[266]\tvalidation_0-mlogloss:1.99684\n",
      "[267]\tvalidation_0-mlogloss:1.99618\n",
      "[268]\tvalidation_0-mlogloss:1.99556\n",
      "[269]\tvalidation_0-mlogloss:1.99558\n",
      "[270]\tvalidation_0-mlogloss:1.99510\n",
      "[271]\tvalidation_0-mlogloss:1.99399\n",
      "[272]\tvalidation_0-mlogloss:1.99301\n",
      "[273]\tvalidation_0-mlogloss:1.99293\n",
      "[274]\tvalidation_0-mlogloss:1.99240\n",
      "[275]\tvalidation_0-mlogloss:1.99101\n",
      "[276]\tvalidation_0-mlogloss:1.99105\n",
      "[277]\tvalidation_0-mlogloss:1.98978\n",
      "[278]\tvalidation_0-mlogloss:1.98855\n",
      "[279]\tvalidation_0-mlogloss:1.98772\n",
      "[280]\tvalidation_0-mlogloss:1.98674\n",
      "[281]\tvalidation_0-mlogloss:1.98566\n",
      "[282]\tvalidation_0-mlogloss:1.98625\n",
      "[283]\tvalidation_0-mlogloss:1.98605\n",
      "[284]\tvalidation_0-mlogloss:1.98521\n",
      "[285]\tvalidation_0-mlogloss:1.98452\n",
      "[286]\tvalidation_0-mlogloss:1.98452\n",
      "[287]\tvalidation_0-mlogloss:1.98362\n",
      "[288]\tvalidation_0-mlogloss:1.98263\n",
      "[289]\tvalidation_0-mlogloss:1.98278\n",
      "[290]\tvalidation_0-mlogloss:1.98195\n",
      "[291]\tvalidation_0-mlogloss:1.98129\n",
      "[292]\tvalidation_0-mlogloss:1.98095\n",
      "[293]\tvalidation_0-mlogloss:1.98078\n",
      "[294]\tvalidation_0-mlogloss:1.98013\n",
      "[295]\tvalidation_0-mlogloss:1.97988\n",
      "[296]\tvalidation_0-mlogloss:1.97999\n",
      "[297]\tvalidation_0-mlogloss:1.97893\n",
      "[298]\tvalidation_0-mlogloss:1.97791\n",
      "[299]\tvalidation_0-mlogloss:1.97772\n",
      "[300]\tvalidation_0-mlogloss:1.97718\n",
      "[301]\tvalidation_0-mlogloss:1.97683\n",
      "[302]\tvalidation_0-mlogloss:1.97637\n",
      "[303]\tvalidation_0-mlogloss:1.97608\n",
      "[304]\tvalidation_0-mlogloss:1.97575\n",
      "[305]\tvalidation_0-mlogloss:1.97552\n",
      "[306]\tvalidation_0-mlogloss:1.97477\n",
      "[307]\tvalidation_0-mlogloss:1.97405\n",
      "[308]\tvalidation_0-mlogloss:1.97311\n",
      "[309]\tvalidation_0-mlogloss:1.97058\n",
      "[310]\tvalidation_0-mlogloss:1.97079\n",
      "[311]\tvalidation_0-mlogloss:1.97094\n",
      "[312]\tvalidation_0-mlogloss:1.97062\n",
      "[313]\tvalidation_0-mlogloss:1.97057\n",
      "[314]\tvalidation_0-mlogloss:1.97061\n",
      "[315]\tvalidation_0-mlogloss:1.96999\n",
      "[316]\tvalidation_0-mlogloss:1.96922\n",
      "[317]\tvalidation_0-mlogloss:1.96931\n",
      "[318]\tvalidation_0-mlogloss:1.96884\n",
      "[319]\tvalidation_0-mlogloss:1.96801\n",
      "[320]\tvalidation_0-mlogloss:1.96756\n",
      "[321]\tvalidation_0-mlogloss:1.96702\n",
      "[322]\tvalidation_0-mlogloss:1.96731\n",
      "[323]\tvalidation_0-mlogloss:1.96686\n",
      "[324]\tvalidation_0-mlogloss:1.96564\n",
      "[325]\tvalidation_0-mlogloss:1.96464\n",
      "[326]\tvalidation_0-mlogloss:1.96385\n",
      "[327]\tvalidation_0-mlogloss:1.96374\n",
      "[328]\tvalidation_0-mlogloss:1.96333\n",
      "[329]\tvalidation_0-mlogloss:1.96263\n",
      "[330]\tvalidation_0-mlogloss:1.96237\n",
      "[331]\tvalidation_0-mlogloss:1.96199\n",
      "[332]\tvalidation_0-mlogloss:1.96283\n",
      "[333]\tvalidation_0-mlogloss:1.96228\n",
      "[334]\tvalidation_0-mlogloss:1.96213\n",
      "[335]\tvalidation_0-mlogloss:1.95968\n",
      "[336]\tvalidation_0-mlogloss:1.95954\n",
      "[337]\tvalidation_0-mlogloss:1.95866\n",
      "[338]\tvalidation_0-mlogloss:1.95876\n",
      "[339]\tvalidation_0-mlogloss:1.95796\n",
      "[340]\tvalidation_0-mlogloss:1.95801\n",
      "[341]\tvalidation_0-mlogloss:1.95732\n",
      "[342]\tvalidation_0-mlogloss:1.95729\n",
      "[343]\tvalidation_0-mlogloss:1.95655\n",
      "[344]\tvalidation_0-mlogloss:1.95638\n",
      "[345]\tvalidation_0-mlogloss:1.95600\n",
      "[346]\tvalidation_0-mlogloss:1.95638\n",
      "[347]\tvalidation_0-mlogloss:1.95655\n",
      "[348]\tvalidation_0-mlogloss:1.95587\n",
      "[349]\tvalidation_0-mlogloss:1.95589\n",
      "[350]\tvalidation_0-mlogloss:1.95598\n",
      "[351]\tvalidation_0-mlogloss:1.95648\n",
      "[352]\tvalidation_0-mlogloss:1.95542\n",
      "[353]\tvalidation_0-mlogloss:1.95530\n",
      "[354]\tvalidation_0-mlogloss:1.95578\n",
      "[355]\tvalidation_0-mlogloss:1.95617\n",
      "[356]\tvalidation_0-mlogloss:1.95611\n",
      "[357]\tvalidation_0-mlogloss:1.95633\n",
      "[358]\tvalidation_0-mlogloss:1.95650\n",
      "[359]\tvalidation_0-mlogloss:1.95618\n",
      "[360]\tvalidation_0-mlogloss:1.95578\n",
      "[361]\tvalidation_0-mlogloss:1.95579\n",
      "[362]\tvalidation_0-mlogloss:1.95571\n",
      "[363]\tvalidation_0-mlogloss:1.95499\n",
      "[364]\tvalidation_0-mlogloss:1.95478\n",
      "[365]\tvalidation_0-mlogloss:1.95485\n",
      "[366]\tvalidation_0-mlogloss:1.95540\n",
      "[367]\tvalidation_0-mlogloss:1.95501\n",
      "[368]\tvalidation_0-mlogloss:1.95431\n",
      "[369]\tvalidation_0-mlogloss:1.95449\n",
      "[370]\tvalidation_0-mlogloss:1.95394\n",
      "[371]\tvalidation_0-mlogloss:1.95354\n",
      "[372]\tvalidation_0-mlogloss:1.95273\n",
      "[373]\tvalidation_0-mlogloss:1.95278\n",
      "[374]\tvalidation_0-mlogloss:1.95232\n",
      "[375]\tvalidation_0-mlogloss:1.95250\n",
      "[376]\tvalidation_0-mlogloss:1.95206\n",
      "[377]\tvalidation_0-mlogloss:1.95158\n",
      "[378]\tvalidation_0-mlogloss:1.95036\n",
      "[379]\tvalidation_0-mlogloss:1.95073\n",
      "[380]\tvalidation_0-mlogloss:1.95002\n",
      "[381]\tvalidation_0-mlogloss:1.95057\n",
      "[382]\tvalidation_0-mlogloss:1.95005\n",
      "[383]\tvalidation_0-mlogloss:1.94999\n",
      "[384]\tvalidation_0-mlogloss:1.95036\n",
      "[385]\tvalidation_0-mlogloss:1.94973\n",
      "[386]\tvalidation_0-mlogloss:1.94963\n",
      "[387]\tvalidation_0-mlogloss:1.94900\n",
      "[388]\tvalidation_0-mlogloss:1.94919\n",
      "[389]\tvalidation_0-mlogloss:1.94938\n",
      "[390]\tvalidation_0-mlogloss:1.94927\n",
      "[391]\tvalidation_0-mlogloss:1.94935\n",
      "[392]\tvalidation_0-mlogloss:1.94921\n",
      "[393]\tvalidation_0-mlogloss:1.94905\n",
      "[394]\tvalidation_0-mlogloss:1.94867\n",
      "[395]\tvalidation_0-mlogloss:1.94874\n",
      "[396]\tvalidation_0-mlogloss:1.94846\n",
      "[397]\tvalidation_0-mlogloss:1.94817\n",
      "[398]\tvalidation_0-mlogloss:1.94799\n",
      "[399]\tvalidation_0-mlogloss:1.94786\n",
      "[400]\tvalidation_0-mlogloss:1.94783\n",
      "[401]\tvalidation_0-mlogloss:1.94808\n",
      "[402]\tvalidation_0-mlogloss:1.94753\n",
      "[403]\tvalidation_0-mlogloss:1.94755\n",
      "[404]\tvalidation_0-mlogloss:1.94700\n",
      "[405]\tvalidation_0-mlogloss:1.94665\n",
      "[406]\tvalidation_0-mlogloss:1.94623\n",
      "[407]\tvalidation_0-mlogloss:1.94570\n",
      "[408]\tvalidation_0-mlogloss:1.94569\n",
      "[409]\tvalidation_0-mlogloss:1.94556\n",
      "[410]\tvalidation_0-mlogloss:1.94514\n",
      "[411]\tvalidation_0-mlogloss:1.94448\n",
      "[412]\tvalidation_0-mlogloss:1.94428\n",
      "[413]\tvalidation_0-mlogloss:1.94394\n",
      "[414]\tvalidation_0-mlogloss:1.94373\n",
      "[415]\tvalidation_0-mlogloss:1.94373\n",
      "[416]\tvalidation_0-mlogloss:1.94369\n",
      "[417]\tvalidation_0-mlogloss:1.94332\n",
      "[418]\tvalidation_0-mlogloss:1.94351\n",
      "[419]\tvalidation_0-mlogloss:1.94335\n",
      "[420]\tvalidation_0-mlogloss:1.94298\n",
      "[421]\tvalidation_0-mlogloss:1.94225\n",
      "[422]\tvalidation_0-mlogloss:1.94205\n",
      "[423]\tvalidation_0-mlogloss:1.94179\n",
      "[424]\tvalidation_0-mlogloss:1.94123\n",
      "[425]\tvalidation_0-mlogloss:1.94075\n",
      "[426]\tvalidation_0-mlogloss:1.94090\n",
      "[427]\tvalidation_0-mlogloss:1.94031\n",
      "[428]\tvalidation_0-mlogloss:1.93988\n",
      "[429]\tvalidation_0-mlogloss:1.93965\n",
      "[430]\tvalidation_0-mlogloss:1.93899\n",
      "[431]\tvalidation_0-mlogloss:1.93835\n",
      "[432]\tvalidation_0-mlogloss:1.93799\n",
      "[433]\tvalidation_0-mlogloss:1.93742\n",
      "[434]\tvalidation_0-mlogloss:1.93729\n",
      "[435]\tvalidation_0-mlogloss:1.93692\n",
      "[436]\tvalidation_0-mlogloss:1.93682\n",
      "[437]\tvalidation_0-mlogloss:1.93645\n",
      "[438]\tvalidation_0-mlogloss:1.93624\n",
      "[439]\tvalidation_0-mlogloss:1.93617\n",
      "[440]\tvalidation_0-mlogloss:1.93577\n",
      "[441]\tvalidation_0-mlogloss:1.93569\n",
      "[442]\tvalidation_0-mlogloss:1.93517\n",
      "[443]\tvalidation_0-mlogloss:1.93511\n",
      "[444]\tvalidation_0-mlogloss:1.93473\n",
      "[445]\tvalidation_0-mlogloss:1.93446\n",
      "[446]\tvalidation_0-mlogloss:1.93439\n",
      "[447]\tvalidation_0-mlogloss:1.93443\n",
      "[448]\tvalidation_0-mlogloss:1.93446\n",
      "[449]\tvalidation_0-mlogloss:1.93392\n",
      "[450]\tvalidation_0-mlogloss:1.93367\n",
      "[451]\tvalidation_0-mlogloss:1.93342\n",
      "[452]\tvalidation_0-mlogloss:1.93337\n",
      "[453]\tvalidation_0-mlogloss:1.93356\n",
      "[454]\tvalidation_0-mlogloss:1.93332\n",
      "[455]\tvalidation_0-mlogloss:1.93354\n",
      "[456]\tvalidation_0-mlogloss:1.93280\n",
      "[457]\tvalidation_0-mlogloss:1.93258\n",
      "[458]\tvalidation_0-mlogloss:1.93232\n",
      "[459]\tvalidation_0-mlogloss:1.93217\n",
      "[460]\tvalidation_0-mlogloss:1.93178\n",
      "[461]\tvalidation_0-mlogloss:1.93176\n",
      "[462]\tvalidation_0-mlogloss:1.93147\n",
      "[463]\tvalidation_0-mlogloss:1.93166\n",
      "[464]\tvalidation_0-mlogloss:1.93157\n",
      "[465]\tvalidation_0-mlogloss:1.93125\n",
      "[466]\tvalidation_0-mlogloss:1.93088\n",
      "[467]\tvalidation_0-mlogloss:1.93070\n",
      "[468]\tvalidation_0-mlogloss:1.93052\n",
      "[469]\tvalidation_0-mlogloss:1.93074\n",
      "[470]\tvalidation_0-mlogloss:1.93063\n",
      "[471]\tvalidation_0-mlogloss:1.93068\n",
      "[472]\tvalidation_0-mlogloss:1.93049\n",
      "[473]\tvalidation_0-mlogloss:1.93038\n",
      "[474]\tvalidation_0-mlogloss:1.93002\n",
      "[475]\tvalidation_0-mlogloss:1.93003\n",
      "[476]\tvalidation_0-mlogloss:1.93003\n",
      "[477]\tvalidation_0-mlogloss:1.92979\n",
      "[478]\tvalidation_0-mlogloss:1.92964\n",
      "[479]\tvalidation_0-mlogloss:1.92936\n",
      "[480]\tvalidation_0-mlogloss:1.92863\n",
      "[481]\tvalidation_0-mlogloss:1.92845\n",
      "[482]\tvalidation_0-mlogloss:1.92800\n",
      "[483]\tvalidation_0-mlogloss:1.92788\n",
      "[484]\tvalidation_0-mlogloss:1.92767\n",
      "[485]\tvalidation_0-mlogloss:1.92797\n",
      "[486]\tvalidation_0-mlogloss:1.92755\n",
      "[487]\tvalidation_0-mlogloss:1.92748\n",
      "[488]\tvalidation_0-mlogloss:1.92704\n",
      "[489]\tvalidation_0-mlogloss:1.92695\n",
      "[490]\tvalidation_0-mlogloss:1.92716\n",
      "[491]\tvalidation_0-mlogloss:1.92702\n",
      "[492]\tvalidation_0-mlogloss:1.92731\n",
      "[493]\tvalidation_0-mlogloss:1.92688\n",
      "[494]\tvalidation_0-mlogloss:1.92715\n",
      "[495]\tvalidation_0-mlogloss:1.92687\n",
      "[496]\tvalidation_0-mlogloss:1.92686\n",
      "[497]\tvalidation_0-mlogloss:1.92679\n",
      "[498]\tvalidation_0-mlogloss:1.92655\n",
      "[499]\tvalidation_0-mlogloss:1.92649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81      1499\n",
      "           1       0.87      0.95      0.90      1499\n",
      "           2       0.62      0.65      0.64      1499\n",
      "           3       0.91      0.70      0.79      1499\n",
      "           4       0.91      0.99      0.95      1499\n",
      "           5       0.98      0.98      0.98      1499\n",
      "           6       0.96      0.97      0.97      1499\n",
      "           7       0.94      0.87      0.91      1499\n",
      "           8       0.46      0.83      0.59      1499\n",
      "           9       0.69      0.62      0.65      1499\n",
      "          10       0.92      0.92      0.92      1499\n",
      "          11       0.96      0.98      0.97      1499\n",
      "          12       0.78      0.87      0.82      1499\n",
      "          13       0.91      0.93      0.92      1499\n",
      "          14       0.73      0.88      0.80      1499\n",
      "          15       0.95      0.65      0.77      1499\n",
      "          16       0.32      0.21      0.25      1499\n",
      "          17       0.86      0.83      0.84      1499\n",
      "          18       0.86      0.83      0.85      1499\n",
      "          19       0.78      0.61      0.68      1499\n",
      "          20       0.98      1.00      0.99      1499\n",
      "          21       0.78      0.83      0.80      1499\n",
      "          22       0.75      0.70      0.72      1499\n",
      "          23       0.95      0.98      0.96      1499\n",
      "          24       0.96      0.92      0.94      1499\n",
      "          25       0.91      0.70      0.79      1499\n",
      "          26       0.42      0.32      0.36      1499\n",
      "          27       0.61      0.57      0.59      1499\n",
      "          28       0.64      0.59      0.61      1499\n",
      "          29       0.68      0.96      0.80      1499\n",
      "          30       0.89      0.94      0.91      1499\n",
      "          31       0.62      0.88      0.73      1499\n",
      "          32       0.92      0.95      0.94      1499\n",
      "          33       0.95      0.49      0.65      1499\n",
      "          34       0.93      0.93      0.93      1499\n",
      "          35       0.81      0.91      0.86      1499\n",
      "          36       0.82      0.88      0.85      1499\n",
      "          37       0.78      0.80      0.79      1499\n",
      "          38       0.25      0.08      0.12      1499\n",
      "          39       0.67      0.78      0.72      1499\n",
      "          40       0.93      0.88      0.90      1499\n",
      "          41       0.83      0.68      0.75      1499\n",
      "          42       0.69      0.71      0.70      1499\n",
      "          43       0.82      0.79      0.80      1499\n",
      "          44       0.92      0.98      0.95      1499\n",
      "          45       0.56      0.66      0.61      1499\n",
      "          46       0.65      0.86      0.74      1499\n",
      "          47       0.53      0.51      0.52      1499\n",
      "          48       0.83      0.90      0.86      1499\n",
      "          49       0.79      0.89      0.84      1499\n",
      "          50       0.92      0.65      0.76      1499\n",
      "          51       0.39      0.68      0.49      1499\n",
      "          52       0.82      0.87      0.84      1499\n",
      "          53       0.82      0.82      0.82      1499\n",
      "          54       0.88      0.89      0.89      1499\n",
      "          55       0.75      0.88      0.81      1499\n",
      "          56       0.71      0.87      0.78      1499\n",
      "          57       0.86      0.95      0.90      1499\n",
      "          58       0.56      0.18      0.28      1499\n",
      "          59       0.38      0.33      0.35      1499\n",
      "          60       0.70      0.71      0.70      1499\n",
      "          61       0.65      0.17      0.26      1499\n",
      "          62       0.89      0.93      0.91      1499\n",
      "          63       0.73      0.71      0.72      1499\n",
      "          64       0.55      0.70      0.61      1499\n",
      "          65       0.89      0.97      0.92      1499\n",
      "          66       0.98      0.89      0.93      1499\n",
      "          67       0.31      0.25      0.28      1499\n",
      "          68       0.74      0.89      0.81      1499\n",
      "          69       0.57      0.79      0.66      1499\n",
      "          70       0.76      0.49      0.60      1499\n",
      "          71       0.80      0.51      0.63      1499\n",
      "          72       0.89      0.87      0.88      1499\n",
      "          73       0.90      0.85      0.88      1499\n",
      "          74       0.39      0.78      0.52      1499\n",
      "          75       0.93      0.78      0.85      1499\n",
      "          76       0.70      0.52      0.60      1499\n",
      "          77       0.77      0.81      0.79      1499\n",
      "          78       0.82      0.68      0.74      1499\n",
      "          79       0.92      0.84      0.88      1499\n",
      "          80       0.88      0.92      0.90      1499\n",
      "          81       0.87      0.66      0.75      1499\n",
      "          82       0.80      0.89      0.84      1499\n",
      "          83       0.71      0.54      0.61      1499\n",
      "          84       0.89      0.92      0.90      1499\n",
      "          85       0.83      0.78      0.80      1499\n",
      "          86       0.72      0.71      0.72      1499\n",
      "          87       0.65      0.62      0.64      1499\n",
      "          88       0.84      1.00      0.91      1499\n",
      "          89       0.77      0.86      0.82      1499\n",
      "          90       0.91      0.95      0.93      1499\n",
      "          91       0.81      0.94      0.87      1499\n",
      "          92       0.88      0.56      0.69      1499\n",
      "          93       0.94      0.98      0.96      1499\n",
      "          94       0.66      0.86      0.75      1499\n",
      "          95       0.79      0.75      0.77      1499\n",
      "          96       0.90      0.98      0.94      1499\n",
      "          97       0.82      0.92      0.87      1499\n",
      "          98       0.78      0.86      0.82      1499\n",
      "          99       0.83      0.86      0.84      1499\n",
      "         100       0.73      0.60      0.66      1499\n",
      "         101       0.85      0.85      0.85      1499\n",
      "         102       0.60      0.61      0.60      1499\n",
      "         103       0.78      0.78      0.78      1499\n",
      "         104       0.97      0.80      0.88      1499\n",
      "         105       0.73      0.76      0.74      1499\n",
      "         106       0.73      0.81      0.77      1499\n",
      "         107       0.76      0.76      0.76      1499\n",
      "         108       0.97      0.81      0.88      1499\n",
      "\n",
      "    accuracy                           0.77    163391\n",
      "   macro avg       0.77      0.77      0.76    163391\n",
      "weighted avg       0.77      0.77      0.76    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.5,verbosity = 1, random_state = 55)\n",
    "xgb_model.fit(x,y, eval_set = [(xv,yv)], early_stopping_rounds = 10)\n",
    "y_pred=xgb_model.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

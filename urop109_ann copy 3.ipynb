{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 109 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081a3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.6.3 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.7.0)\n",
      "Requirement already satisfied: tqdm in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (4.64.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.7.0)\n",
      "Requirement already satisfied: decorator in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: packaging in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (22.0)\n",
      "Requirement already satisfied: jinja2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (3.8.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from jinja2->mne) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files4/S001R06.edf',\n",
       " 'files4/S002R06.edf',\n",
       " 'files4/S003R06.edf',\n",
       " 'files4/S004R06.edf',\n",
       " 'files4/S005R06.edf',\n",
       " 'files4/S006R06.edf',\n",
       " 'files4/S007R06.edf',\n",
       " 'files4/S008R06.edf',\n",
       " 'files4/S009R06.edf',\n",
       " 'files4/S010R06.edf',\n",
       " 'files4/S011R06.edf',\n",
       " 'files4/S012R06.edf',\n",
       " 'files4/S013R06.edf',\n",
       " 'files4/S014R06.edf',\n",
       " 'files4/S015R06.edf',\n",
       " 'files4/S016R06.edf',\n",
       " 'files4/S017R06.edf',\n",
       " 'files4/S018R06.edf',\n",
       " 'files4/S019R06.edf',\n",
       " 'files4/S020R06.edf',\n",
       " 'files4/S021R06.edf',\n",
       " 'files4/S022R06.edf',\n",
       " 'files4/S023R06.edf',\n",
       " 'files4/S024R06.edf',\n",
       " 'files4/S025R06.edf',\n",
       " 'files4/S026R06.edf',\n",
       " 'files4/S027R06.edf',\n",
       " 'files4/S028R06.edf',\n",
       " 'files4/S029R06.edf',\n",
       " 'files4/S030R06.edf',\n",
       " 'files4/S031R06.edf',\n",
       " 'files4/S032R06.edf',\n",
       " 'files4/S033R06.edf',\n",
       " 'files4/S034R06.edf',\n",
       " 'files4/S035R06.edf',\n",
       " 'files4/S036R06.edf',\n",
       " 'files4/S037R06.edf',\n",
       " 'files4/S038R06.edf',\n",
       " 'files4/S039R06.edf',\n",
       " 'files4/S040R06.edf',\n",
       " 'files4/S041R06.edf',\n",
       " 'files4/S042R06.edf',\n",
       " 'files4/S043R06.edf',\n",
       " 'files4/S044R06.edf',\n",
       " 'files4/S045R06.edf',\n",
       " 'files4/S046R06.edf',\n",
       " 'files4/S047R06.edf',\n",
       " 'files4/S048R06.edf',\n",
       " 'files4/S049R06.edf',\n",
       " 'files4/S050R06.edf',\n",
       " 'files4/S051R06.edf',\n",
       " 'files4/S052R06.edf',\n",
       " 'files4/S053R06.edf',\n",
       " 'files4/S054R06.edf',\n",
       " 'files4/S055R06.edf',\n",
       " 'files4/S056R06.edf',\n",
       " 'files4/S057R06.edf',\n",
       " 'files4/S058R06.edf',\n",
       " 'files4/S059R06.edf',\n",
       " 'files4/S060R06.edf',\n",
       " 'files4/S061R06.edf',\n",
       " 'files4/S062R06.edf',\n",
       " 'files4/S063R06.edf',\n",
       " 'files4/S064R06.edf',\n",
       " 'files4/S065R06.edf',\n",
       " 'files4/S066R06.edf',\n",
       " 'files4/S067R06.edf',\n",
       " 'files4/S068R06.edf',\n",
       " 'files4/S069R06.edf',\n",
       " 'files4/S070R06.edf',\n",
       " 'files4/S071R06.edf',\n",
       " 'files4/S072R06.edf',\n",
       " 'files4/S073R06.edf',\n",
       " 'files4/S074R06.edf',\n",
       " 'files4/S075R06.edf',\n",
       " 'files4/S076R06.edf',\n",
       " 'files4/S077R06.edf',\n",
       " 'files4/S078R06.edf',\n",
       " 'files4/S079R06.edf',\n",
       " 'files4/S080R06.edf',\n",
       " 'files4/S081R06.edf',\n",
       " 'files4/S082R06.edf',\n",
       " 'files4/S083R06.edf',\n",
       " 'files4/S084R06.edf',\n",
       " 'files4/S085R06.edf',\n",
       " 'files4/S086R06.edf',\n",
       " 'files4/S087R06.edf',\n",
       " 'files4/S088R06.edf',\n",
       " 'files4/S089R06.edf',\n",
       " 'files4/S090R06.edf',\n",
       " 'files4/S091R06.edf',\n",
       " 'files4/S092R06.edf',\n",
       " 'files4/S093R06.edf',\n",
       " 'files4/S094R06.edf',\n",
       " 'files4/S095R06.edf',\n",
       " 'files4/S096R06.edf',\n",
       " 'files4/S097R06.edf',\n",
       " 'files4/S098R06.edf',\n",
       " 'files4/S099R06.edf',\n",
       " 'files4/S100R06.edf',\n",
       " 'files4/S101R06.edf',\n",
       " 'files4/S102R06.edf',\n",
       " 'files4/S103R06.edf',\n",
       " 'files4/S104R06.edf',\n",
       " 'files4/S105R06.edf',\n",
       " 'files4/S106R06.edf',\n",
       " 'files4/S107R06.edf',\n",
       " 'files4/S108R06.edf',\n",
       " 'files4/S109R06.edf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files4/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1308000, 163391, 1308000, 163391)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.8e-05 -3.7e-05 -1.0e-06 ... -2.6e-05 -1.7e-05 -5.0e-06]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307995</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307996</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307997</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307998</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307999</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0         0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1         0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2         0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3         0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4         0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "1307995   0.000045   0.000128   0.000290   0.000177   0.000145   0.000001   \n",
       "1307996   0.000043   0.000117   0.000287   0.000166   0.000133   0.000051   \n",
       "1307997   0.000043   0.000092   0.000284   0.000149   0.000106   0.000031   \n",
       "1307998   0.000052   0.000086   0.000277   0.000142   0.000096   0.000022   \n",
       "1307999   0.000051   0.000082   0.000279   0.000125   0.000079   0.000057   \n",
       "\n",
       "         Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0         0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1         0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2         0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3         0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4         0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...            ...        ...        ...        ...  ...         ...   \n",
       "1307995   0.000156   0.000072   0.000023   0.000128  ...    0.000040   \n",
       "1307996   0.000139   0.000062   0.000010   0.000135  ...    0.000039   \n",
       "1307997   0.000121   0.000050  -0.000010   0.000103  ...    0.000039   \n",
       "1307998   0.000116   0.000046  -0.000010   0.000099  ...    0.000021   \n",
       "1307999   0.000108   0.000025  -0.000023   0.000082  ...    0.000021   \n",
       "\n",
       "         Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0          0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1          0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2          0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3          0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4          0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1307995    0.000062    0.000023    0.000045   -0.000007    0.000083   \n",
       "1307996    0.000064    0.000024    0.000033   -0.000011    0.000079   \n",
       "1307997    0.000060    0.000018    0.000024   -0.000018    0.000076   \n",
       "1307998    0.000031   -0.000009    0.000001   -0.000045    0.000051   \n",
       "1307999    0.000029   -0.000012    0.000002   -0.000046    0.000048   \n",
       "\n",
       "         Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0          0.000029   -0.000004    0.000059   -0.000092  \n",
       "1          0.000032    0.000004    0.000069   -0.000093  \n",
       "2          0.000025   -0.000003    0.000065   -0.000102  \n",
       "3          0.000013   -0.000016    0.000050   -0.000107  \n",
       "4          0.000015   -0.000013    0.000044   -0.000094  \n",
       "...             ...         ...         ...         ...  \n",
       "1307995    0.000036    0.000061    0.000064    0.000045  \n",
       "1307996    0.000034    0.000058    0.000060    0.000047  \n",
       "1307997    0.000026    0.000049    0.000047    0.000037  \n",
       "1307998   -0.000001    0.000025    0.000012    0.000014  \n",
       "1307999    0.000005    0.000030    0.000011    0.000025  \n",
       "\n",
       "[1308000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gk/c7h7qkyd3sn7d5qd8rlphgk80000gn/T/ipykernel_63563/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/var/folders/gk/c7h7qkyd3sn7d5qd8rlphgk80000gn/T/ipykernel_63563/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/var/folders/gk/c7h7qkyd3sn7d5qd8rlphgk80000gn/T/ipykernel_63563/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307995</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307996</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307997</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307998</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307999</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0         0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1         0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2         0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3         0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4         0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "1307995   0.000045   0.000128   0.000290   0.000177   0.000145   0.000001   \n",
       "1307996   0.000043   0.000117   0.000287   0.000166   0.000133   0.000051   \n",
       "1307997   0.000043   0.000092   0.000284   0.000149   0.000106   0.000031   \n",
       "1307998   0.000052   0.000086   0.000277   0.000142   0.000096   0.000022   \n",
       "1307999   0.000051   0.000082   0.000279   0.000125   0.000079   0.000057   \n",
       "\n",
       "         Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0         0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1         0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2         0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3         0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4         0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...            ...        ...        ...        ...  ...         ...   \n",
       "1307995   0.000156   0.000072   0.000023   0.000128  ...    0.000040   \n",
       "1307996   0.000139   0.000062   0.000010   0.000135  ...    0.000039   \n",
       "1307997   0.000121   0.000050  -0.000010   0.000103  ...    0.000039   \n",
       "1307998   0.000116   0.000046  -0.000010   0.000099  ...    0.000021   \n",
       "1307999   0.000108   0.000025  -0.000023   0.000082  ...    0.000021   \n",
       "\n",
       "         Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0          0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1          0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2          0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3          0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4          0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1307995    0.000062    0.000023    0.000045   -0.000007    0.000083   \n",
       "1307996    0.000064    0.000024    0.000033   -0.000011    0.000079   \n",
       "1307997    0.000060    0.000018    0.000024   -0.000018    0.000076   \n",
       "1307998    0.000031   -0.000009    0.000001   -0.000045    0.000051   \n",
       "1307999    0.000029   -0.000012    0.000002   -0.000046    0.000048   \n",
       "\n",
       "         Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0          0.000029   -0.000004    0.000059   -0.000092  \n",
       "1          0.000032    0.000004    0.000069   -0.000093  \n",
       "2          0.000025   -0.000003    0.000065   -0.000102  \n",
       "3          0.000013   -0.000016    0.000050   -0.000107  \n",
       "4          0.000015   -0.000013    0.000044   -0.000094  \n",
       "...             ...         ...         ...         ...  \n",
       "1307995    0.000036    0.000061    0.000064    0.000045  \n",
       "1307996    0.000034    0.000058    0.000060    0.000047  \n",
       "1307997    0.000026    0.000049    0.000047    0.000037  \n",
       "1307998   -0.000001    0.000025    0.000012    0.000014  \n",
       "1307999    0.000005    0.000030    0.000011    0.000025  \n",
       "\n",
       "[1308000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b271d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(109, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40875/40875 [==============================] - 34s 831us/step - loss: 3.1358 - val_loss: 3.3545\n",
      "Epoch 2/10\n",
      "40875/40875 [==============================] - 194s 5ms/step - loss: 2.6282 - val_loss: 3.2771\n",
      "Epoch 3/10\n",
      "40875/40875 [==============================] - 216s 5ms/step - loss: 2.4791 - val_loss: 3.2644\n",
      "Epoch 4/10\n",
      "40875/40875 [==============================] - 110s 3ms/step - loss: 2.4296 - val_loss: 3.3183\n",
      "Epoch 5/10\n",
      "40875/40875 [==============================] - 127s 3ms/step - loss: 2.3977 - val_loss: 3.2919\n",
      "Epoch 6/10\n",
      "40875/40875 [==============================] - 72s 2ms/step - loss: 2.3754 - val_loss: 3.3805\n",
      "Epoch 7/10\n",
      "40875/40875 [==============================] - 77s 2ms/step - loss: 2.3576 - val_loss: 3.4164\n",
      "Epoch 8/10\n",
      "40875/40875 [==============================] - 1084s 27ms/step - loss: 2.3432 - val_loss: 3.3356\n",
      "Epoch 9/10\n",
      "40875/40875 [==============================] - 333s 8ms/step - loss: 2.3306 - val_loss: 3.4074\n",
      "Epoch 10/10\n",
      "40875/40875 [==============================] - 41s 993us/step - loss: 2.3228 - val_loss: 3.4044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a3c59f60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106/5106 [==============================] - 2s 373us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.09      0.10      1499\n",
      "           1       0.05      0.03      0.03      1499\n",
      "           2       0.17      0.20      0.18      1499\n",
      "           3       0.72      0.34      0.46      1499\n",
      "           4       0.68      0.66      0.67      1499\n",
      "           5       0.37      0.30      0.33      1499\n",
      "           6       0.41      0.44      0.43      1499\n",
      "           7       0.46      0.49      0.47      1499\n",
      "           8       0.40      0.60      0.48      1499\n",
      "           9       0.40      0.25      0.30      1499\n",
      "          10       0.27      0.31      0.29      1499\n",
      "          11       0.17      0.41      0.24      1499\n",
      "          12       0.17      0.04      0.06      1499\n",
      "          13       0.06      0.06      0.06      1499\n",
      "          14       0.07      0.01      0.02      1499\n",
      "          15       0.34      0.28      0.31      1499\n",
      "          16       0.13      0.06      0.08      1499\n",
      "          17       0.34      0.79      0.48      1499\n",
      "          18       0.09      0.04      0.06      1499\n",
      "          19       0.12      0.08      0.10      1499\n",
      "          20       0.60      0.44      0.51      1499\n",
      "          21       0.31      0.41      0.35      1499\n",
      "          22       0.15      0.16      0.15      1499\n",
      "          23       0.51      0.66      0.57      1499\n",
      "          24       0.17      0.33      0.22      1499\n",
      "          25       0.13      0.04      0.06      1499\n",
      "          26       0.11      0.07      0.09      1499\n",
      "          27       0.16      0.21      0.19      1499\n",
      "          28       0.44      0.56      0.49      1499\n",
      "          29       0.34      0.33      0.34      1499\n",
      "          30       0.16      0.26      0.20      1499\n",
      "          31       0.12      0.17      0.14      1499\n",
      "          32       0.10      0.05      0.07      1499\n",
      "          33       0.10      0.16      0.12      1499\n",
      "          34       0.62      0.60      0.61      1499\n",
      "          35       0.35      0.73      0.47      1499\n",
      "          36       0.18      0.20      0.19      1499\n",
      "          37       0.35      0.08      0.13      1499\n",
      "          38       0.14      0.05      0.07      1499\n",
      "          39       0.22      0.17      0.19      1499\n",
      "          40       0.17      0.28      0.21      1499\n",
      "          41       0.43      0.39      0.41      1499\n",
      "          42       0.08      0.10      0.09      1499\n",
      "          43       0.21      0.18      0.20      1499\n",
      "          44       0.41      0.53      0.46      1499\n",
      "          45       0.07      0.02      0.03      1499\n",
      "          46       0.22      0.30      0.26      1499\n",
      "          47       0.18      0.14      0.16      1499\n",
      "          48       0.46      0.83      0.59      1499\n",
      "          49       0.06      0.08      0.07      1499\n",
      "          50       0.36      0.47      0.41      1499\n",
      "          51       0.17      0.06      0.08      1499\n",
      "          52       0.16      0.07      0.10      1499\n",
      "          53       0.12      0.04      0.06      1499\n",
      "          54       0.27      0.46      0.34      1499\n",
      "          55       0.31      0.44      0.36      1499\n",
      "          56       0.06      0.03      0.04      1499\n",
      "          57       0.57      0.62      0.59      1499\n",
      "          58       0.72      0.58      0.64      1499\n",
      "          59       0.08      0.13      0.10      1499\n",
      "          60       0.47      0.52      0.49      1499\n",
      "          61       0.14      0.09      0.11      1499\n",
      "          62       0.37      0.58      0.45      1499\n",
      "          63       0.06      0.03      0.04      1499\n",
      "          64       0.11      0.08      0.09      1499\n",
      "          65       0.12      0.07      0.09      1499\n",
      "          66       0.15      0.17      0.16      1499\n",
      "          67       0.36      0.24      0.29      1499\n",
      "          68       0.39      0.49      0.43      1499\n",
      "          69       0.21      0.19      0.20      1499\n",
      "          70       0.21      0.11      0.15      1499\n",
      "          71       0.06      0.04      0.05      1499\n",
      "          72       0.09      0.15      0.12      1499\n",
      "          73       0.15      0.24      0.18      1499\n",
      "          74       0.08      0.05      0.06      1499\n",
      "          75       0.45      0.62      0.52      1499\n",
      "          76       0.22      0.11      0.14      1499\n",
      "          77       0.16      0.10      0.13      1499\n",
      "          78       0.12      0.09      0.10      1499\n",
      "          79       0.18      0.10      0.13      1499\n",
      "          80       0.54      0.72      0.62      1499\n",
      "          81       0.30      0.13      0.18      1499\n",
      "          82       0.12      0.23      0.16      1499\n",
      "          83       0.09      0.05      0.07      1499\n",
      "          84       0.17      0.18      0.18      1499\n",
      "          85       0.22      0.37      0.27      1499\n",
      "          86       0.34      0.55      0.42      1499\n",
      "          87       0.12      0.03      0.04      1499\n",
      "          88       0.20      0.23      0.21      1499\n",
      "          89       0.13      0.11      0.12      1499\n",
      "          90       0.37      0.62      0.46      1499\n",
      "          91       0.14      0.23      0.18      1499\n",
      "          92       0.12      0.12      0.12      1499\n",
      "          93       0.45      0.48      0.46      1499\n",
      "          94       0.21      0.10      0.13      1499\n",
      "          95       0.06      0.00      0.01      1499\n",
      "          96       0.19      0.16      0.17      1499\n",
      "          97       0.08      0.20      0.12      1499\n",
      "          98       0.24      0.26      0.25      1499\n",
      "          99       0.47      0.67      0.55      1499\n",
      "         100       0.32      0.30      0.31      1499\n",
      "         101       0.08      0.11      0.09      1499\n",
      "         102       0.21      0.19      0.20      1499\n",
      "         103       0.05      0.02      0.03      1499\n",
      "         104       0.69      0.50      0.58      1499\n",
      "         105       0.07      0.04      0.05      1499\n",
      "         106       0.05      0.01      0.02      1499\n",
      "         107       0.09      0.13      0.10      1499\n",
      "         108       0.59      0.57      0.58      1499\n",
      "\n",
      "    accuracy                           0.26    163391\n",
      "   macro avg       0.25      0.26      0.24    163391\n",
      "weighted avg       0.25      0.26      0.24    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gk/c7h7qkyd3sn7d5qd8rlphgk80000gn/T/ipykernel_63563/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/var/folders/gk/c7h7qkyd3sn7d5qd8rlphgk80000gn/T/ipykernel_63563/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/var/folders/gk/c7h7qkyd3sn7d5qd8rlphgk80000gn/T/ipykernel_63563/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307995</th>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307996</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307997</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307998</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307999</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0         0.000073   0.000073   0.000091   0.000116   0.000101   0.000107   \n",
       "1         0.000063   0.000064   0.000078   0.000107   0.000086   0.000087   \n",
       "2         0.000082   0.000074   0.000082   0.000100   0.000083   0.000081   \n",
       "3         0.000063   0.000042   0.000050   0.000065   0.000059   0.000061   \n",
       "4         0.000051   0.000039   0.000052   0.000081   0.000069   0.000073   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "1307995   0.000045   0.000128   0.000290   0.000177   0.000145   0.000001   \n",
       "1307996   0.000043   0.000117   0.000287   0.000166   0.000133   0.000051   \n",
       "1307997   0.000043   0.000092   0.000284   0.000149   0.000106   0.000031   \n",
       "1307998   0.000052   0.000086   0.000277   0.000142   0.000096   0.000022   \n",
       "1307999   0.000051   0.000082   0.000279   0.000125   0.000079   0.000057   \n",
       "\n",
       "         Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0         0.000062   0.000035   0.000058   0.000058  ...    0.000036   \n",
       "1         0.000044   0.000036   0.000061   0.000058  ...    0.000043   \n",
       "2         0.000048   0.000049   0.000067   0.000058  ...    0.000041   \n",
       "3         0.000032   0.000024   0.000035   0.000023  ...    0.000034   \n",
       "4         0.000039   0.000018   0.000035   0.000030  ...    0.000034   \n",
       "...            ...        ...        ...        ...  ...         ...   \n",
       "1307995   0.000156   0.000072   0.000023   0.000128  ...    0.000040   \n",
       "1307996   0.000139   0.000062   0.000010   0.000135  ...    0.000039   \n",
       "1307997   0.000121   0.000050  -0.000010   0.000103  ...    0.000039   \n",
       "1307998   0.000116   0.000046  -0.000010   0.000099  ...    0.000021   \n",
       "1307999   0.000108   0.000025  -0.000023   0.000082  ...    0.000021   \n",
       "\n",
       "         Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0          0.000048    0.000046    0.000065    0.000039    0.000073   \n",
       "1          0.000062    0.000056    0.000068    0.000043    0.000088   \n",
       "2          0.000049    0.000040    0.000043    0.000025    0.000079   \n",
       "3          0.000040    0.000029    0.000033    0.000014    0.000069   \n",
       "4          0.000056    0.000049    0.000056    0.000025    0.000075   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1307995    0.000062    0.000023    0.000045   -0.000007    0.000083   \n",
       "1307996    0.000064    0.000024    0.000033   -0.000011    0.000079   \n",
       "1307997    0.000060    0.000018    0.000024   -0.000018    0.000076   \n",
       "1307998    0.000031   -0.000009    0.000001   -0.000045    0.000051   \n",
       "1307999    0.000029   -0.000012    0.000002   -0.000046    0.000048   \n",
       "\n",
       "         Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0          0.000029   -0.000004    0.000059   -0.000092  \n",
       "1          0.000032    0.000004    0.000069   -0.000093  \n",
       "2          0.000025   -0.000003    0.000065   -0.000102  \n",
       "3          0.000013   -0.000016    0.000050   -0.000107  \n",
       "4          0.000015   -0.000013    0.000044   -0.000094  \n",
       "...             ...         ...         ...         ...  \n",
       "1307995    0.000036    0.000061    0.000064    0.000045  \n",
       "1307996    0.000034    0.000058    0.000060    0.000047  \n",
       "1307997    0.000026    0.000049    0.000047    0.000037  \n",
       "1307998   -0.000001    0.000025    0.000012    0.000014  \n",
       "1307999    0.000005    0.000030    0.000011    0.000025  \n",
       "\n",
       "[1308000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7844baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(109, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7980b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40875/40875 [==============================] - 36s 864us/step - loss: 2.3679 - val_loss: 3.1601\n",
      "Epoch 2/10\n",
      "40875/40875 [==============================] - 40s 980us/step - loss: 1.7040 - val_loss: 3.2252\n",
      "Epoch 3/10\n",
      "40875/40875 [==============================] - 39s 960us/step - loss: 1.5331 - val_loss: 3.0932\n",
      "Epoch 4/10\n",
      "40875/40875 [==============================] - 37s 916us/step - loss: 1.4436 - val_loss: 3.2501\n",
      "Epoch 5/10\n",
      "40875/40875 [==============================] - 38s 929us/step - loss: 1.3935 - val_loss: 3.2252\n",
      "Epoch 6/10\n",
      "40875/40875 [==============================] - 38s 926us/step - loss: 1.3598 - val_loss: 3.3637\n",
      "Epoch 7/10\n",
      "40875/40875 [==============================] - 37s 917us/step - loss: 1.3343 - val_loss: 3.3413\n",
      "Epoch 8/10\n",
      "40875/40875 [==============================] - 41s 1ms/step - loss: 1.3151 - val_loss: 3.4990\n",
      "Epoch 9/10\n",
      "40875/40875 [==============================] - 43s 1ms/step - loss: 1.2987 - val_loss: 3.2693\n",
      "Epoch 10/10\n",
      "40875/40875 [==============================] - 51s 1ms/step - loss: 1.2867 - val_loss: 3.4404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x33cdf9e40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106/5106 [==============================] - 2s 420us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.40      0.29      1499\n",
      "           1       0.10      0.13      0.11      1499\n",
      "           2       0.29      0.17      0.21      1499\n",
      "           3       0.69      0.41      0.51      1499\n",
      "           4       0.84      0.81      0.82      1499\n",
      "           5       0.55      0.60      0.57      1499\n",
      "           6       0.69      0.70      0.69      1499\n",
      "           7       0.70      0.40      0.51      1499\n",
      "           8       0.48      0.64      0.54      1499\n",
      "           9       0.42      0.65      0.51      1499\n",
      "          10       0.50      0.28      0.35      1499\n",
      "          11       0.49      0.79      0.60      1499\n",
      "          12       0.30      0.42      0.35      1499\n",
      "          13       0.20      0.24      0.21      1499\n",
      "          14       0.16      0.10      0.12      1499\n",
      "          15       0.62      0.51      0.56      1499\n",
      "          16       0.25      0.05      0.09      1499\n",
      "          17       0.69      0.44      0.54      1499\n",
      "          18       0.28      0.45      0.35      1499\n",
      "          19       0.27      0.11      0.16      1499\n",
      "          20       0.70      0.82      0.75      1499\n",
      "          21       0.30      0.42      0.35      1499\n",
      "          22       0.28      0.27      0.27      1499\n",
      "          23       0.69      0.85      0.76      1499\n",
      "          24       0.60      0.34      0.43      1499\n",
      "          25       0.52      0.22      0.31      1499\n",
      "          26       0.33      0.21      0.26      1499\n",
      "          27       0.09      0.09      0.09      1499\n",
      "          28       0.58      0.61      0.60      1499\n",
      "          29       0.38      0.71      0.50      1499\n",
      "          30       0.32      0.49      0.38      1499\n",
      "          31       0.21      0.32      0.25      1499\n",
      "          32       0.10      0.09      0.09      1499\n",
      "          33       0.22      0.29      0.25      1499\n",
      "          34       0.87      0.84      0.85      1499\n",
      "          35       0.64      0.79      0.71      1499\n",
      "          36       0.66      0.48      0.55      1499\n",
      "          37       0.40      0.27      0.32      1499\n",
      "          38       0.13      0.10      0.11      1499\n",
      "          39       0.34      0.38      0.36      1499\n",
      "          40       0.74      0.24      0.36      1499\n",
      "          41       0.83      0.57      0.68      1499\n",
      "          42       0.12      0.19      0.15      1499\n",
      "          43       0.44      0.22      0.29      1499\n",
      "          44       0.48      0.82      0.61      1499\n",
      "          45       0.19      0.21      0.20      1499\n",
      "          46       0.20      0.15      0.17      1499\n",
      "          47       0.78      0.45      0.57      1499\n",
      "          48       0.46      0.82      0.59      1499\n",
      "          49       0.33      0.25      0.29      1499\n",
      "          50       0.46      0.49      0.48      1499\n",
      "          51       0.34      0.19      0.24      1499\n",
      "          52       0.30      0.21      0.25      1499\n",
      "          53       0.17      0.14      0.16      1499\n",
      "          54       0.56      0.35      0.43      1499\n",
      "          55       0.46      0.44      0.45      1499\n",
      "          56       0.37      0.34      0.35      1499\n",
      "          57       0.73      0.64      0.68      1499\n",
      "          58       0.94      0.72      0.81      1499\n",
      "          59       0.16      0.22      0.18      1499\n",
      "          60       0.47      0.36      0.41      1499\n",
      "          61       0.27      0.41      0.33      1499\n",
      "          62       0.77      0.69      0.73      1499\n",
      "          63       0.10      0.06      0.08      1499\n",
      "          64       0.38      0.23      0.29      1499\n",
      "          65       0.42      0.26      0.32      1499\n",
      "          66       0.37      0.36      0.37      1499\n",
      "          67       0.15      0.05      0.08      1499\n",
      "          68       0.57      0.50      0.53      1499\n",
      "          69       0.21      0.35      0.26      1499\n",
      "          70       0.60      0.30      0.40      1499\n",
      "          71       0.24      0.19      0.21      1499\n",
      "          72       0.59      0.25      0.35      1499\n",
      "          73       0.30      0.26      0.28      1499\n",
      "          74       0.14      0.24      0.17      1499\n",
      "          75       0.83      0.92      0.88      1499\n",
      "          76       0.29      0.23      0.26      1499\n",
      "          77       0.38      0.57      0.45      1499\n",
      "          78       0.24      0.19      0.21      1499\n",
      "          79       0.36      0.45      0.40      1499\n",
      "          80       0.91      0.83      0.87      1499\n",
      "          81       0.39      0.29      0.33      1499\n",
      "          82       0.42      0.34      0.37      1499\n",
      "          83       0.23      0.09      0.13      1499\n",
      "          84       0.40      0.35      0.37      1499\n",
      "          85       0.36      0.42      0.39      1499\n",
      "          86       0.37      0.87      0.51      1499\n",
      "          87       0.08      0.03      0.05      1499\n",
      "          88       0.80      0.86      0.83      1499\n",
      "          89       0.30      0.25      0.27      1499\n",
      "          90       0.62      0.80      0.70      1499\n",
      "          91       0.36      0.48      0.41      1499\n",
      "          92       0.27      0.19      0.23      1499\n",
      "          93       0.87      0.76      0.81      1499\n",
      "          94       0.43      0.44      0.44      1499\n",
      "          95       0.07      0.07      0.07      1499\n",
      "          96       0.31      0.32      0.31      1499\n",
      "          97       0.15      0.56      0.23      1499\n",
      "          98       0.39      0.45      0.41      1499\n",
      "          99       0.55      0.70      0.62      1499\n",
      "         100       0.53      0.48      0.51      1499\n",
      "         101       0.27      0.31      0.29      1499\n",
      "         102       0.32      0.50      0.39      1499\n",
      "         103       0.83      0.21      0.34      1499\n",
      "         104       0.89      0.36      0.51      1499\n",
      "         105       0.17      0.11      0.13      1499\n",
      "         106       0.14      0.07      0.09      1499\n",
      "         107       0.28      0.21      0.24      1499\n",
      "         108       0.49      0.85      0.62      1499\n",
      "\n",
      "    accuracy                           0.40    163391\n",
      "   macro avg       0.42      0.40      0.39    163391\n",
      "weighted avg       0.42      0.40      0.39    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gk/c7h7qkyd3sn7d5qd8rlphgk80000gn/T/ipykernel_63563/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/var/folders/gk/c7h7qkyd3sn7d5qd8rlphgk80000gn/T/ipykernel_63563/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/var/folders/gk/c7h7qkyd3sn7d5qd8rlphgk80000gn/T/ipykernel_63563/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8f654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(109, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "300aa492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40875/40875 [==============================] - 46s 1ms/step - loss: 1.5920 - val_loss: 2.6741\n",
      "Epoch 2/10\n",
      "40875/40875 [==============================] - 48s 1ms/step - loss: 0.8376 - val_loss: 2.6282\n",
      "Epoch 3/10\n",
      "40875/40875 [==============================] - 53s 1ms/step - loss: 0.6869 - val_loss: 2.5394\n",
      "Epoch 4/10\n",
      "40875/40875 [==============================] - 50s 1ms/step - loss: 0.6111 - val_loss: 2.4157\n",
      "Epoch 5/10\n",
      "40875/40875 [==============================] - 52s 1ms/step - loss: 0.5653 - val_loss: 2.3471\n",
      "Epoch 6/10\n",
      "40875/40875 [==============================] - 55s 1ms/step - loss: 0.5337 - val_loss: 2.4478\n",
      "Epoch 7/10\n",
      "40875/40875 [==============================] - 56s 1ms/step - loss: 0.5086 - val_loss: 2.4176\n",
      "Epoch 8/10\n",
      "40875/40875 [==============================] - 56s 1ms/step - loss: 0.4879 - val_loss: 2.3727\n",
      "Epoch 9/10\n",
      "40875/40875 [==============================] - 55s 1ms/step - loss: 0.4698 - val_loss: 2.3888\n",
      "Epoch 10/10\n",
      "40875/40875 [==============================] - 61s 1ms/step - loss: 0.4538 - val_loss: 2.3467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x178f127d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106/5106 [==============================] - 3s 663us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.63      0.68      1499\n",
      "           1       0.26      0.18      0.21      1499\n",
      "           2       0.62      0.81      0.70      1499\n",
      "           3       0.76      0.30      0.43      1499\n",
      "           4       0.85      0.95      0.90      1499\n",
      "           5       0.90      0.91      0.90      1499\n",
      "           6       0.91      0.70      0.79      1499\n",
      "           7       0.70      0.57      0.63      1499\n",
      "           8       0.54      0.82      0.65      1499\n",
      "           9       0.85      0.52      0.65      1499\n",
      "          10       0.84      0.82      0.83      1499\n",
      "          11       0.91      0.92      0.91      1499\n",
      "          12       0.86      0.69      0.77      1499\n",
      "          13       0.61      0.70      0.65      1499\n",
      "          14       0.45      0.65      0.54      1499\n",
      "          15       0.78      0.52      0.63      1499\n",
      "          16       0.43      0.15      0.22      1499\n",
      "          17       0.69      0.86      0.77      1499\n",
      "          18       0.64      0.89      0.75      1499\n",
      "          19       0.40      0.44      0.42      1499\n",
      "          20       0.85      0.92      0.89      1499\n",
      "          21       0.71      0.63      0.67      1499\n",
      "          22       0.54      0.82      0.65      1499\n",
      "          23       0.91      0.94      0.92      1499\n",
      "          24       0.76      0.61      0.68      1499\n",
      "          25       0.86      0.46      0.60      1499\n",
      "          26       0.39      0.26      0.31      1499\n",
      "          27       0.52      0.26      0.34      1499\n",
      "          28       0.77      0.80      0.78      1499\n",
      "          29       0.72      0.80      0.76      1499\n",
      "          30       0.45      0.92      0.61      1499\n",
      "          31       0.47      0.62      0.53      1499\n",
      "          32       0.36      0.11      0.17      1499\n",
      "          33       0.51      0.94      0.66      1499\n",
      "          34       0.94      0.98      0.96      1499\n",
      "          35       0.84      0.88      0.86      1499\n",
      "          36       0.79      0.81      0.80      1499\n",
      "          37       0.75      0.52      0.62      1499\n",
      "          38       0.48      0.50      0.49      1499\n",
      "          39       0.34      0.59      0.43      1499\n",
      "          40       0.85      0.84      0.84      1499\n",
      "          41       0.78      0.65      0.71      1499\n",
      "          42       0.34      0.42      0.37      1499\n",
      "          43       0.80      0.82      0.81      1499\n",
      "          44       0.84      0.88      0.86      1499\n",
      "          45       0.48      0.38      0.42      1499\n",
      "          46       0.34      0.17      0.23      1499\n",
      "          47       0.67      0.60      0.63      1499\n",
      "          48       0.70      0.76      0.73      1499\n",
      "          49       0.69      0.46      0.56      1499\n",
      "          50       0.66      0.76      0.70      1499\n",
      "          51       0.58      0.36      0.45      1499\n",
      "          52       0.66      0.77      0.71      1499\n",
      "          53       0.58      0.74      0.65      1499\n",
      "          54       0.77      0.87      0.82      1499\n",
      "          55       0.62      0.78      0.69      1499\n",
      "          56       0.65      0.41      0.50      1499\n",
      "          57       0.88      0.84      0.86      1499\n",
      "          58       0.87      0.68      0.77      1499\n",
      "          59       0.41      0.46      0.44      1499\n",
      "          60       0.71      0.71      0.71      1499\n",
      "          61       0.82      0.75      0.78      1499\n",
      "          62       0.92      0.91      0.91      1499\n",
      "          63       0.53      0.27      0.36      1499\n",
      "          64       0.38      0.52      0.44      1499\n",
      "          65       0.85      0.62      0.71      1499\n",
      "          66       0.83      0.86      0.84      1499\n",
      "          67       0.80      0.58      0.67      1499\n",
      "          68       0.74      0.74      0.74      1499\n",
      "          69       0.33      0.49      0.39      1499\n",
      "          70       0.67      0.72      0.69      1499\n",
      "          71       0.52      0.38      0.44      1499\n",
      "          72       0.62      0.71      0.66      1499\n",
      "          73       0.64      0.48      0.55      1499\n",
      "          74       0.35      0.53      0.42      1499\n",
      "          75       0.87      0.93      0.90      1499\n",
      "          76       0.50      0.41      0.45      1499\n",
      "          77       0.68      0.67      0.68      1499\n",
      "          78       0.44      0.73      0.55      1499\n",
      "          79       0.70      0.75      0.72      1499\n",
      "          80       0.98      0.86      0.92      1499\n",
      "          81       0.74      0.41      0.53      1499\n",
      "          82       0.58      0.79      0.67      1499\n",
      "          83       0.72      0.68      0.70      1499\n",
      "          84       0.55      0.68      0.61      1499\n",
      "          85       0.79      0.60      0.68      1499\n",
      "          86       0.58      0.83      0.69      1499\n",
      "          87       0.33      0.29      0.31      1499\n",
      "          88       0.97      0.91      0.94      1499\n",
      "          89       0.80      0.58      0.67      1499\n",
      "          90       0.93      0.92      0.93      1499\n",
      "          91       0.67      0.74      0.70      1499\n",
      "          92       0.39      0.36      0.37      1499\n",
      "          93       1.00      0.92      0.96      1499\n",
      "          94       0.51      0.77      0.61      1499\n",
      "          95       0.24      0.06      0.10      1499\n",
      "          96       0.60      0.90      0.72      1499\n",
      "          97       0.54      0.68      0.60      1499\n",
      "          98       0.81      0.54      0.65      1499\n",
      "          99       0.72      0.71      0.72      1499\n",
      "         100       0.52      0.56      0.54      1499\n",
      "         101       0.52      0.52      0.52      1499\n",
      "         102       0.23      0.35      0.28      1499\n",
      "         103       0.91      0.49      0.64      1499\n",
      "         104       0.84      0.66      0.74      1499\n",
      "         105       0.14      0.09      0.11      1499\n",
      "         106       0.57      0.54      0.56      1499\n",
      "         107       0.45      0.60      0.52      1499\n",
      "         108       0.89      0.84      0.86      1499\n",
      "\n",
      "    accuracy                           0.64    163391\n",
      "   macro avg       0.65      0.64      0.63    163391\n",
      "weighted avg       0.65      0.64      0.63    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1dad4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(109, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3361defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40875/40875 [==============================] - 62s 2ms/step - loss: 1.2391 - val_loss: 2.6627\n",
      "Epoch 2/10\n",
      "40875/40875 [==============================] - 69s 2ms/step - loss: 0.5587 - val_loss: 2.5208\n",
      "Epoch 3/10\n",
      "40875/40875 [==============================] - 64s 2ms/step - loss: 0.4044 - val_loss: 2.2964\n",
      "Epoch 4/10\n",
      "40875/40875 [==============================] - 66s 2ms/step - loss: 0.3271 - val_loss: 2.2771\n",
      "Epoch 5/10\n",
      "40875/40875 [==============================] - 65s 2ms/step - loss: 0.2807 - val_loss: 2.3602\n",
      "Epoch 6/10\n",
      "40875/40875 [==============================] - 62s 2ms/step - loss: 0.2488 - val_loss: 2.2493\n",
      "Epoch 7/10\n",
      "40875/40875 [==============================] - 75s 2ms/step - loss: 0.2253 - val_loss: 2.0812\n",
      "Epoch 8/10\n",
      "40875/40875 [==============================] - 76s 2ms/step - loss: 0.2062 - val_loss: 1.9723\n",
      "Epoch 9/10\n",
      "40875/40875 [==============================] - 64s 2ms/step - loss: 0.1909 - val_loss: 2.0480\n",
      "Epoch 10/10\n",
      "40875/40875 [==============================] - 68s 2ms/step - loss: 0.1791 - val_loss: 2.2008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x33d8a0430>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5106/5106 [==============================] - 4s 689us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84      1499\n",
      "           1       0.56      0.45      0.50      1499\n",
      "           2       0.73      0.85      0.78      1499\n",
      "           3       0.81      0.94      0.87      1499\n",
      "           4       0.96      0.99      0.97      1499\n",
      "           5       0.93      0.97      0.95      1499\n",
      "           6       0.92      0.87      0.89      1499\n",
      "           7       0.88      0.62      0.72      1499\n",
      "           8       0.72      0.85      0.78      1499\n",
      "           9       0.81      0.88      0.85      1499\n",
      "          10       0.68      0.72      0.70      1499\n",
      "          11       0.96      0.99      0.97      1499\n",
      "          12       0.92      0.82      0.87      1499\n",
      "          13       0.75      0.77      0.76      1499\n",
      "          14       0.78      0.80      0.79      1499\n",
      "          15       0.71      0.94      0.81      1499\n",
      "          16       0.84      0.87      0.86      1499\n",
      "          17       0.83      0.88      0.85      1499\n",
      "          18       0.85      0.87      0.86      1499\n",
      "          19       0.47      0.72      0.57      1499\n",
      "          20       0.98      0.96      0.97      1499\n",
      "          21       0.70      0.84      0.77      1499\n",
      "          22       0.79      0.84      0.82      1499\n",
      "          23       0.94      1.00      0.97      1499\n",
      "          24       0.91      0.83      0.87      1499\n",
      "          25       0.78      0.33      0.46      1499\n",
      "          26       0.75      0.39      0.52      1499\n",
      "          27       0.80      0.31      0.45      1499\n",
      "          28       0.79      0.92      0.85      1499\n",
      "          29       0.96      0.84      0.89      1499\n",
      "          30       0.87      0.98      0.92      1499\n",
      "          31       0.85      0.90      0.87      1499\n",
      "          32       0.77      0.50      0.61      1499\n",
      "          33       0.78      0.87      0.82      1499\n",
      "          34       0.98      0.99      0.99      1499\n",
      "          35       0.88      0.98      0.93      1499\n",
      "          36       0.73      0.87      0.80      1499\n",
      "          37       0.85      0.88      0.86      1499\n",
      "          38       0.77      0.37      0.50      1499\n",
      "          39       0.52      0.59      0.55      1499\n",
      "          40       0.93      0.74      0.82      1499\n",
      "          41       0.72      0.60      0.66      1499\n",
      "          42       0.44      0.81      0.57      1499\n",
      "          43       0.91      0.92      0.91      1499\n",
      "          44       0.96      0.97      0.96      1499\n",
      "          45       0.64      0.47      0.54      1499\n",
      "          46       0.19      0.23      0.21      1499\n",
      "          47       0.76      0.73      0.74      1499\n",
      "          48       0.79      0.94      0.86      1499\n",
      "          49       0.67      0.74      0.71      1499\n",
      "          50       0.87      0.59      0.70      1499\n",
      "          51       0.54      0.71      0.62      1499\n",
      "          52       0.81      0.95      0.87      1499\n",
      "          53       0.54      0.80      0.64      1499\n",
      "          54       0.86      0.90      0.88      1499\n",
      "          55       0.86      0.89      0.88      1499\n",
      "          56       0.67      0.79      0.73      1499\n",
      "          57       0.95      0.96      0.95      1499\n",
      "          58       0.82      0.67      0.74      1499\n",
      "          59       0.54      0.48      0.51      1499\n",
      "          60       0.83      0.74      0.78      1499\n",
      "          61       0.93      0.62      0.74      1499\n",
      "          62       0.97      0.93      0.95      1499\n",
      "          63       0.33      0.43      0.38      1499\n",
      "          64       0.53      0.85      0.66      1499\n",
      "          65       0.91      0.89      0.90      1499\n",
      "          66       0.88      0.81      0.84      1499\n",
      "          67       0.74      0.52      0.61      1499\n",
      "          68       0.94      0.90      0.92      1499\n",
      "          69       0.59      0.57      0.58      1499\n",
      "          70       0.83      0.64      0.73      1499\n",
      "          71       0.78      0.71      0.75      1499\n",
      "          72       0.73      0.55      0.63      1499\n",
      "          73       0.85      0.86      0.86      1499\n",
      "          74       0.33      0.75      0.46      1499\n",
      "          75       0.96      0.87      0.91      1499\n",
      "          76       0.67      0.43      0.53      1499\n",
      "          77       0.89      0.73      0.80      1499\n",
      "          78       0.69      0.74      0.71      1499\n",
      "          79       0.81      0.97      0.88      1499\n",
      "          80       0.88      0.87      0.88      1499\n",
      "          81       0.67      0.62      0.65      1499\n",
      "          82       0.77      0.91      0.83      1499\n",
      "          83       0.53      0.41      0.46      1499\n",
      "          84       0.78      0.84      0.81      1499\n",
      "          85       0.71      0.78      0.75      1499\n",
      "          86       0.89      0.82      0.85      1499\n",
      "          87       0.57      0.67      0.62      1499\n",
      "          88       0.95      0.98      0.97      1499\n",
      "          89       0.91      0.84      0.87      1499\n",
      "          90       0.95      0.97      0.96      1499\n",
      "          91       0.82      0.95      0.88      1499\n",
      "          92       0.69      0.62      0.65      1499\n",
      "          93       1.00      0.93      0.96      1499\n",
      "          94       0.71      0.69      0.70      1499\n",
      "          95       0.42      0.16      0.23      1499\n",
      "          96       0.90      0.87      0.88      1499\n",
      "          97       0.79      0.86      0.82      1499\n",
      "          98       0.85      0.81      0.83      1499\n",
      "          99       0.90      0.82      0.86      1499\n",
      "         100       0.64      0.78      0.70      1499\n",
      "         101       0.40      0.58      0.48      1499\n",
      "         102       0.72      0.48      0.58      1499\n",
      "         103       0.55      0.09      0.16      1499\n",
      "         104       0.97      0.60      0.74      1499\n",
      "         105       0.65      0.32      0.43      1499\n",
      "         106       0.85      0.83      0.84      1499\n",
      "         107       0.41      0.89      0.56      1499\n",
      "         108       0.89      0.74      0.81      1499\n",
      "\n",
      "    accuracy                           0.75    163391\n",
      "   macro avg       0.77      0.75      0.75    163391\n",
      "weighted avg       0.77      0.75      0.75    163391\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ab267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 50 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 07:44:44.435345: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 07:44:44.469701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 07:44:44.469748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 07:44:44.470972: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 07:44:44.476952: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 07:44:44.477901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 07:44:45.512716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081a3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mne\n",
      "  Obtaining dependency information for mne from https://files.pythonhosted.org/packages/6d/72/7d8ce176e042580a6261c7af0af6d4bbd3b495d304d8c73a229365f52ede/mne-1.5.1-py3-none-any.whl.metadata\n",
      "  Downloading mne-1.5.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.6.3 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (1.11.2)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (3.7.3)\n",
      "Collecting tqdm (from mne)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m655.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pooch>=1.5 (from mne)\n",
      "  Downloading pooch-1.7.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator in /home/codespace/.local/lib/python3.10/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from mne) (23.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from pooch>=1.5->mne) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from pooch>=1.5->mne) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->mne) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2023.7.22)\n",
      "Downloading mne-1.5.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, pooch, mne\n",
      "Successfully installed mne-1.5.1 pooch-1.7.0 tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/S001R03.edf',\n",
       " 'files/S002R03.edf',\n",
       " 'files/S003R03.edf',\n",
       " 'files/S004R03.edf',\n",
       " 'files/S005R03.edf',\n",
       " 'files/S006R03.edf',\n",
       " 'files/S007R03.edf',\n",
       " 'files/S008R03.edf',\n",
       " 'files/S009R03.edf',\n",
       " 'files/S010R03.edf',\n",
       " 'files/S011R03.edf',\n",
       " 'files/S012R03.edf',\n",
       " 'files/S013R03.edf',\n",
       " 'files/S014R03.edf',\n",
       " 'files/S015R03.edf',\n",
       " 'files/S016R03.edf',\n",
       " 'files/S017R03.edf',\n",
       " 'files/S018R03.edf',\n",
       " 'files/S019R03.edf',\n",
       " 'files/S020R03.edf',\n",
       " 'files/S021R03.edf',\n",
       " 'files/S022R03.edf',\n",
       " 'files/S023R03.edf',\n",
       " 'files/S024R03.edf',\n",
       " 'files/S025R03.edf',\n",
       " 'files/S026R03.edf',\n",
       " 'files/S027R03.edf',\n",
       " 'files/S028R03.edf',\n",
       " 'files/S029R03.edf',\n",
       " 'files/S030R03.edf',\n",
       " 'files/S031R03.edf',\n",
       " 'files/S032R03.edf',\n",
       " 'files/S033R03.edf',\n",
       " 'files/S034R03.edf',\n",
       " 'files/S035R03.edf',\n",
       " 'files/S036R03.edf',\n",
       " 'files/S037R03.edf',\n",
       " 'files/S038R03.edf',\n",
       " 'files/S039R03.edf',\n",
       " 'files/S040R03.edf',\n",
       " 'files/S041R03.edf',\n",
       " 'files/S042R03.edf',\n",
       " 'files/S043R03.edf',\n",
       " 'files/S044R03.edf',\n",
       " 'files/S045R03.edf',\n",
       " 'files/S046R03.edf',\n",
       " 'files/S047R03.edf',\n",
       " 'files/S048R03.edf',\n",
       " 'files/S049R03.edf',\n",
       " 'files/S050R03.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 74950, 600000, 74950)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.7e-05 -4.5e-05 -2.9e-05 ...  9.0e-06  4.0e-06  1.6e-05]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000057  -0.000013  -0.000015  -0.000012  -0.000013  -0.000008   \n",
       "1       -0.000049  -0.000011  -0.000010  -0.000012  -0.000019  -0.000024   \n",
       "2       -0.000055  -0.000017  -0.000016  -0.000019  -0.000024  -0.000029   \n",
       "3       -0.000073  -0.000042  -0.000040  -0.000037  -0.000037  -0.000040   \n",
       "4       -0.000087  -0.000053  -0.000052  -0.000051  -0.000045  -0.000043   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995   0.000025   0.000034   0.000019   0.000012   0.000017   0.000013   \n",
       "599996   0.000061   0.000068   0.000057   0.000048   0.000050   0.000044   \n",
       "599997   0.000066   0.000065   0.000059   0.000050   0.000051   0.000051   \n",
       "599998   0.000081   0.000087   0.000066   0.000059   0.000069   0.000060   \n",
       "599999   0.000102   0.000097   0.000083   0.000072   0.000071   0.000058   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000040  -0.000054  -0.000012  -0.000014  ...   -0.000048   \n",
       "1       -0.000058  -0.000051  -0.000019  -0.000023  ...   -0.000055   \n",
       "2       -0.000066  -0.000061  -0.000030  -0.000036  ...   -0.000054   \n",
       "3       -0.000071  -0.000078  -0.000053  -0.000053  ...   -0.000065   \n",
       "4       -0.000071  -0.000087  -0.000065  -0.000064  ...   -0.000075   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995   0.000018   0.000029   0.000014   0.000010  ...    0.000020   \n",
       "599996   0.000043   0.000048   0.000049   0.000043  ...    0.000013   \n",
       "599997   0.000054   0.000031   0.000040   0.000035  ...   -0.000002   \n",
       "599998   0.000064   0.000054   0.000043   0.000041  ...    0.000011   \n",
       "599999   0.000047   0.000064   0.000065   0.000058  ...    0.000002   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000038   -0.000042   -0.000068   -0.000076   -0.000103   \n",
       "1        -0.000055   -0.000063   -0.000082   -0.000087   -0.000099   \n",
       "2        -0.000063   -0.000072   -0.000091   -0.000092   -0.000091   \n",
       "3        -0.000052   -0.000066   -0.000100   -0.000105   -0.000105   \n",
       "4        -0.000082   -0.000090   -0.000117   -0.000119   -0.000118   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995    0.000008   -0.000002    0.000019    0.000013    0.000005   \n",
       "599996    0.000010   -0.000007    0.000011    0.000012    0.000003   \n",
       "599997   -0.000025   -0.000032   -0.000006   -0.000002   -0.000013   \n",
       "599998    0.000000   -0.000006    0.000014    0.000014   -0.000003   \n",
       "599999   -0.000004   -0.000011    0.000010   -0.000001   -0.000025   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000051   -0.000056   -0.000124   -0.000028  \n",
       "1        -0.000059   -0.000070   -0.000149   -0.000040  \n",
       "2        -0.000067   -0.000077   -0.000153   -0.000037  \n",
       "3        -0.000067   -0.000072   -0.000148   -0.000026  \n",
       "4        -0.000075   -0.000082   -0.000161   -0.000035  \n",
       "...            ...         ...         ...         ...  \n",
       "599995    0.000013    0.000007    0.000006   -0.000060  \n",
       "599996    0.000013    0.000020    0.000016   -0.000027  \n",
       "599997   -0.000019   -0.000010   -0.000002   -0.000059  \n",
       "599998    0.000005    0.000000    0.000005   -0.000064  \n",
       "599999    0.000000   -0.000018   -0.000020   -0.000087  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37312/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_37312/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_37312/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000057  -0.000013  -0.000015  -0.000012  -0.000013  -0.000008   \n",
       "1       -0.000049  -0.000011  -0.000010  -0.000012  -0.000019  -0.000024   \n",
       "2       -0.000055  -0.000017  -0.000016  -0.000019  -0.000024  -0.000029   \n",
       "3       -0.000073  -0.000042  -0.000040  -0.000037  -0.000037  -0.000040   \n",
       "4       -0.000087  -0.000053  -0.000052  -0.000051  -0.000045  -0.000043   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995   0.000025   0.000034   0.000019   0.000012   0.000017   0.000013   \n",
       "599996   0.000061   0.000068   0.000057   0.000048   0.000050   0.000044   \n",
       "599997   0.000066   0.000065   0.000059   0.000050   0.000051   0.000051   \n",
       "599998   0.000081   0.000087   0.000066   0.000059   0.000069   0.000060   \n",
       "599999   0.000102   0.000097   0.000083   0.000072   0.000071   0.000058   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000040  -0.000054  -0.000012  -0.000014  ...   -0.000048   \n",
       "1       -0.000058  -0.000051  -0.000019  -0.000023  ...   -0.000055   \n",
       "2       -0.000066  -0.000061  -0.000030  -0.000036  ...   -0.000054   \n",
       "3       -0.000071  -0.000078  -0.000053  -0.000053  ...   -0.000065   \n",
       "4       -0.000071  -0.000087  -0.000065  -0.000064  ...   -0.000075   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995   0.000018   0.000029   0.000014   0.000010  ...    0.000020   \n",
       "599996   0.000043   0.000048   0.000049   0.000043  ...    0.000013   \n",
       "599997   0.000054   0.000031   0.000040   0.000035  ...   -0.000002   \n",
       "599998   0.000064   0.000054   0.000043   0.000041  ...    0.000011   \n",
       "599999   0.000047   0.000064   0.000065   0.000058  ...    0.000002   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000038   -0.000042   -0.000068   -0.000076   -0.000103   \n",
       "1        -0.000055   -0.000063   -0.000082   -0.000087   -0.000099   \n",
       "2        -0.000063   -0.000072   -0.000091   -0.000092   -0.000091   \n",
       "3        -0.000052   -0.000066   -0.000100   -0.000105   -0.000105   \n",
       "4        -0.000082   -0.000090   -0.000117   -0.000119   -0.000118   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995    0.000008   -0.000002    0.000019    0.000013    0.000005   \n",
       "599996    0.000010   -0.000007    0.000011    0.000012    0.000003   \n",
       "599997   -0.000025   -0.000032   -0.000006   -0.000002   -0.000013   \n",
       "599998    0.000000   -0.000006    0.000014    0.000014   -0.000003   \n",
       "599999   -0.000004   -0.000011    0.000010   -0.000001   -0.000025   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000051   -0.000056   -0.000124   -0.000028  \n",
       "1        -0.000059   -0.000070   -0.000149   -0.000040  \n",
       "2        -0.000067   -0.000077   -0.000153   -0.000037  \n",
       "3        -0.000067   -0.000072   -0.000148   -0.000026  \n",
       "4        -0.000075   -0.000082   -0.000161   -0.000035  \n",
       "...            ...         ...         ...         ...  \n",
       "599995    0.000013    0.000007    0.000006   -0.000060  \n",
       "599996    0.000013    0.000020    0.000016   -0.000027  \n",
       "599997   -0.000019   -0.000010   -0.000002   -0.000059  \n",
       "599998    0.000005    0.000000    0.000005   -0.000064  \n",
       "599999    0.000000   -0.000018   -0.000020   -0.000087  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.65619\n",
      "[1]\tvalidation_0-mlogloss:3.53640\n",
      "[2]\tvalidation_0-mlogloss:3.45357\n",
      "[3]\tvalidation_0-mlogloss:3.38939\n",
      "[4]\tvalidation_0-mlogloss:3.33728\n",
      "[5]\tvalidation_0-mlogloss:3.29218\n",
      "[6]\tvalidation_0-mlogloss:3.25401\n",
      "[7]\tvalidation_0-mlogloss:3.22164\n",
      "[8]\tvalidation_0-mlogloss:3.19184\n",
      "[9]\tvalidation_0-mlogloss:3.16235\n",
      "[10]\tvalidation_0-mlogloss:3.13872\n",
      "[11]\tvalidation_0-mlogloss:3.11597\n",
      "[12]\tvalidation_0-mlogloss:3.09707\n",
      "[13]\tvalidation_0-mlogloss:3.07800\n",
      "[14]\tvalidation_0-mlogloss:3.06319\n",
      "[15]\tvalidation_0-mlogloss:3.04546\n",
      "[16]\tvalidation_0-mlogloss:3.02813\n",
      "[17]\tvalidation_0-mlogloss:3.01325\n",
      "[18]\tvalidation_0-mlogloss:2.99873\n",
      "[19]\tvalidation_0-mlogloss:2.98326\n",
      "[20]\tvalidation_0-mlogloss:2.97057\n",
      "[21]\tvalidation_0-mlogloss:2.95850\n",
      "[22]\tvalidation_0-mlogloss:2.94614\n",
      "[23]\tvalidation_0-mlogloss:2.93830\n",
      "[24]\tvalidation_0-mlogloss:2.92912\n",
      "[25]\tvalidation_0-mlogloss:2.91964\n",
      "[26]\tvalidation_0-mlogloss:2.90740\n",
      "[27]\tvalidation_0-mlogloss:2.89832\n",
      "[28]\tvalidation_0-mlogloss:2.88777\n",
      "[29]\tvalidation_0-mlogloss:2.87616\n",
      "[30]\tvalidation_0-mlogloss:2.86645\n",
      "[31]\tvalidation_0-mlogloss:2.85877\n",
      "[32]\tvalidation_0-mlogloss:2.85083\n",
      "[33]\tvalidation_0-mlogloss:2.84083\n",
      "[34]\tvalidation_0-mlogloss:2.83229\n",
      "[35]\tvalidation_0-mlogloss:2.82428\n",
      "[36]\tvalidation_0-mlogloss:2.81383\n",
      "[37]\tvalidation_0-mlogloss:2.80620\n",
      "[38]\tvalidation_0-mlogloss:2.79687\n",
      "[39]\tvalidation_0-mlogloss:2.78810\n",
      "[40]\tvalidation_0-mlogloss:2.78023\n",
      "[41]\tvalidation_0-mlogloss:2.77279\n",
      "[42]\tvalidation_0-mlogloss:2.76589\n",
      "[43]\tvalidation_0-mlogloss:2.75928\n",
      "[44]\tvalidation_0-mlogloss:2.75188\n",
      "[45]\tvalidation_0-mlogloss:2.74386\n",
      "[46]\tvalidation_0-mlogloss:2.73809\n",
      "[47]\tvalidation_0-mlogloss:2.73181\n",
      "[48]\tvalidation_0-mlogloss:2.72424\n",
      "[49]\tvalidation_0-mlogloss:2.71835\n",
      "[50]\tvalidation_0-mlogloss:2.71192\n",
      "[51]\tvalidation_0-mlogloss:2.70718\n",
      "[52]\tvalidation_0-mlogloss:2.70196\n",
      "[53]\tvalidation_0-mlogloss:2.69831\n",
      "[54]\tvalidation_0-mlogloss:2.69266\n",
      "[55]\tvalidation_0-mlogloss:2.68673\n",
      "[56]\tvalidation_0-mlogloss:2.68121\n",
      "[57]\tvalidation_0-mlogloss:2.67619\n",
      "[58]\tvalidation_0-mlogloss:2.67115\n",
      "[59]\tvalidation_0-mlogloss:2.66658\n",
      "[60]\tvalidation_0-mlogloss:2.66063\n",
      "[61]\tvalidation_0-mlogloss:2.65683\n",
      "[62]\tvalidation_0-mlogloss:2.65090\n",
      "[63]\tvalidation_0-mlogloss:2.64564\n",
      "[64]\tvalidation_0-mlogloss:2.64202\n",
      "[65]\tvalidation_0-mlogloss:2.63707\n",
      "[66]\tvalidation_0-mlogloss:2.63318\n",
      "[67]\tvalidation_0-mlogloss:2.62908\n",
      "[68]\tvalidation_0-mlogloss:2.62522\n",
      "[69]\tvalidation_0-mlogloss:2.62125\n",
      "[70]\tvalidation_0-mlogloss:2.61777\n",
      "[71]\tvalidation_0-mlogloss:2.61315\n",
      "[72]\tvalidation_0-mlogloss:2.60866\n",
      "[73]\tvalidation_0-mlogloss:2.60427\n",
      "[74]\tvalidation_0-mlogloss:2.59929\n",
      "[75]\tvalidation_0-mlogloss:2.59478\n",
      "[76]\tvalidation_0-mlogloss:2.59160\n",
      "[77]\tvalidation_0-mlogloss:2.58858\n",
      "[78]\tvalidation_0-mlogloss:2.58638\n",
      "[79]\tvalidation_0-mlogloss:2.58236\n",
      "[80]\tvalidation_0-mlogloss:2.57871\n",
      "[81]\tvalidation_0-mlogloss:2.57552\n",
      "[82]\tvalidation_0-mlogloss:2.57318\n",
      "[83]\tvalidation_0-mlogloss:2.56954\n",
      "[84]\tvalidation_0-mlogloss:2.56534\n",
      "[85]\tvalidation_0-mlogloss:2.56197\n",
      "[86]\tvalidation_0-mlogloss:2.55761\n",
      "[87]\tvalidation_0-mlogloss:2.55404\n",
      "[88]\tvalidation_0-mlogloss:2.55129\n",
      "[89]\tvalidation_0-mlogloss:2.54883\n",
      "[90]\tvalidation_0-mlogloss:2.54576\n",
      "[91]\tvalidation_0-mlogloss:2.54337\n",
      "[92]\tvalidation_0-mlogloss:2.54014\n",
      "[93]\tvalidation_0-mlogloss:2.53699\n",
      "[94]\tvalidation_0-mlogloss:2.53313\n",
      "[95]\tvalidation_0-mlogloss:2.52888\n",
      "[96]\tvalidation_0-mlogloss:2.52604\n",
      "[97]\tvalidation_0-mlogloss:2.52403\n",
      "[98]\tvalidation_0-mlogloss:2.52191\n",
      "[99]\tvalidation_0-mlogloss:2.51964\n",
      "[100]\tvalidation_0-mlogloss:2.51650\n",
      "[101]\tvalidation_0-mlogloss:2.51358\n",
      "[102]\tvalidation_0-mlogloss:2.51142\n",
      "[103]\tvalidation_0-mlogloss:2.50742\n",
      "[104]\tvalidation_0-mlogloss:2.50475\n",
      "[105]\tvalidation_0-mlogloss:2.50312\n",
      "[106]\tvalidation_0-mlogloss:2.50101\n",
      "[107]\tvalidation_0-mlogloss:2.49919\n",
      "[108]\tvalidation_0-mlogloss:2.49695\n",
      "[109]\tvalidation_0-mlogloss:2.49458\n",
      "[110]\tvalidation_0-mlogloss:2.49308\n",
      "[111]\tvalidation_0-mlogloss:2.49088\n",
      "[112]\tvalidation_0-mlogloss:2.48870\n",
      "[113]\tvalidation_0-mlogloss:2.48669\n",
      "[114]\tvalidation_0-mlogloss:2.48514\n",
      "[115]\tvalidation_0-mlogloss:2.48226\n",
      "[116]\tvalidation_0-mlogloss:2.48034\n",
      "[117]\tvalidation_0-mlogloss:2.47847\n",
      "[118]\tvalidation_0-mlogloss:2.47647\n",
      "[119]\tvalidation_0-mlogloss:2.47473\n",
      "[120]\tvalidation_0-mlogloss:2.47307\n",
      "[121]\tvalidation_0-mlogloss:2.47028\n",
      "[122]\tvalidation_0-mlogloss:2.46809\n",
      "[123]\tvalidation_0-mlogloss:2.46599\n",
      "[124]\tvalidation_0-mlogloss:2.46469\n",
      "[125]\tvalidation_0-mlogloss:2.46311\n",
      "[126]\tvalidation_0-mlogloss:2.46081\n",
      "[127]\tvalidation_0-mlogloss:2.45885\n",
      "[128]\tvalidation_0-mlogloss:2.45703\n",
      "[129]\tvalidation_0-mlogloss:2.45536\n",
      "[130]\tvalidation_0-mlogloss:2.45341\n",
      "[131]\tvalidation_0-mlogloss:2.45132\n",
      "[132]\tvalidation_0-mlogloss:2.44981\n",
      "[133]\tvalidation_0-mlogloss:2.44797\n",
      "[134]\tvalidation_0-mlogloss:2.44616\n",
      "[135]\tvalidation_0-mlogloss:2.44345\n",
      "[136]\tvalidation_0-mlogloss:2.44190\n",
      "[137]\tvalidation_0-mlogloss:2.44083\n",
      "[138]\tvalidation_0-mlogloss:2.43963\n",
      "[139]\tvalidation_0-mlogloss:2.43849\n",
      "[140]\tvalidation_0-mlogloss:2.43702\n",
      "[141]\tvalidation_0-mlogloss:2.43587\n",
      "[142]\tvalidation_0-mlogloss:2.43478\n",
      "[143]\tvalidation_0-mlogloss:2.43339\n",
      "[144]\tvalidation_0-mlogloss:2.43266\n",
      "[145]\tvalidation_0-mlogloss:2.43077\n",
      "[146]\tvalidation_0-mlogloss:2.42890\n",
      "[147]\tvalidation_0-mlogloss:2.42666\n",
      "[148]\tvalidation_0-mlogloss:2.42427\n",
      "[149]\tvalidation_0-mlogloss:2.42280\n",
      "[150]\tvalidation_0-mlogloss:2.42109\n",
      "[151]\tvalidation_0-mlogloss:2.41884\n",
      "[152]\tvalidation_0-mlogloss:2.41759\n",
      "[153]\tvalidation_0-mlogloss:2.41559\n",
      "[154]\tvalidation_0-mlogloss:2.41431\n",
      "[155]\tvalidation_0-mlogloss:2.41276\n",
      "[156]\tvalidation_0-mlogloss:2.41128\n",
      "[157]\tvalidation_0-mlogloss:2.41079\n",
      "[158]\tvalidation_0-mlogloss:2.40966\n",
      "[159]\tvalidation_0-mlogloss:2.40832\n",
      "[160]\tvalidation_0-mlogloss:2.40693\n",
      "[161]\tvalidation_0-mlogloss:2.40507\n",
      "[162]\tvalidation_0-mlogloss:2.40449\n",
      "[163]\tvalidation_0-mlogloss:2.40303\n",
      "[164]\tvalidation_0-mlogloss:2.40187\n",
      "[165]\tvalidation_0-mlogloss:2.40056\n",
      "[166]\tvalidation_0-mlogloss:2.39966\n",
      "[167]\tvalidation_0-mlogloss:2.39882\n",
      "[168]\tvalidation_0-mlogloss:2.39847\n",
      "[169]\tvalidation_0-mlogloss:2.39768\n",
      "[170]\tvalidation_0-mlogloss:2.39683\n",
      "[171]\tvalidation_0-mlogloss:2.39553\n",
      "[172]\tvalidation_0-mlogloss:2.39432\n",
      "[173]\tvalidation_0-mlogloss:2.39294\n",
      "[174]\tvalidation_0-mlogloss:2.39165\n",
      "[175]\tvalidation_0-mlogloss:2.39020\n",
      "[176]\tvalidation_0-mlogloss:2.38893\n",
      "[177]\tvalidation_0-mlogloss:2.38790\n",
      "[178]\tvalidation_0-mlogloss:2.38748\n",
      "[179]\tvalidation_0-mlogloss:2.38664\n",
      "[180]\tvalidation_0-mlogloss:2.38598\n",
      "[181]\tvalidation_0-mlogloss:2.38491\n",
      "[182]\tvalidation_0-mlogloss:2.38423\n",
      "[183]\tvalidation_0-mlogloss:2.38346\n",
      "[184]\tvalidation_0-mlogloss:2.38185\n",
      "[185]\tvalidation_0-mlogloss:2.38063\n",
      "[186]\tvalidation_0-mlogloss:2.38032\n",
      "[187]\tvalidation_0-mlogloss:2.37944\n",
      "[188]\tvalidation_0-mlogloss:2.37937\n",
      "[189]\tvalidation_0-mlogloss:2.37833\n",
      "[190]\tvalidation_0-mlogloss:2.37721\n",
      "[191]\tvalidation_0-mlogloss:2.37668\n",
      "[192]\tvalidation_0-mlogloss:2.37586\n",
      "[193]\tvalidation_0-mlogloss:2.37466\n",
      "[194]\tvalidation_0-mlogloss:2.37338\n",
      "[195]\tvalidation_0-mlogloss:2.37258\n",
      "[196]\tvalidation_0-mlogloss:2.37183\n",
      "[197]\tvalidation_0-mlogloss:2.37135\n",
      "[198]\tvalidation_0-mlogloss:2.37054\n",
      "[199]\tvalidation_0-mlogloss:2.36966\n",
      "[200]\tvalidation_0-mlogloss:2.36923\n",
      "[201]\tvalidation_0-mlogloss:2.36856\n",
      "[202]\tvalidation_0-mlogloss:2.36785\n",
      "[203]\tvalidation_0-mlogloss:2.36731\n",
      "[204]\tvalidation_0-mlogloss:2.36639\n",
      "[205]\tvalidation_0-mlogloss:2.36601\n",
      "[206]\tvalidation_0-mlogloss:2.36506\n",
      "[207]\tvalidation_0-mlogloss:2.36529\n",
      "[208]\tvalidation_0-mlogloss:2.36482\n",
      "[209]\tvalidation_0-mlogloss:2.36359\n",
      "[210]\tvalidation_0-mlogloss:2.36256\n",
      "[211]\tvalidation_0-mlogloss:2.36207\n",
      "[212]\tvalidation_0-mlogloss:2.36121\n",
      "[213]\tvalidation_0-mlogloss:2.36071\n",
      "[214]\tvalidation_0-mlogloss:2.35950\n",
      "[215]\tvalidation_0-mlogloss:2.35914\n",
      "[216]\tvalidation_0-mlogloss:2.35863\n",
      "[217]\tvalidation_0-mlogloss:2.35812\n",
      "[218]\tvalidation_0-mlogloss:2.35737\n",
      "[219]\tvalidation_0-mlogloss:2.35693\n",
      "[220]\tvalidation_0-mlogloss:2.35591\n",
      "[221]\tvalidation_0-mlogloss:2.35568\n",
      "[222]\tvalidation_0-mlogloss:2.35468\n",
      "[223]\tvalidation_0-mlogloss:2.35454\n",
      "[224]\tvalidation_0-mlogloss:2.35351\n",
      "[225]\tvalidation_0-mlogloss:2.35247\n",
      "[226]\tvalidation_0-mlogloss:2.35157\n",
      "[227]\tvalidation_0-mlogloss:2.35077\n",
      "[228]\tvalidation_0-mlogloss:2.34966\n",
      "[229]\tvalidation_0-mlogloss:2.34864\n",
      "[230]\tvalidation_0-mlogloss:2.34770\n",
      "[231]\tvalidation_0-mlogloss:2.34808\n",
      "[232]\tvalidation_0-mlogloss:2.34780\n",
      "[233]\tvalidation_0-mlogloss:2.34720\n",
      "[234]\tvalidation_0-mlogloss:2.34703\n",
      "[235]\tvalidation_0-mlogloss:2.34654\n",
      "[236]\tvalidation_0-mlogloss:2.34619\n",
      "[237]\tvalidation_0-mlogloss:2.34616\n",
      "[238]\tvalidation_0-mlogloss:2.34605\n",
      "[239]\tvalidation_0-mlogloss:2.34551\n",
      "[240]\tvalidation_0-mlogloss:2.34545\n",
      "[241]\tvalidation_0-mlogloss:2.34486\n",
      "[242]\tvalidation_0-mlogloss:2.34495\n",
      "[243]\tvalidation_0-mlogloss:2.34464\n",
      "[244]\tvalidation_0-mlogloss:2.34450\n",
      "[245]\tvalidation_0-mlogloss:2.34405\n",
      "[246]\tvalidation_0-mlogloss:2.34371\n",
      "[247]\tvalidation_0-mlogloss:2.34326\n",
      "[248]\tvalidation_0-mlogloss:2.34308\n",
      "[249]\tvalidation_0-mlogloss:2.34285\n",
      "[250]\tvalidation_0-mlogloss:2.34251\n",
      "[251]\tvalidation_0-mlogloss:2.34214\n",
      "[252]\tvalidation_0-mlogloss:2.34193\n",
      "[253]\tvalidation_0-mlogloss:2.34140\n",
      "[254]\tvalidation_0-mlogloss:2.34152\n",
      "[255]\tvalidation_0-mlogloss:2.34119\n",
      "[256]\tvalidation_0-mlogloss:2.34101\n",
      "[257]\tvalidation_0-mlogloss:2.34078\n",
      "[258]\tvalidation_0-mlogloss:2.34037\n",
      "[259]\tvalidation_0-mlogloss:2.34003\n",
      "[260]\tvalidation_0-mlogloss:2.33982\n",
      "[261]\tvalidation_0-mlogloss:2.33981\n",
      "[262]\tvalidation_0-mlogloss:2.33928\n",
      "[263]\tvalidation_0-mlogloss:2.33939\n",
      "[264]\tvalidation_0-mlogloss:2.33971\n",
      "[265]\tvalidation_0-mlogloss:2.33956\n",
      "[266]\tvalidation_0-mlogloss:2.33932\n",
      "[267]\tvalidation_0-mlogloss:2.33882\n",
      "[268]\tvalidation_0-mlogloss:2.33853\n",
      "[269]\tvalidation_0-mlogloss:2.33837\n",
      "[270]\tvalidation_0-mlogloss:2.33824\n",
      "[271]\tvalidation_0-mlogloss:2.33777\n",
      "[272]\tvalidation_0-mlogloss:2.33783\n",
      "[273]\tvalidation_0-mlogloss:2.33771\n",
      "[274]\tvalidation_0-mlogloss:2.33774\n",
      "[275]\tvalidation_0-mlogloss:2.33730\n",
      "[276]\tvalidation_0-mlogloss:2.33682\n",
      "[277]\tvalidation_0-mlogloss:2.33680\n",
      "[278]\tvalidation_0-mlogloss:2.33664\n",
      "[279]\tvalidation_0-mlogloss:2.33646\n",
      "[280]\tvalidation_0-mlogloss:2.33555\n",
      "[281]\tvalidation_0-mlogloss:2.33570\n",
      "[282]\tvalidation_0-mlogloss:2.33530\n",
      "[283]\tvalidation_0-mlogloss:2.33471\n",
      "[284]\tvalidation_0-mlogloss:2.33482\n",
      "[285]\tvalidation_0-mlogloss:2.33491\n",
      "[286]\tvalidation_0-mlogloss:2.33526\n",
      "[287]\tvalidation_0-mlogloss:2.33561\n",
      "[288]\tvalidation_0-mlogloss:2.33529\n",
      "[289]\tvalidation_0-mlogloss:2.33496\n",
      "[290]\tvalidation_0-mlogloss:2.33480\n",
      "[291]\tvalidation_0-mlogloss:2.33476\n",
      "[292]\tvalidation_0-mlogloss:2.33435\n",
      "[293]\tvalidation_0-mlogloss:2.33382\n",
      "[294]\tvalidation_0-mlogloss:2.33423\n",
      "[295]\tvalidation_0-mlogloss:2.33393\n",
      "[296]\tvalidation_0-mlogloss:2.33385\n",
      "[297]\tvalidation_0-mlogloss:2.33403\n",
      "[298]\tvalidation_0-mlogloss:2.33356\n",
      "[299]\tvalidation_0-mlogloss:2.33299\n",
      "[300]\tvalidation_0-mlogloss:2.33298\n",
      "[301]\tvalidation_0-mlogloss:2.33254\n",
      "[302]\tvalidation_0-mlogloss:2.33247\n",
      "[303]\tvalidation_0-mlogloss:2.33237\n",
      "[304]\tvalidation_0-mlogloss:2.33221\n",
      "[305]\tvalidation_0-mlogloss:2.33201\n",
      "[306]\tvalidation_0-mlogloss:2.33135\n",
      "[307]\tvalidation_0-mlogloss:2.33116\n",
      "[308]\tvalidation_0-mlogloss:2.33102\n",
      "[309]\tvalidation_0-mlogloss:2.33089\n",
      "[310]\tvalidation_0-mlogloss:2.33070\n",
      "[311]\tvalidation_0-mlogloss:2.33069\n",
      "[312]\tvalidation_0-mlogloss:2.33043\n",
      "[313]\tvalidation_0-mlogloss:2.33032\n",
      "[314]\tvalidation_0-mlogloss:2.33029\n",
      "[315]\tvalidation_0-mlogloss:2.32994\n",
      "[316]\tvalidation_0-mlogloss:2.32999\n",
      "[317]\tvalidation_0-mlogloss:2.33012\n",
      "[318]\tvalidation_0-mlogloss:2.32992\n",
      "[319]\tvalidation_0-mlogloss:2.32966\n",
      "[320]\tvalidation_0-mlogloss:2.32966\n",
      "[321]\tvalidation_0-mlogloss:2.33017\n",
      "[322]\tvalidation_0-mlogloss:2.32982\n",
      "[323]\tvalidation_0-mlogloss:2.32979\n",
      "[324]\tvalidation_0-mlogloss:2.32982\n",
      "[325]\tvalidation_0-mlogloss:2.33009\n",
      "[326]\tvalidation_0-mlogloss:2.32977\n",
      "[327]\tvalidation_0-mlogloss:2.32958\n",
      "[328]\tvalidation_0-mlogloss:2.32948\n",
      "[329]\tvalidation_0-mlogloss:2.32939\n",
      "[330]\tvalidation_0-mlogloss:2.32957\n",
      "[331]\tvalidation_0-mlogloss:2.32956\n",
      "[332]\tvalidation_0-mlogloss:2.32928\n",
      "[333]\tvalidation_0-mlogloss:2.32963\n",
      "[334]\tvalidation_0-mlogloss:2.32973\n",
      "[335]\tvalidation_0-mlogloss:2.33023\n",
      "[336]\tvalidation_0-mlogloss:2.33006\n",
      "[337]\tvalidation_0-mlogloss:2.33016\n",
      "[338]\tvalidation_0-mlogloss:2.33029\n",
      "[339]\tvalidation_0-mlogloss:2.33024\n",
      "[340]\tvalidation_0-mlogloss:2.33022\n",
      "[341]\tvalidation_0-mlogloss:2.32997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.13      0.16      1499\n",
      "           1       0.20      0.20      0.20      1499\n",
      "           2       0.30      0.29      0.29      1499\n",
      "           3       0.51      0.54      0.53      1499\n",
      "           4       0.67      0.70      0.69      1499\n",
      "           5       0.37      0.42      0.39      1499\n",
      "           6       0.37      0.56      0.44      1499\n",
      "           7       0.57      0.45      0.50      1499\n",
      "           8       0.46      0.65      0.54      1499\n",
      "           9       0.54      0.47      0.50      1499\n",
      "          10       0.32      0.33      0.33      1499\n",
      "          11       0.19      0.17      0.18      1499\n",
      "          12       0.32      0.24      0.27      1499\n",
      "          13       0.19      0.13      0.15      1499\n",
      "          14       0.22      0.13      0.17      1499\n",
      "          15       0.44      0.36      0.39      1499\n",
      "          16       0.07      0.05      0.06      1499\n",
      "          17       0.44      0.69      0.54      1499\n",
      "          18       0.21      0.14      0.17      1499\n",
      "          19       0.19      0.20      0.20      1499\n",
      "          20       0.55      0.75      0.64      1499\n",
      "          21       0.43      0.44      0.43      1499\n",
      "          22       0.18      0.13      0.15      1499\n",
      "          23       0.49      0.59      0.54      1499\n",
      "          24       0.62      0.78      0.69      1499\n",
      "          25       0.23      0.23      0.23      1499\n",
      "          26       0.55      0.55      0.55      1499\n",
      "          27       0.23      0.18      0.20      1499\n",
      "          28       0.23      0.21      0.22      1499\n",
      "          29       0.40      0.50      0.44      1499\n",
      "          30       0.21      0.21      0.21      1499\n",
      "          31       0.22      0.24      0.23      1499\n",
      "          32       0.27      0.18      0.21      1499\n",
      "          33       0.12      0.08      0.10      1499\n",
      "          34       0.61      0.75      0.68      1499\n",
      "          35       0.50      0.66      0.57      1499\n",
      "          36       0.33      0.28      0.30      1499\n",
      "          37       0.23      0.21      0.22      1499\n",
      "          38       0.17      0.11      0.13      1499\n",
      "          39       0.52      0.47      0.49      1499\n",
      "          40       0.30      0.34      0.32      1499\n",
      "          41       0.47      0.73      0.57      1499\n",
      "          42       0.31      0.28      0.30      1499\n",
      "          43       0.34      0.39      0.36      1499\n",
      "          44       0.57      0.68      0.62      1499\n",
      "          45       0.11      0.07      0.09      1499\n",
      "          46       0.38      0.51      0.44      1499\n",
      "          47       0.32      0.30      0.31      1499\n",
      "          48       0.54      0.73      0.62      1499\n",
      "          49       0.21      0.17      0.19      1499\n",
      "\n",
      "    accuracy                           0.37     74950\n",
      "   macro avg       0.35      0.37      0.36     74950\n",
      "weighted avg       0.35      0.37      0.36     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b271d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 28s 1ms/step - loss: 2.4155 - val_loss: 2.4537\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 2.0203 - val_loss: 2.3376\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 1.7825 - val_loss: 2.2107\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 1.6656 - val_loss: 2.3141\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 27s 1ms/step - loss: 1.6130 - val_loss: 2.2517\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 1.5797 - val_loss: 2.2798\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 1.5552 - val_loss: 2.3051\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 1.5373 - val_loss: 2.3609\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 1.5238 - val_loss: 2.3464\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 1.5112 - val_loss: 2.3448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc1adb0b2e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  60/2343 [..............................] - ETA: 1s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 2s 832us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.13      0.16      1499\n",
      "           1       0.24      0.15      0.18      1499\n",
      "           2       0.34      0.18      0.23      1499\n",
      "           3       0.60      0.54      0.57      1499\n",
      "           4       0.76      0.65      0.70      1499\n",
      "           5       0.53      0.49      0.51      1499\n",
      "           6       0.41      0.61      0.49      1499\n",
      "           7       0.69      0.49      0.57      1499\n",
      "           8       0.51      0.54      0.52      1499\n",
      "           9       0.50      0.33      0.40      1499\n",
      "          10       0.31      0.24      0.27      1499\n",
      "          11       0.19      0.42      0.26      1499\n",
      "          12       0.34      0.15      0.21      1499\n",
      "          13       0.16      0.16      0.16      1499\n",
      "          14       0.25      0.10      0.14      1499\n",
      "          15       0.42      0.35      0.38      1499\n",
      "          16       0.12      0.07      0.09      1499\n",
      "          17       0.40      0.80      0.54      1499\n",
      "          18       0.18      0.13      0.15      1499\n",
      "          19       0.20      0.13      0.16      1499\n",
      "          20       0.57      0.60      0.58      1499\n",
      "          21       0.39      0.69      0.50      1499\n",
      "          22       0.23      0.15      0.18      1499\n",
      "          23       0.52      0.61      0.56      1499\n",
      "          24       0.66      0.57      0.61      1499\n",
      "          25       0.38      0.35      0.36      1499\n",
      "          26       0.67      0.48      0.56      1499\n",
      "          27       0.19      0.22      0.20      1499\n",
      "          28       0.26      0.24      0.25      1499\n",
      "          29       0.47      0.26      0.34      1499\n",
      "          30       0.27      0.25      0.26      1499\n",
      "          31       0.26      0.18      0.21      1499\n",
      "          32       0.26      0.29      0.27      1499\n",
      "          33       0.08      0.03      0.04      1499\n",
      "          34       0.75      0.51      0.61      1499\n",
      "          35       0.49      0.53      0.51      1499\n",
      "          36       0.28      0.32      0.30      1499\n",
      "          37       0.17      0.35      0.22      1499\n",
      "          38       0.20      0.17      0.18      1499\n",
      "          39       0.44      0.50      0.47      1499\n",
      "          40       0.37      0.25      0.30      1499\n",
      "          41       0.50      0.83      0.62      1499\n",
      "          42       0.23      0.40      0.30      1499\n",
      "          43       0.39      0.43      0.41      1499\n",
      "          44       0.60      0.68      0.64      1499\n",
      "          45       0.14      0.14      0.14      1499\n",
      "          46       0.44      0.52      0.48      1499\n",
      "          47       0.31      0.34      0.32      1499\n",
      "          48       0.57      0.67      0.61      1499\n",
      "          49       0.19      0.27      0.22      1499\n",
      "\n",
      "    accuracy                           0.37     74950\n",
      "   macro avg       0.37      0.37      0.36     74950\n",
      "weighted avg       0.37      0.37      0.36     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37312/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_37312/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_37312/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000057  -0.000013  -0.000015  -0.000012  -0.000013  -0.000008   \n",
       "1       -0.000049  -0.000011  -0.000010  -0.000012  -0.000019  -0.000024   \n",
       "2       -0.000055  -0.000017  -0.000016  -0.000019  -0.000024  -0.000029   \n",
       "3       -0.000073  -0.000042  -0.000040  -0.000037  -0.000037  -0.000040   \n",
       "4       -0.000087  -0.000053  -0.000052  -0.000051  -0.000045  -0.000043   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "599995   0.000025   0.000034   0.000019   0.000012   0.000017   0.000013   \n",
       "599996   0.000061   0.000068   0.000057   0.000048   0.000050   0.000044   \n",
       "599997   0.000066   0.000065   0.000059   0.000050   0.000051   0.000051   \n",
       "599998   0.000081   0.000087   0.000066   0.000059   0.000069   0.000060   \n",
       "599999   0.000102   0.000097   0.000083   0.000072   0.000071   0.000058   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0       -0.000040  -0.000054  -0.000012  -0.000014  ...   -0.000048   \n",
       "1       -0.000058  -0.000051  -0.000019  -0.000023  ...   -0.000055   \n",
       "2       -0.000066  -0.000061  -0.000030  -0.000036  ...   -0.000054   \n",
       "3       -0.000071  -0.000078  -0.000053  -0.000053  ...   -0.000065   \n",
       "4       -0.000071  -0.000087  -0.000065  -0.000064  ...   -0.000075   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "599995   0.000018   0.000029   0.000014   0.000010  ...    0.000020   \n",
       "599996   0.000043   0.000048   0.000049   0.000043  ...    0.000013   \n",
       "599997   0.000054   0.000031   0.000040   0.000035  ...   -0.000002   \n",
       "599998   0.000064   0.000054   0.000043   0.000041  ...    0.000011   \n",
       "599999   0.000047   0.000064   0.000065   0.000058  ...    0.000002   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0        -0.000038   -0.000042   -0.000068   -0.000076   -0.000103   \n",
       "1        -0.000055   -0.000063   -0.000082   -0.000087   -0.000099   \n",
       "2        -0.000063   -0.000072   -0.000091   -0.000092   -0.000091   \n",
       "3        -0.000052   -0.000066   -0.000100   -0.000105   -0.000105   \n",
       "4        -0.000082   -0.000090   -0.000117   -0.000119   -0.000118   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "599995    0.000008   -0.000002    0.000019    0.000013    0.000005   \n",
       "599996    0.000010   -0.000007    0.000011    0.000012    0.000003   \n",
       "599997   -0.000025   -0.000032   -0.000006   -0.000002   -0.000013   \n",
       "599998    0.000000   -0.000006    0.000014    0.000014   -0.000003   \n",
       "599999   -0.000004   -0.000011    0.000010   -0.000001   -0.000025   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0        -0.000051   -0.000056   -0.000124   -0.000028  \n",
       "1        -0.000059   -0.000070   -0.000149   -0.000040  \n",
       "2        -0.000067   -0.000077   -0.000153   -0.000037  \n",
       "3        -0.000067   -0.000072   -0.000148   -0.000026  \n",
       "4        -0.000075   -0.000082   -0.000161   -0.000035  \n",
       "...            ...         ...         ...         ...  \n",
       "599995    0.000013    0.000007    0.000006   -0.000060  \n",
       "599996    0.000013    0.000020    0.000016   -0.000027  \n",
       "599997   -0.000019   -0.000010   -0.000002   -0.000059  \n",
       "599998    0.000005    0.000000    0.000005   -0.000064  \n",
       "599999    0.000000   -0.000018   -0.000020   -0.000087  \n",
       "\n",
       "[600000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.58298\n",
      "[1]\tvalidation_0-mlogloss:3.43123\n",
      "[2]\tvalidation_0-mlogloss:3.32472\n",
      "[3]\tvalidation_0-mlogloss:3.24059\n",
      "[4]\tvalidation_0-mlogloss:3.17167\n",
      "[5]\tvalidation_0-mlogloss:3.11263\n",
      "[6]\tvalidation_0-mlogloss:3.06031\n",
      "[7]\tvalidation_0-mlogloss:3.01581\n",
      "[8]\tvalidation_0-mlogloss:2.97464\n",
      "[9]\tvalidation_0-mlogloss:2.93529\n",
      "[10]\tvalidation_0-mlogloss:2.90282\n",
      "[11]\tvalidation_0-mlogloss:2.87252\n",
      "[12]\tvalidation_0-mlogloss:2.84612\n",
      "[13]\tvalidation_0-mlogloss:2.81776\n",
      "[14]\tvalidation_0-mlogloss:2.79218\n",
      "[15]\tvalidation_0-mlogloss:2.77145\n",
      "[16]\tvalidation_0-mlogloss:2.74817\n",
      "[17]\tvalidation_0-mlogloss:2.72689\n",
      "[18]\tvalidation_0-mlogloss:2.70555\n",
      "[19]\tvalidation_0-mlogloss:2.68634\n",
      "[20]\tvalidation_0-mlogloss:2.66898\n",
      "[21]\tvalidation_0-mlogloss:2.65002\n",
      "[22]\tvalidation_0-mlogloss:2.63356\n",
      "[23]\tvalidation_0-mlogloss:2.61772\n",
      "[24]\tvalidation_0-mlogloss:2.60055\n",
      "[25]\tvalidation_0-mlogloss:2.58534\n",
      "[26]\tvalidation_0-mlogloss:2.57193\n",
      "[27]\tvalidation_0-mlogloss:2.56120\n",
      "[28]\tvalidation_0-mlogloss:2.54771\n",
      "[29]\tvalidation_0-mlogloss:2.53401\n",
      "[30]\tvalidation_0-mlogloss:2.51958\n",
      "[31]\tvalidation_0-mlogloss:2.50851\n",
      "[32]\tvalidation_0-mlogloss:2.49461\n",
      "[33]\tvalidation_0-mlogloss:2.48111\n",
      "[34]\tvalidation_0-mlogloss:2.46902\n",
      "[35]\tvalidation_0-mlogloss:2.45581\n",
      "[36]\tvalidation_0-mlogloss:2.44512\n",
      "[37]\tvalidation_0-mlogloss:2.43405\n",
      "[38]\tvalidation_0-mlogloss:2.42381\n",
      "[39]\tvalidation_0-mlogloss:2.41396\n",
      "[40]\tvalidation_0-mlogloss:2.40457\n",
      "[41]\tvalidation_0-mlogloss:2.39564\n",
      "[42]\tvalidation_0-mlogloss:2.38673\n",
      "[43]\tvalidation_0-mlogloss:2.37792\n",
      "[44]\tvalidation_0-mlogloss:2.37020\n",
      "[45]\tvalidation_0-mlogloss:2.36116\n",
      "[46]\tvalidation_0-mlogloss:2.35146\n",
      "[47]\tvalidation_0-mlogloss:2.34373\n",
      "[48]\tvalidation_0-mlogloss:2.33703\n",
      "[49]\tvalidation_0-mlogloss:2.32622\n",
      "[50]\tvalidation_0-mlogloss:2.31653\n",
      "[51]\tvalidation_0-mlogloss:2.30858\n",
      "[52]\tvalidation_0-mlogloss:2.30189\n",
      "[53]\tvalidation_0-mlogloss:2.29401\n",
      "[54]\tvalidation_0-mlogloss:2.28683\n",
      "[55]\tvalidation_0-mlogloss:2.27709\n",
      "[56]\tvalidation_0-mlogloss:2.26895\n",
      "[57]\tvalidation_0-mlogloss:2.26017\n",
      "[58]\tvalidation_0-mlogloss:2.25310\n",
      "[59]\tvalidation_0-mlogloss:2.24662\n",
      "[60]\tvalidation_0-mlogloss:2.23823\n",
      "[61]\tvalidation_0-mlogloss:2.23132\n",
      "[62]\tvalidation_0-mlogloss:2.22482\n",
      "[63]\tvalidation_0-mlogloss:2.21920\n",
      "[64]\tvalidation_0-mlogloss:2.21352\n",
      "[65]\tvalidation_0-mlogloss:2.20811\n",
      "[66]\tvalidation_0-mlogloss:2.20161\n",
      "[67]\tvalidation_0-mlogloss:2.19657\n",
      "[68]\tvalidation_0-mlogloss:2.18998\n",
      "[69]\tvalidation_0-mlogloss:2.18390\n",
      "[70]\tvalidation_0-mlogloss:2.17816\n",
      "[71]\tvalidation_0-mlogloss:2.17224\n",
      "[72]\tvalidation_0-mlogloss:2.16726\n",
      "[73]\tvalidation_0-mlogloss:2.16120\n",
      "[74]\tvalidation_0-mlogloss:2.15472\n",
      "[75]\tvalidation_0-mlogloss:2.15063\n",
      "[76]\tvalidation_0-mlogloss:2.14522\n",
      "[77]\tvalidation_0-mlogloss:2.14064\n",
      "[78]\tvalidation_0-mlogloss:2.13581\n",
      "[79]\tvalidation_0-mlogloss:2.13121\n",
      "[80]\tvalidation_0-mlogloss:2.12632\n",
      "[81]\tvalidation_0-mlogloss:2.12122\n",
      "[82]\tvalidation_0-mlogloss:2.11604\n",
      "[83]\tvalidation_0-mlogloss:2.11071\n",
      "[84]\tvalidation_0-mlogloss:2.10552\n",
      "[85]\tvalidation_0-mlogloss:2.10115\n",
      "[86]\tvalidation_0-mlogloss:2.09683\n",
      "[87]\tvalidation_0-mlogloss:2.09186\n",
      "[88]\tvalidation_0-mlogloss:2.08683\n",
      "[89]\tvalidation_0-mlogloss:2.08376\n",
      "[90]\tvalidation_0-mlogloss:2.07992\n",
      "[91]\tvalidation_0-mlogloss:2.07476\n",
      "[92]\tvalidation_0-mlogloss:2.06988\n",
      "[93]\tvalidation_0-mlogloss:2.06450\n",
      "[94]\tvalidation_0-mlogloss:2.05931\n",
      "[95]\tvalidation_0-mlogloss:2.05569\n",
      "[96]\tvalidation_0-mlogloss:2.05274\n",
      "[97]\tvalidation_0-mlogloss:2.04586\n",
      "[98]\tvalidation_0-mlogloss:2.04176\n",
      "[99]\tvalidation_0-mlogloss:2.03747\n",
      "[100]\tvalidation_0-mlogloss:2.03433\n",
      "[101]\tvalidation_0-mlogloss:2.03085\n",
      "[102]\tvalidation_0-mlogloss:2.02772\n",
      "[103]\tvalidation_0-mlogloss:2.02376\n",
      "[104]\tvalidation_0-mlogloss:2.02022\n",
      "[105]\tvalidation_0-mlogloss:2.01723\n",
      "[106]\tvalidation_0-mlogloss:2.01386\n",
      "[107]\tvalidation_0-mlogloss:2.00899\n",
      "[108]\tvalidation_0-mlogloss:2.00533\n",
      "[109]\tvalidation_0-mlogloss:2.00146\n",
      "[110]\tvalidation_0-mlogloss:1.99873\n",
      "[111]\tvalidation_0-mlogloss:1.99521\n",
      "[112]\tvalidation_0-mlogloss:1.99239\n",
      "[113]\tvalidation_0-mlogloss:1.98926\n",
      "[114]\tvalidation_0-mlogloss:1.98575\n",
      "[115]\tvalidation_0-mlogloss:1.98261\n",
      "[116]\tvalidation_0-mlogloss:1.97915\n",
      "[117]\tvalidation_0-mlogloss:1.97620\n",
      "[118]\tvalidation_0-mlogloss:1.97418\n",
      "[119]\tvalidation_0-mlogloss:1.97158\n",
      "[120]\tvalidation_0-mlogloss:1.96984\n",
      "[121]\tvalidation_0-mlogloss:1.96683\n",
      "[122]\tvalidation_0-mlogloss:1.96405\n",
      "[123]\tvalidation_0-mlogloss:1.96054\n",
      "[124]\tvalidation_0-mlogloss:1.95743\n",
      "[125]\tvalidation_0-mlogloss:1.95497\n",
      "[126]\tvalidation_0-mlogloss:1.95249\n",
      "[127]\tvalidation_0-mlogloss:1.94968\n",
      "[128]\tvalidation_0-mlogloss:1.94717\n",
      "[129]\tvalidation_0-mlogloss:1.94475\n",
      "[130]\tvalidation_0-mlogloss:1.94277\n",
      "[131]\tvalidation_0-mlogloss:1.94001\n",
      "[132]\tvalidation_0-mlogloss:1.93799\n",
      "[133]\tvalidation_0-mlogloss:1.93572\n",
      "[134]\tvalidation_0-mlogloss:1.93280\n",
      "[135]\tvalidation_0-mlogloss:1.93038\n",
      "[136]\tvalidation_0-mlogloss:1.92802\n",
      "[137]\tvalidation_0-mlogloss:1.92650\n",
      "[138]\tvalidation_0-mlogloss:1.92410\n",
      "[139]\tvalidation_0-mlogloss:1.92145\n",
      "[140]\tvalidation_0-mlogloss:1.91911\n",
      "[141]\tvalidation_0-mlogloss:1.91719\n",
      "[142]\tvalidation_0-mlogloss:1.91484\n",
      "[143]\tvalidation_0-mlogloss:1.91233\n",
      "[144]\tvalidation_0-mlogloss:1.91073\n",
      "[145]\tvalidation_0-mlogloss:1.90849\n",
      "[146]\tvalidation_0-mlogloss:1.90561\n",
      "[147]\tvalidation_0-mlogloss:1.90402\n",
      "[148]\tvalidation_0-mlogloss:1.90304\n",
      "[149]\tvalidation_0-mlogloss:1.90102\n",
      "[150]\tvalidation_0-mlogloss:1.89942\n",
      "[151]\tvalidation_0-mlogloss:1.89634\n",
      "[152]\tvalidation_0-mlogloss:1.89470\n",
      "[153]\tvalidation_0-mlogloss:1.89314\n",
      "[154]\tvalidation_0-mlogloss:1.89010\n",
      "[155]\tvalidation_0-mlogloss:1.88888\n",
      "[156]\tvalidation_0-mlogloss:1.88658\n",
      "[157]\tvalidation_0-mlogloss:1.88452\n",
      "[158]\tvalidation_0-mlogloss:1.88263\n",
      "[159]\tvalidation_0-mlogloss:1.88115\n",
      "[160]\tvalidation_0-mlogloss:1.87855\n",
      "[161]\tvalidation_0-mlogloss:1.87668\n",
      "[162]\tvalidation_0-mlogloss:1.87483\n",
      "[163]\tvalidation_0-mlogloss:1.87375\n",
      "[164]\tvalidation_0-mlogloss:1.87202\n",
      "[165]\tvalidation_0-mlogloss:1.87064\n",
      "[166]\tvalidation_0-mlogloss:1.86948\n",
      "[167]\tvalidation_0-mlogloss:1.86837\n",
      "[168]\tvalidation_0-mlogloss:1.86694\n",
      "[169]\tvalidation_0-mlogloss:1.86588\n",
      "[170]\tvalidation_0-mlogloss:1.86435\n",
      "[171]\tvalidation_0-mlogloss:1.86276\n",
      "[172]\tvalidation_0-mlogloss:1.86149\n",
      "[173]\tvalidation_0-mlogloss:1.85971\n",
      "[174]\tvalidation_0-mlogloss:1.85794\n",
      "[175]\tvalidation_0-mlogloss:1.85686\n",
      "[176]\tvalidation_0-mlogloss:1.85562\n",
      "[177]\tvalidation_0-mlogloss:1.85341\n",
      "[178]\tvalidation_0-mlogloss:1.85228\n",
      "[179]\tvalidation_0-mlogloss:1.85116\n",
      "[180]\tvalidation_0-mlogloss:1.85011\n",
      "[181]\tvalidation_0-mlogloss:1.84823\n",
      "[182]\tvalidation_0-mlogloss:1.84711\n",
      "[183]\tvalidation_0-mlogloss:1.84623\n",
      "[184]\tvalidation_0-mlogloss:1.84525\n",
      "[185]\tvalidation_0-mlogloss:1.84393\n",
      "[186]\tvalidation_0-mlogloss:1.84302\n",
      "[187]\tvalidation_0-mlogloss:1.84181\n",
      "[188]\tvalidation_0-mlogloss:1.84089\n",
      "[189]\tvalidation_0-mlogloss:1.84017\n",
      "[190]\tvalidation_0-mlogloss:1.83857\n",
      "[191]\tvalidation_0-mlogloss:1.83736\n",
      "[192]\tvalidation_0-mlogloss:1.83556\n",
      "[193]\tvalidation_0-mlogloss:1.83445\n",
      "[194]\tvalidation_0-mlogloss:1.83267\n",
      "[195]\tvalidation_0-mlogloss:1.83182\n",
      "[196]\tvalidation_0-mlogloss:1.83073\n",
      "[197]\tvalidation_0-mlogloss:1.82960\n",
      "[198]\tvalidation_0-mlogloss:1.82805\n",
      "[199]\tvalidation_0-mlogloss:1.82739\n",
      "[200]\tvalidation_0-mlogloss:1.82626\n",
      "[201]\tvalidation_0-mlogloss:1.82479\n",
      "[202]\tvalidation_0-mlogloss:1.82381\n",
      "[203]\tvalidation_0-mlogloss:1.82338\n",
      "[204]\tvalidation_0-mlogloss:1.82294\n",
      "[205]\tvalidation_0-mlogloss:1.82175\n",
      "[206]\tvalidation_0-mlogloss:1.82065\n",
      "[207]\tvalidation_0-mlogloss:1.81937\n",
      "[208]\tvalidation_0-mlogloss:1.81887\n",
      "[209]\tvalidation_0-mlogloss:1.81820\n",
      "[210]\tvalidation_0-mlogloss:1.81728\n",
      "[211]\tvalidation_0-mlogloss:1.81569\n",
      "[212]\tvalidation_0-mlogloss:1.81360\n",
      "[213]\tvalidation_0-mlogloss:1.81267\n",
      "[214]\tvalidation_0-mlogloss:1.81172\n",
      "[215]\tvalidation_0-mlogloss:1.81104\n",
      "[216]\tvalidation_0-mlogloss:1.81016\n",
      "[217]\tvalidation_0-mlogloss:1.80901\n",
      "[218]\tvalidation_0-mlogloss:1.80747\n",
      "[219]\tvalidation_0-mlogloss:1.80650\n",
      "[220]\tvalidation_0-mlogloss:1.80632\n",
      "[221]\tvalidation_0-mlogloss:1.80613\n",
      "[222]\tvalidation_0-mlogloss:1.80578\n",
      "[223]\tvalidation_0-mlogloss:1.80504\n",
      "[224]\tvalidation_0-mlogloss:1.80454\n",
      "[225]\tvalidation_0-mlogloss:1.80389\n",
      "[226]\tvalidation_0-mlogloss:1.80271\n",
      "[227]\tvalidation_0-mlogloss:1.80178\n",
      "[228]\tvalidation_0-mlogloss:1.80113\n",
      "[229]\tvalidation_0-mlogloss:1.80112\n",
      "[230]\tvalidation_0-mlogloss:1.80052\n",
      "[231]\tvalidation_0-mlogloss:1.79965\n",
      "[232]\tvalidation_0-mlogloss:1.79902\n",
      "[233]\tvalidation_0-mlogloss:1.79815\n",
      "[234]\tvalidation_0-mlogloss:1.79808\n",
      "[235]\tvalidation_0-mlogloss:1.79779\n",
      "[236]\tvalidation_0-mlogloss:1.79702\n",
      "[237]\tvalidation_0-mlogloss:1.79583\n",
      "[238]\tvalidation_0-mlogloss:1.79540\n",
      "[239]\tvalidation_0-mlogloss:1.79455\n",
      "[240]\tvalidation_0-mlogloss:1.79500\n",
      "[241]\tvalidation_0-mlogloss:1.79432\n",
      "[242]\tvalidation_0-mlogloss:1.79368\n",
      "[243]\tvalidation_0-mlogloss:1.79322\n",
      "[244]\tvalidation_0-mlogloss:1.79223\n",
      "[245]\tvalidation_0-mlogloss:1.79096\n",
      "[246]\tvalidation_0-mlogloss:1.78985\n",
      "[247]\tvalidation_0-mlogloss:1.78898\n",
      "[248]\tvalidation_0-mlogloss:1.78856\n",
      "[249]\tvalidation_0-mlogloss:1.78794\n",
      "[250]\tvalidation_0-mlogloss:1.78699\n",
      "[251]\tvalidation_0-mlogloss:1.78593\n",
      "[252]\tvalidation_0-mlogloss:1.78563\n",
      "[253]\tvalidation_0-mlogloss:1.78493\n",
      "[254]\tvalidation_0-mlogloss:1.78396\n",
      "[255]\tvalidation_0-mlogloss:1.78317\n",
      "[256]\tvalidation_0-mlogloss:1.78280\n",
      "[257]\tvalidation_0-mlogloss:1.78215\n",
      "[258]\tvalidation_0-mlogloss:1.78162\n",
      "[259]\tvalidation_0-mlogloss:1.78124\n",
      "[260]\tvalidation_0-mlogloss:1.78082\n",
      "[261]\tvalidation_0-mlogloss:1.78042\n",
      "[262]\tvalidation_0-mlogloss:1.78019\n",
      "[263]\tvalidation_0-mlogloss:1.77955\n",
      "[264]\tvalidation_0-mlogloss:1.77866\n",
      "[265]\tvalidation_0-mlogloss:1.77863\n",
      "[266]\tvalidation_0-mlogloss:1.77819\n",
      "[267]\tvalidation_0-mlogloss:1.77760\n",
      "[268]\tvalidation_0-mlogloss:1.77672\n",
      "[269]\tvalidation_0-mlogloss:1.77653\n",
      "[270]\tvalidation_0-mlogloss:1.77660\n",
      "[271]\tvalidation_0-mlogloss:1.77610\n",
      "[272]\tvalidation_0-mlogloss:1.77530\n",
      "[273]\tvalidation_0-mlogloss:1.77490\n",
      "[274]\tvalidation_0-mlogloss:1.77453\n",
      "[275]\tvalidation_0-mlogloss:1.77404\n",
      "[276]\tvalidation_0-mlogloss:1.77354\n",
      "[277]\tvalidation_0-mlogloss:1.77360\n",
      "[278]\tvalidation_0-mlogloss:1.77303\n",
      "[279]\tvalidation_0-mlogloss:1.77293\n",
      "[280]\tvalidation_0-mlogloss:1.77287\n",
      "[281]\tvalidation_0-mlogloss:1.77278\n",
      "[282]\tvalidation_0-mlogloss:1.77263\n",
      "[283]\tvalidation_0-mlogloss:1.77220\n",
      "[284]\tvalidation_0-mlogloss:1.77179\n",
      "[285]\tvalidation_0-mlogloss:1.77189\n",
      "[286]\tvalidation_0-mlogloss:1.77125\n",
      "[287]\tvalidation_0-mlogloss:1.77145\n",
      "[288]\tvalidation_0-mlogloss:1.77127\n",
      "[289]\tvalidation_0-mlogloss:1.77096\n",
      "[290]\tvalidation_0-mlogloss:1.77026\n",
      "[291]\tvalidation_0-mlogloss:1.77037\n",
      "[292]\tvalidation_0-mlogloss:1.76979\n",
      "[293]\tvalidation_0-mlogloss:1.76947\n",
      "[294]\tvalidation_0-mlogloss:1.77010\n",
      "[295]\tvalidation_0-mlogloss:1.76995\n",
      "[296]\tvalidation_0-mlogloss:1.76938\n",
      "[297]\tvalidation_0-mlogloss:1.76877\n",
      "[298]\tvalidation_0-mlogloss:1.76822\n",
      "[299]\tvalidation_0-mlogloss:1.76777\n",
      "[300]\tvalidation_0-mlogloss:1.76750\n",
      "[301]\tvalidation_0-mlogloss:1.76709\n",
      "[302]\tvalidation_0-mlogloss:1.76687\n",
      "[303]\tvalidation_0-mlogloss:1.76615\n",
      "[304]\tvalidation_0-mlogloss:1.76653\n",
      "[305]\tvalidation_0-mlogloss:1.76656\n",
      "[306]\tvalidation_0-mlogloss:1.76547\n",
      "[307]\tvalidation_0-mlogloss:1.76560\n",
      "[308]\tvalidation_0-mlogloss:1.76555\n",
      "[309]\tvalidation_0-mlogloss:1.76465\n",
      "[310]\tvalidation_0-mlogloss:1.76448\n",
      "[311]\tvalidation_0-mlogloss:1.76417\n",
      "[312]\tvalidation_0-mlogloss:1.76410\n",
      "[313]\tvalidation_0-mlogloss:1.76429\n",
      "[314]\tvalidation_0-mlogloss:1.76377\n",
      "[315]\tvalidation_0-mlogloss:1.76344\n",
      "[316]\tvalidation_0-mlogloss:1.76316\n",
      "[317]\tvalidation_0-mlogloss:1.76312\n",
      "[318]\tvalidation_0-mlogloss:1.76235\n",
      "[319]\tvalidation_0-mlogloss:1.76211\n",
      "[320]\tvalidation_0-mlogloss:1.76173\n",
      "[321]\tvalidation_0-mlogloss:1.76133\n",
      "[322]\tvalidation_0-mlogloss:1.76106\n",
      "[323]\tvalidation_0-mlogloss:1.76178\n",
      "[324]\tvalidation_0-mlogloss:1.76177\n",
      "[325]\tvalidation_0-mlogloss:1.76117\n",
      "[326]\tvalidation_0-mlogloss:1.76147\n",
      "[327]\tvalidation_0-mlogloss:1.76085\n",
      "[328]\tvalidation_0-mlogloss:1.76058\n",
      "[329]\tvalidation_0-mlogloss:1.76068\n",
      "[330]\tvalidation_0-mlogloss:1.76064\n",
      "[331]\tvalidation_0-mlogloss:1.76061\n",
      "[332]\tvalidation_0-mlogloss:1.76055\n",
      "[333]\tvalidation_0-mlogloss:1.76100\n",
      "[334]\tvalidation_0-mlogloss:1.76105\n",
      "[335]\tvalidation_0-mlogloss:1.76083\n",
      "[336]\tvalidation_0-mlogloss:1.76046\n",
      "[337]\tvalidation_0-mlogloss:1.76066\n",
      "[338]\tvalidation_0-mlogloss:1.76065\n",
      "[339]\tvalidation_0-mlogloss:1.76051\n",
      "[340]\tvalidation_0-mlogloss:1.76062\n",
      "[341]\tvalidation_0-mlogloss:1.76109\n",
      "[342]\tvalidation_0-mlogloss:1.76078\n",
      "[343]\tvalidation_0-mlogloss:1.76112\n",
      "[344]\tvalidation_0-mlogloss:1.76140\n",
      "[345]\tvalidation_0-mlogloss:1.76124\n",
      "[346]\tvalidation_0-mlogloss:1.76122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.40      0.39      1499\n",
      "           1       0.39      0.39      0.39      1499\n",
      "           2       0.34      0.31      0.32      1499\n",
      "           3       0.69      0.48      0.56      1499\n",
      "           4       0.85      0.85      0.85      1499\n",
      "           5       0.53      0.51      0.52      1499\n",
      "           6       0.63      0.83      0.71      1499\n",
      "           7       0.85      0.75      0.79      1499\n",
      "           8       0.47      0.80      0.59      1499\n",
      "           9       0.63      0.62      0.63      1499\n",
      "          10       0.67      0.66      0.66      1499\n",
      "          11       0.49      0.51      0.50      1499\n",
      "          12       0.46      0.56      0.51      1499\n",
      "          13       0.37      0.43      0.40      1499\n",
      "          14       0.33      0.37      0.35      1499\n",
      "          15       0.47      0.12      0.19      1499\n",
      "          16       0.40      0.23      0.30      1499\n",
      "          17       0.68      0.74      0.71      1499\n",
      "          18       0.37      0.48      0.42      1499\n",
      "          19       0.44      0.47      0.46      1499\n",
      "          20       0.81      0.90      0.85      1499\n",
      "          21       0.51      0.51      0.51      1499\n",
      "          22       0.34      0.28      0.31      1499\n",
      "          23       0.64      0.81      0.72      1499\n",
      "          24       0.86      0.83      0.85      1499\n",
      "          25       0.49      0.18      0.26      1499\n",
      "          26       0.68      0.61      0.64      1499\n",
      "          27       0.36      0.35      0.35      1499\n",
      "          28       0.49      0.30      0.37      1499\n",
      "          29       0.63      0.64      0.64      1499\n",
      "          30       0.40      0.33      0.37      1499\n",
      "          31       0.33      0.49      0.39      1499\n",
      "          32       0.44      0.45      0.45      1499\n",
      "          33       0.58      0.37      0.45      1499\n",
      "          34       0.74      0.89      0.81      1499\n",
      "          35       0.62      0.75      0.68      1499\n",
      "          36       0.63      0.60      0.61      1499\n",
      "          37       0.43      0.46      0.44      1499\n",
      "          38       0.42      0.24      0.31      1499\n",
      "          39       0.72      0.66      0.69      1499\n",
      "          40       0.45      0.48      0.46      1499\n",
      "          41       0.65      0.60      0.62      1499\n",
      "          42       0.46      0.44      0.45      1499\n",
      "          43       0.36      0.42      0.39      1499\n",
      "          44       0.65      0.79      0.72      1499\n",
      "          45       0.37      0.36      0.36      1499\n",
      "          46       0.50      0.74      0.60      1499\n",
      "          47       0.61      0.48      0.54      1499\n",
      "          48       0.68      0.81      0.74      1499\n",
      "          49       0.42      0.39      0.40      1499\n",
      "\n",
      "    accuracy                           0.53     74950\n",
      "   macro avg       0.53      0.53      0.52     74950\n",
      "weighted avg       0.53      0.53      0.52     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7844baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7980b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 12:42:23.360328: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 38400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 26s 1ms/step - loss: 1.6465 - val_loss: 2.0962\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 1.0031 - val_loss: 2.2111\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.8547 - val_loss: 2.0914\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.7681 - val_loss: 2.0955\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.7143 - val_loss: 2.0314\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.6766 - val_loss: 2.1990\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.6468 - val_loss: 2.1870\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.6237 - val_loss: 2.2895\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.6036 - val_loss: 2.2044\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.5878 - val_loss: 2.3784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc1ab9decb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 2s 822us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.42      0.44      1499\n",
      "           1       0.45      0.37      0.41      1499\n",
      "           2       0.24      0.31      0.27      1499\n",
      "           3       0.47      0.61      0.53      1499\n",
      "           4       0.85      0.85      0.85      1499\n",
      "           5       0.66      0.69      0.68      1499\n",
      "           6       0.76      0.91      0.83      1499\n",
      "           7       0.84      0.72      0.77      1499\n",
      "           8       0.44      0.78      0.56      1499\n",
      "           9       0.60      0.68      0.64      1499\n",
      "          10       0.42      0.14      0.21      1499\n",
      "          11       0.67      0.54      0.60      1499\n",
      "          12       0.50      0.38      0.43      1499\n",
      "          13       0.25      0.53      0.34      1499\n",
      "          14       0.28      0.47      0.35      1499\n",
      "          15       0.65      0.24      0.35      1499\n",
      "          16       0.15      0.05      0.08      1499\n",
      "          17       0.82      0.68      0.74      1499\n",
      "          18       0.39      0.49      0.43      1499\n",
      "          19       0.47      0.42      0.45      1499\n",
      "          20       0.90      0.77      0.83      1499\n",
      "          21       0.45      0.62      0.52      1499\n",
      "          22       0.43      0.25      0.32      1499\n",
      "          23       0.64      0.86      0.73      1499\n",
      "          24       0.82      0.53      0.64      1499\n",
      "          25       0.57      0.06      0.11      1499\n",
      "          26       0.50      0.30      0.37      1499\n",
      "          27       0.36      0.49      0.42      1499\n",
      "          28       0.52      0.35      0.42      1499\n",
      "          29       0.71      0.66      0.68      1499\n",
      "          30       0.61      0.41      0.49      1499\n",
      "          31       0.41      0.41      0.41      1499\n",
      "          32       0.40      0.56      0.47      1499\n",
      "          33       0.49      0.20      0.28      1499\n",
      "          34       0.90      0.89      0.90      1499\n",
      "          35       0.71      0.75      0.73      1499\n",
      "          36       0.59      0.60      0.59      1499\n",
      "          37       0.40      0.59      0.47      1499\n",
      "          38       0.32      0.31      0.31      1499\n",
      "          39       0.50      0.55      0.52      1499\n",
      "          40       0.49      0.35      0.41      1499\n",
      "          41       0.82      0.60      0.69      1499\n",
      "          42       0.35      0.57      0.43      1499\n",
      "          43       0.38      0.42      0.40      1499\n",
      "          44       0.69      0.84      0.76      1499\n",
      "          45       0.41      0.36      0.38      1499\n",
      "          46       0.46      0.79      0.58      1499\n",
      "          47       0.58      0.46      0.52      1499\n",
      "          48       0.68      0.82      0.75      1499\n",
      "          49       0.45      0.35      0.40      1499\n",
      "\n",
      "    accuracy                           0.52     74950\n",
      "   macro avg       0.54      0.52      0.51     74950\n",
      "weighted avg       0.54      0.52      0.51     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37312/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_37312/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_37312/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.36006\n",
      "[1]\tvalidation_0-mlogloss:3.12523\n",
      "[2]\tvalidation_0-mlogloss:2.97498\n",
      "[3]\tvalidation_0-mlogloss:2.85641\n",
      "[4]\tvalidation_0-mlogloss:2.76006\n",
      "[5]\tvalidation_0-mlogloss:2.68293\n",
      "[6]\tvalidation_0-mlogloss:2.61182\n",
      "[7]\tvalidation_0-mlogloss:2.54959\n",
      "[8]\tvalidation_0-mlogloss:2.49145\n",
      "[9]\tvalidation_0-mlogloss:2.44333\n",
      "[10]\tvalidation_0-mlogloss:2.39608\n",
      "[11]\tvalidation_0-mlogloss:2.35553\n",
      "[12]\tvalidation_0-mlogloss:2.31693\n",
      "[13]\tvalidation_0-mlogloss:2.28224\n",
      "[14]\tvalidation_0-mlogloss:2.24607\n",
      "[15]\tvalidation_0-mlogloss:2.21522\n",
      "[16]\tvalidation_0-mlogloss:2.18454\n",
      "[17]\tvalidation_0-mlogloss:2.15732\n",
      "[18]\tvalidation_0-mlogloss:2.13137\n",
      "[19]\tvalidation_0-mlogloss:2.10774\n",
      "[20]\tvalidation_0-mlogloss:2.08460\n",
      "[21]\tvalidation_0-mlogloss:2.06076\n",
      "[22]\tvalidation_0-mlogloss:2.03857\n",
      "[23]\tvalidation_0-mlogloss:2.01797\n",
      "[24]\tvalidation_0-mlogloss:1.99674\n",
      "[25]\tvalidation_0-mlogloss:1.97802\n",
      "[26]\tvalidation_0-mlogloss:1.96037\n",
      "[27]\tvalidation_0-mlogloss:1.94096\n",
      "[28]\tvalidation_0-mlogloss:1.92013\n",
      "[29]\tvalidation_0-mlogloss:1.90363\n",
      "[30]\tvalidation_0-mlogloss:1.88710\n",
      "[31]\tvalidation_0-mlogloss:1.86982\n",
      "[32]\tvalidation_0-mlogloss:1.85415\n",
      "[33]\tvalidation_0-mlogloss:1.83825\n",
      "[34]\tvalidation_0-mlogloss:1.82368\n",
      "[35]\tvalidation_0-mlogloss:1.80828\n",
      "[36]\tvalidation_0-mlogloss:1.79264\n",
      "[37]\tvalidation_0-mlogloss:1.77848\n",
      "[38]\tvalidation_0-mlogloss:1.76458\n",
      "[39]\tvalidation_0-mlogloss:1.75282\n",
      "[40]\tvalidation_0-mlogloss:1.74259\n",
      "[41]\tvalidation_0-mlogloss:1.72981\n",
      "[42]\tvalidation_0-mlogloss:1.71723\n",
      "[43]\tvalidation_0-mlogloss:1.70465\n",
      "[44]\tvalidation_0-mlogloss:1.69257\n",
      "[45]\tvalidation_0-mlogloss:1.68108\n",
      "[46]\tvalidation_0-mlogloss:1.67009\n",
      "[47]\tvalidation_0-mlogloss:1.66058\n",
      "[48]\tvalidation_0-mlogloss:1.64815\n",
      "[49]\tvalidation_0-mlogloss:1.64020\n",
      "[50]\tvalidation_0-mlogloss:1.63050\n",
      "[51]\tvalidation_0-mlogloss:1.62008\n",
      "[52]\tvalidation_0-mlogloss:1.61083\n",
      "[53]\tvalidation_0-mlogloss:1.60167\n",
      "[54]\tvalidation_0-mlogloss:1.59250\n",
      "[55]\tvalidation_0-mlogloss:1.58380\n",
      "[56]\tvalidation_0-mlogloss:1.57486\n",
      "[57]\tvalidation_0-mlogloss:1.56511\n",
      "[58]\tvalidation_0-mlogloss:1.55595\n",
      "[59]\tvalidation_0-mlogloss:1.54808\n",
      "[60]\tvalidation_0-mlogloss:1.54153\n",
      "[61]\tvalidation_0-mlogloss:1.53452\n",
      "[62]\tvalidation_0-mlogloss:1.52538\n",
      "[63]\tvalidation_0-mlogloss:1.51550\n",
      "[64]\tvalidation_0-mlogloss:1.50921\n",
      "[65]\tvalidation_0-mlogloss:1.50176\n",
      "[66]\tvalidation_0-mlogloss:1.49516\n",
      "[67]\tvalidation_0-mlogloss:1.48802\n",
      "[68]\tvalidation_0-mlogloss:1.47889\n",
      "[69]\tvalidation_0-mlogloss:1.47274\n",
      "[70]\tvalidation_0-mlogloss:1.46679\n",
      "[71]\tvalidation_0-mlogloss:1.46078\n",
      "[72]\tvalidation_0-mlogloss:1.45346\n",
      "[73]\tvalidation_0-mlogloss:1.44863\n",
      "[74]\tvalidation_0-mlogloss:1.44303\n",
      "[75]\tvalidation_0-mlogloss:1.43710\n",
      "[76]\tvalidation_0-mlogloss:1.43077\n",
      "[77]\tvalidation_0-mlogloss:1.42485\n",
      "[78]\tvalidation_0-mlogloss:1.41841\n",
      "[79]\tvalidation_0-mlogloss:1.41212\n",
      "[80]\tvalidation_0-mlogloss:1.40595\n",
      "[81]\tvalidation_0-mlogloss:1.40133\n",
      "[82]\tvalidation_0-mlogloss:1.39710\n",
      "[83]\tvalidation_0-mlogloss:1.39161\n",
      "[84]\tvalidation_0-mlogloss:1.38514\n",
      "[85]\tvalidation_0-mlogloss:1.37999\n",
      "[86]\tvalidation_0-mlogloss:1.37479\n",
      "[87]\tvalidation_0-mlogloss:1.37066\n",
      "[88]\tvalidation_0-mlogloss:1.36545\n",
      "[89]\tvalidation_0-mlogloss:1.36010\n",
      "[90]\tvalidation_0-mlogloss:1.35495\n",
      "[91]\tvalidation_0-mlogloss:1.35109\n",
      "[92]\tvalidation_0-mlogloss:1.34639\n",
      "[93]\tvalidation_0-mlogloss:1.34199\n",
      "[94]\tvalidation_0-mlogloss:1.33796\n",
      "[95]\tvalidation_0-mlogloss:1.33373\n",
      "[96]\tvalidation_0-mlogloss:1.32838\n",
      "[97]\tvalidation_0-mlogloss:1.32420\n",
      "[98]\tvalidation_0-mlogloss:1.32162\n",
      "[99]\tvalidation_0-mlogloss:1.31718\n",
      "[100]\tvalidation_0-mlogloss:1.31312\n",
      "[101]\tvalidation_0-mlogloss:1.30918\n",
      "[102]\tvalidation_0-mlogloss:1.30486\n",
      "[103]\tvalidation_0-mlogloss:1.30071\n",
      "[104]\tvalidation_0-mlogloss:1.29593\n",
      "[105]\tvalidation_0-mlogloss:1.29094\n",
      "[106]\tvalidation_0-mlogloss:1.28674\n",
      "[107]\tvalidation_0-mlogloss:1.28389\n",
      "[108]\tvalidation_0-mlogloss:1.28043\n",
      "[109]\tvalidation_0-mlogloss:1.27790\n",
      "[110]\tvalidation_0-mlogloss:1.27537\n",
      "[111]\tvalidation_0-mlogloss:1.27146\n",
      "[112]\tvalidation_0-mlogloss:1.26699\n",
      "[113]\tvalidation_0-mlogloss:1.26382\n",
      "[114]\tvalidation_0-mlogloss:1.25956\n",
      "[115]\tvalidation_0-mlogloss:1.25635\n",
      "[116]\tvalidation_0-mlogloss:1.25393\n",
      "[117]\tvalidation_0-mlogloss:1.24975\n",
      "[118]\tvalidation_0-mlogloss:1.24744\n",
      "[119]\tvalidation_0-mlogloss:1.24562\n",
      "[120]\tvalidation_0-mlogloss:1.24292\n",
      "[121]\tvalidation_0-mlogloss:1.23926\n",
      "[122]\tvalidation_0-mlogloss:1.23619\n",
      "[123]\tvalidation_0-mlogloss:1.23232\n",
      "[124]\tvalidation_0-mlogloss:1.22956\n",
      "[125]\tvalidation_0-mlogloss:1.22602\n",
      "[126]\tvalidation_0-mlogloss:1.22282\n",
      "[127]\tvalidation_0-mlogloss:1.22089\n",
      "[128]\tvalidation_0-mlogloss:1.21820\n",
      "[129]\tvalidation_0-mlogloss:1.21503\n",
      "[130]\tvalidation_0-mlogloss:1.21249\n",
      "[131]\tvalidation_0-mlogloss:1.21026\n",
      "[132]\tvalidation_0-mlogloss:1.20762\n",
      "[133]\tvalidation_0-mlogloss:1.20421\n",
      "[134]\tvalidation_0-mlogloss:1.20157\n",
      "[135]\tvalidation_0-mlogloss:1.20007\n",
      "[136]\tvalidation_0-mlogloss:1.19658\n",
      "[137]\tvalidation_0-mlogloss:1.19410\n",
      "[138]\tvalidation_0-mlogloss:1.19277\n",
      "[139]\tvalidation_0-mlogloss:1.19067\n",
      "[140]\tvalidation_0-mlogloss:1.18829\n",
      "[141]\tvalidation_0-mlogloss:1.18769\n",
      "[142]\tvalidation_0-mlogloss:1.18444\n",
      "[143]\tvalidation_0-mlogloss:1.18234\n",
      "[144]\tvalidation_0-mlogloss:1.18053\n",
      "[145]\tvalidation_0-mlogloss:1.17824\n",
      "[146]\tvalidation_0-mlogloss:1.17508\n",
      "[147]\tvalidation_0-mlogloss:1.17338\n",
      "[148]\tvalidation_0-mlogloss:1.17120\n",
      "[149]\tvalidation_0-mlogloss:1.16858\n",
      "[150]\tvalidation_0-mlogloss:1.16605\n",
      "[151]\tvalidation_0-mlogloss:1.16460\n",
      "[152]\tvalidation_0-mlogloss:1.16317\n",
      "[153]\tvalidation_0-mlogloss:1.16104\n",
      "[154]\tvalidation_0-mlogloss:1.15837\n",
      "[155]\tvalidation_0-mlogloss:1.15625\n",
      "[156]\tvalidation_0-mlogloss:1.15471\n",
      "[157]\tvalidation_0-mlogloss:1.15285\n",
      "[158]\tvalidation_0-mlogloss:1.15143\n",
      "[159]\tvalidation_0-mlogloss:1.14982\n",
      "[160]\tvalidation_0-mlogloss:1.14809\n",
      "[161]\tvalidation_0-mlogloss:1.14589\n",
      "[162]\tvalidation_0-mlogloss:1.14351\n",
      "[163]\tvalidation_0-mlogloss:1.14221\n",
      "[164]\tvalidation_0-mlogloss:1.14032\n",
      "[165]\tvalidation_0-mlogloss:1.13914\n",
      "[166]\tvalidation_0-mlogloss:1.13709\n",
      "[167]\tvalidation_0-mlogloss:1.13456\n",
      "[168]\tvalidation_0-mlogloss:1.13266\n",
      "[169]\tvalidation_0-mlogloss:1.13087\n",
      "[170]\tvalidation_0-mlogloss:1.12863\n",
      "[171]\tvalidation_0-mlogloss:1.12825\n",
      "[172]\tvalidation_0-mlogloss:1.12595\n",
      "[173]\tvalidation_0-mlogloss:1.12353\n",
      "[174]\tvalidation_0-mlogloss:1.12278\n",
      "[175]\tvalidation_0-mlogloss:1.12131\n",
      "[176]\tvalidation_0-mlogloss:1.12001\n",
      "[177]\tvalidation_0-mlogloss:1.11893\n",
      "[178]\tvalidation_0-mlogloss:1.11750\n",
      "[179]\tvalidation_0-mlogloss:1.11540\n",
      "[180]\tvalidation_0-mlogloss:1.11380\n",
      "[181]\tvalidation_0-mlogloss:1.11189\n",
      "[182]\tvalidation_0-mlogloss:1.11107\n",
      "[183]\tvalidation_0-mlogloss:1.11005\n",
      "[184]\tvalidation_0-mlogloss:1.10838\n",
      "[185]\tvalidation_0-mlogloss:1.10685\n",
      "[186]\tvalidation_0-mlogloss:1.10526\n",
      "[187]\tvalidation_0-mlogloss:1.10380\n",
      "[188]\tvalidation_0-mlogloss:1.10291\n",
      "[189]\tvalidation_0-mlogloss:1.10258\n",
      "[190]\tvalidation_0-mlogloss:1.10149\n",
      "[191]\tvalidation_0-mlogloss:1.10016\n",
      "[192]\tvalidation_0-mlogloss:1.09878\n",
      "[193]\tvalidation_0-mlogloss:1.09741\n",
      "[194]\tvalidation_0-mlogloss:1.09674\n",
      "[195]\tvalidation_0-mlogloss:1.09565\n",
      "[196]\tvalidation_0-mlogloss:1.09411\n",
      "[197]\tvalidation_0-mlogloss:1.09331\n",
      "[198]\tvalidation_0-mlogloss:1.09199\n",
      "[199]\tvalidation_0-mlogloss:1.09069\n",
      "[200]\tvalidation_0-mlogloss:1.08901\n",
      "[201]\tvalidation_0-mlogloss:1.08800\n",
      "[202]\tvalidation_0-mlogloss:1.08715\n",
      "[203]\tvalidation_0-mlogloss:1.08598\n",
      "[204]\tvalidation_0-mlogloss:1.08494\n",
      "[205]\tvalidation_0-mlogloss:1.08484\n",
      "[206]\tvalidation_0-mlogloss:1.08340\n",
      "[207]\tvalidation_0-mlogloss:1.08194\n",
      "[208]\tvalidation_0-mlogloss:1.08173\n",
      "[209]\tvalidation_0-mlogloss:1.08100\n",
      "[210]\tvalidation_0-mlogloss:1.08008\n",
      "[211]\tvalidation_0-mlogloss:1.07894\n",
      "[212]\tvalidation_0-mlogloss:1.07809\n",
      "[213]\tvalidation_0-mlogloss:1.07718\n",
      "[214]\tvalidation_0-mlogloss:1.07628\n",
      "[215]\tvalidation_0-mlogloss:1.07494\n",
      "[216]\tvalidation_0-mlogloss:1.07406\n",
      "[217]\tvalidation_0-mlogloss:1.07320\n",
      "[218]\tvalidation_0-mlogloss:1.07200\n",
      "[219]\tvalidation_0-mlogloss:1.07166\n",
      "[220]\tvalidation_0-mlogloss:1.07034\n",
      "[221]\tvalidation_0-mlogloss:1.07070\n",
      "[222]\tvalidation_0-mlogloss:1.06975\n",
      "[223]\tvalidation_0-mlogloss:1.06898\n",
      "[224]\tvalidation_0-mlogloss:1.06776\n",
      "[225]\tvalidation_0-mlogloss:1.06617\n",
      "[226]\tvalidation_0-mlogloss:1.06506\n",
      "[227]\tvalidation_0-mlogloss:1.06434\n",
      "[228]\tvalidation_0-mlogloss:1.06342\n",
      "[229]\tvalidation_0-mlogloss:1.06314\n",
      "[230]\tvalidation_0-mlogloss:1.06292\n",
      "[231]\tvalidation_0-mlogloss:1.06282\n",
      "[232]\tvalidation_0-mlogloss:1.06188\n",
      "[233]\tvalidation_0-mlogloss:1.06143\n",
      "[234]\tvalidation_0-mlogloss:1.06107\n",
      "[235]\tvalidation_0-mlogloss:1.06058\n",
      "[236]\tvalidation_0-mlogloss:1.06043\n",
      "[237]\tvalidation_0-mlogloss:1.05974\n",
      "[238]\tvalidation_0-mlogloss:1.05859\n",
      "[239]\tvalidation_0-mlogloss:1.05797\n",
      "[240]\tvalidation_0-mlogloss:1.05810\n",
      "[241]\tvalidation_0-mlogloss:1.05663\n",
      "[242]\tvalidation_0-mlogloss:1.05627\n",
      "[243]\tvalidation_0-mlogloss:1.05525\n",
      "[244]\tvalidation_0-mlogloss:1.05389\n",
      "[245]\tvalidation_0-mlogloss:1.05329\n",
      "[246]\tvalidation_0-mlogloss:1.05272\n",
      "[247]\tvalidation_0-mlogloss:1.05238\n",
      "[248]\tvalidation_0-mlogloss:1.05198\n",
      "[249]\tvalidation_0-mlogloss:1.05160\n",
      "[250]\tvalidation_0-mlogloss:1.05033\n",
      "[251]\tvalidation_0-mlogloss:1.05000\n",
      "[252]\tvalidation_0-mlogloss:1.04956\n",
      "[253]\tvalidation_0-mlogloss:1.04889\n",
      "[254]\tvalidation_0-mlogloss:1.04867\n",
      "[255]\tvalidation_0-mlogloss:1.04733\n",
      "[256]\tvalidation_0-mlogloss:1.04661\n",
      "[257]\tvalidation_0-mlogloss:1.04598\n",
      "[258]\tvalidation_0-mlogloss:1.04536\n",
      "[259]\tvalidation_0-mlogloss:1.04466\n",
      "[260]\tvalidation_0-mlogloss:1.04401\n",
      "[261]\tvalidation_0-mlogloss:1.04327\n",
      "[262]\tvalidation_0-mlogloss:1.04295\n",
      "[263]\tvalidation_0-mlogloss:1.04260\n",
      "[264]\tvalidation_0-mlogloss:1.04253\n",
      "[265]\tvalidation_0-mlogloss:1.04173\n",
      "[266]\tvalidation_0-mlogloss:1.04145\n",
      "[267]\tvalidation_0-mlogloss:1.04065\n",
      "[268]\tvalidation_0-mlogloss:1.04015\n",
      "[269]\tvalidation_0-mlogloss:1.04009\n",
      "[270]\tvalidation_0-mlogloss:1.03909\n",
      "[271]\tvalidation_0-mlogloss:1.03847\n",
      "[272]\tvalidation_0-mlogloss:1.03795\n",
      "[273]\tvalidation_0-mlogloss:1.03765\n",
      "[274]\tvalidation_0-mlogloss:1.03724\n",
      "[275]\tvalidation_0-mlogloss:1.03729\n",
      "[276]\tvalidation_0-mlogloss:1.03705\n",
      "[277]\tvalidation_0-mlogloss:1.03661\n",
      "[278]\tvalidation_0-mlogloss:1.03567\n",
      "[279]\tvalidation_0-mlogloss:1.03526\n",
      "[280]\tvalidation_0-mlogloss:1.03455\n",
      "[281]\tvalidation_0-mlogloss:1.03440\n",
      "[282]\tvalidation_0-mlogloss:1.03419\n",
      "[283]\tvalidation_0-mlogloss:1.03388\n",
      "[284]\tvalidation_0-mlogloss:1.03300\n",
      "[285]\tvalidation_0-mlogloss:1.03275\n",
      "[286]\tvalidation_0-mlogloss:1.03214\n",
      "[287]\tvalidation_0-mlogloss:1.03226\n",
      "[288]\tvalidation_0-mlogloss:1.03189\n",
      "[289]\tvalidation_0-mlogloss:1.03178\n",
      "[290]\tvalidation_0-mlogloss:1.03097\n",
      "[291]\tvalidation_0-mlogloss:1.03050\n",
      "[292]\tvalidation_0-mlogloss:1.02992\n",
      "[293]\tvalidation_0-mlogloss:1.02971\n",
      "[294]\tvalidation_0-mlogloss:1.02941\n",
      "[295]\tvalidation_0-mlogloss:1.02957\n",
      "[296]\tvalidation_0-mlogloss:1.02957\n",
      "[297]\tvalidation_0-mlogloss:1.02861\n",
      "[298]\tvalidation_0-mlogloss:1.02834\n",
      "[299]\tvalidation_0-mlogloss:1.02842\n",
      "[300]\tvalidation_0-mlogloss:1.02816\n",
      "[301]\tvalidation_0-mlogloss:1.02801\n",
      "[302]\tvalidation_0-mlogloss:1.02853\n",
      "[303]\tvalidation_0-mlogloss:1.02785\n",
      "[304]\tvalidation_0-mlogloss:1.02787\n",
      "[305]\tvalidation_0-mlogloss:1.02728\n",
      "[306]\tvalidation_0-mlogloss:1.02719\n",
      "[307]\tvalidation_0-mlogloss:1.02713\n",
      "[308]\tvalidation_0-mlogloss:1.02680\n",
      "[309]\tvalidation_0-mlogloss:1.02637\n",
      "[310]\tvalidation_0-mlogloss:1.02595\n",
      "[311]\tvalidation_0-mlogloss:1.02575\n",
      "[312]\tvalidation_0-mlogloss:1.02560\n",
      "[313]\tvalidation_0-mlogloss:1.02537\n",
      "[314]\tvalidation_0-mlogloss:1.02485\n",
      "[315]\tvalidation_0-mlogloss:1.02494\n",
      "[316]\tvalidation_0-mlogloss:1.02480\n",
      "[317]\tvalidation_0-mlogloss:1.02485\n",
      "[318]\tvalidation_0-mlogloss:1.02410\n",
      "[319]\tvalidation_0-mlogloss:1.02348\n",
      "[320]\tvalidation_0-mlogloss:1.02319\n",
      "[321]\tvalidation_0-mlogloss:1.02319\n",
      "[322]\tvalidation_0-mlogloss:1.02238\n",
      "[323]\tvalidation_0-mlogloss:1.02202\n",
      "[324]\tvalidation_0-mlogloss:1.02158\n",
      "[325]\tvalidation_0-mlogloss:1.02155\n",
      "[326]\tvalidation_0-mlogloss:1.02151\n",
      "[327]\tvalidation_0-mlogloss:1.02079\n",
      "[328]\tvalidation_0-mlogloss:1.02071\n",
      "[329]\tvalidation_0-mlogloss:1.02078\n",
      "[330]\tvalidation_0-mlogloss:1.01993\n",
      "[331]\tvalidation_0-mlogloss:1.01944\n",
      "[332]\tvalidation_0-mlogloss:1.01923\n",
      "[333]\tvalidation_0-mlogloss:1.01911\n",
      "[334]\tvalidation_0-mlogloss:1.01843\n",
      "[335]\tvalidation_0-mlogloss:1.01789\n",
      "[336]\tvalidation_0-mlogloss:1.01731\n",
      "[337]\tvalidation_0-mlogloss:1.01721\n",
      "[338]\tvalidation_0-mlogloss:1.01717\n",
      "[339]\tvalidation_0-mlogloss:1.01670\n",
      "[340]\tvalidation_0-mlogloss:1.01663\n",
      "[341]\tvalidation_0-mlogloss:1.01656\n",
      "[342]\tvalidation_0-mlogloss:1.01661\n",
      "[343]\tvalidation_0-mlogloss:1.01618\n",
      "[344]\tvalidation_0-mlogloss:1.01624\n",
      "[345]\tvalidation_0-mlogloss:1.01615\n",
      "[346]\tvalidation_0-mlogloss:1.01589\n",
      "[347]\tvalidation_0-mlogloss:1.01534\n",
      "[348]\tvalidation_0-mlogloss:1.01577\n",
      "[349]\tvalidation_0-mlogloss:1.01546\n",
      "[350]\tvalidation_0-mlogloss:1.01448\n",
      "[351]\tvalidation_0-mlogloss:1.01428\n",
      "[352]\tvalidation_0-mlogloss:1.01349\n",
      "[353]\tvalidation_0-mlogloss:1.01315\n",
      "[354]\tvalidation_0-mlogloss:1.01306\n",
      "[355]\tvalidation_0-mlogloss:1.01329\n",
      "[356]\tvalidation_0-mlogloss:1.01293\n",
      "[357]\tvalidation_0-mlogloss:1.01295\n",
      "[358]\tvalidation_0-mlogloss:1.01206\n",
      "[359]\tvalidation_0-mlogloss:1.01203\n",
      "[360]\tvalidation_0-mlogloss:1.01192\n",
      "[361]\tvalidation_0-mlogloss:1.01181\n",
      "[362]\tvalidation_0-mlogloss:1.01189\n",
      "[363]\tvalidation_0-mlogloss:1.01124\n",
      "[364]\tvalidation_0-mlogloss:1.01083\n",
      "[365]\tvalidation_0-mlogloss:1.01004\n",
      "[366]\tvalidation_0-mlogloss:1.00982\n",
      "[367]\tvalidation_0-mlogloss:1.00972\n",
      "[368]\tvalidation_0-mlogloss:1.00982\n",
      "[369]\tvalidation_0-mlogloss:1.00933\n",
      "[370]\tvalidation_0-mlogloss:1.00868\n",
      "[371]\tvalidation_0-mlogloss:1.00856\n",
      "[372]\tvalidation_0-mlogloss:1.00847\n",
      "[373]\tvalidation_0-mlogloss:1.00893\n",
      "[374]\tvalidation_0-mlogloss:1.00862\n",
      "[375]\tvalidation_0-mlogloss:1.00826\n",
      "[376]\tvalidation_0-mlogloss:1.00791\n",
      "[377]\tvalidation_0-mlogloss:1.00730\n",
      "[378]\tvalidation_0-mlogloss:1.00698\n",
      "[379]\tvalidation_0-mlogloss:1.00717\n",
      "[380]\tvalidation_0-mlogloss:1.00690\n",
      "[381]\tvalidation_0-mlogloss:1.00606\n",
      "[382]\tvalidation_0-mlogloss:1.00597\n",
      "[383]\tvalidation_0-mlogloss:1.00487\n",
      "[384]\tvalidation_0-mlogloss:1.00479\n",
      "[385]\tvalidation_0-mlogloss:1.00465\n",
      "[386]\tvalidation_0-mlogloss:1.00477\n",
      "[387]\tvalidation_0-mlogloss:1.00433\n",
      "[388]\tvalidation_0-mlogloss:1.00416\n",
      "[389]\tvalidation_0-mlogloss:1.00407\n",
      "[390]\tvalidation_0-mlogloss:1.00350\n",
      "[391]\tvalidation_0-mlogloss:1.00317\n",
      "[392]\tvalidation_0-mlogloss:1.00292\n",
      "[393]\tvalidation_0-mlogloss:1.00263\n",
      "[394]\tvalidation_0-mlogloss:1.00266\n",
      "[395]\tvalidation_0-mlogloss:1.00233\n",
      "[396]\tvalidation_0-mlogloss:1.00213\n",
      "[397]\tvalidation_0-mlogloss:1.00179\n",
      "[398]\tvalidation_0-mlogloss:1.00160\n",
      "[399]\tvalidation_0-mlogloss:1.00122\n",
      "[400]\tvalidation_0-mlogloss:1.00111\n",
      "[401]\tvalidation_0-mlogloss:1.00071\n",
      "[402]\tvalidation_0-mlogloss:1.00061\n",
      "[403]\tvalidation_0-mlogloss:1.00037\n",
      "[404]\tvalidation_0-mlogloss:1.00036\n",
      "[405]\tvalidation_0-mlogloss:0.99979\n",
      "[406]\tvalidation_0-mlogloss:0.99966\n",
      "[407]\tvalidation_0-mlogloss:0.99958\n",
      "[408]\tvalidation_0-mlogloss:0.99966\n",
      "[409]\tvalidation_0-mlogloss:0.99980\n",
      "[410]\tvalidation_0-mlogloss:0.99932\n",
      "[411]\tvalidation_0-mlogloss:0.99926\n",
      "[412]\tvalidation_0-mlogloss:0.99903\n",
      "[413]\tvalidation_0-mlogloss:0.99866\n",
      "[414]\tvalidation_0-mlogloss:0.99869\n",
      "[415]\tvalidation_0-mlogloss:0.99858\n",
      "[416]\tvalidation_0-mlogloss:0.99832\n",
      "[417]\tvalidation_0-mlogloss:0.99795\n",
      "[418]\tvalidation_0-mlogloss:0.99779\n",
      "[419]\tvalidation_0-mlogloss:0.99766\n",
      "[420]\tvalidation_0-mlogloss:0.99727\n",
      "[421]\tvalidation_0-mlogloss:0.99714\n",
      "[422]\tvalidation_0-mlogloss:0.99672\n",
      "[423]\tvalidation_0-mlogloss:0.99638\n",
      "[424]\tvalidation_0-mlogloss:0.99645\n",
      "[425]\tvalidation_0-mlogloss:0.99609\n",
      "[426]\tvalidation_0-mlogloss:0.99586\n",
      "[427]\tvalidation_0-mlogloss:0.99556\n",
      "[428]\tvalidation_0-mlogloss:0.99563\n",
      "[429]\tvalidation_0-mlogloss:0.99523\n",
      "[430]\tvalidation_0-mlogloss:0.99463\n",
      "[431]\tvalidation_0-mlogloss:0.99422\n",
      "[432]\tvalidation_0-mlogloss:0.99376\n",
      "[433]\tvalidation_0-mlogloss:0.99404\n",
      "[434]\tvalidation_0-mlogloss:0.99335\n",
      "[435]\tvalidation_0-mlogloss:0.99277\n",
      "[436]\tvalidation_0-mlogloss:0.99242\n",
      "[437]\tvalidation_0-mlogloss:0.99192\n",
      "[438]\tvalidation_0-mlogloss:0.99162\n",
      "[439]\tvalidation_0-mlogloss:0.99125\n",
      "[440]\tvalidation_0-mlogloss:0.99143\n",
      "[441]\tvalidation_0-mlogloss:0.99148\n",
      "[442]\tvalidation_0-mlogloss:0.99138\n",
      "[443]\tvalidation_0-mlogloss:0.99175\n",
      "[444]\tvalidation_0-mlogloss:0.99188\n",
      "[445]\tvalidation_0-mlogloss:0.99210\n",
      "[446]\tvalidation_0-mlogloss:0.99172\n",
      "[447]\tvalidation_0-mlogloss:0.99128\n",
      "[448]\tvalidation_0-mlogloss:0.99095\n",
      "[449]\tvalidation_0-mlogloss:0.99080\n",
      "[450]\tvalidation_0-mlogloss:0.99082\n",
      "[451]\tvalidation_0-mlogloss:0.99059\n",
      "[452]\tvalidation_0-mlogloss:0.99033\n",
      "[453]\tvalidation_0-mlogloss:0.99027\n",
      "[454]\tvalidation_0-mlogloss:0.98998\n",
      "[455]\tvalidation_0-mlogloss:0.98958\n",
      "[456]\tvalidation_0-mlogloss:0.98954\n",
      "[457]\tvalidation_0-mlogloss:0.98964\n",
      "[458]\tvalidation_0-mlogloss:0.98935\n",
      "[459]\tvalidation_0-mlogloss:0.98910\n",
      "[460]\tvalidation_0-mlogloss:0.98864\n",
      "[461]\tvalidation_0-mlogloss:0.98838\n",
      "[462]\tvalidation_0-mlogloss:0.98809\n",
      "[463]\tvalidation_0-mlogloss:0.98815\n",
      "[464]\tvalidation_0-mlogloss:0.98835\n",
      "[465]\tvalidation_0-mlogloss:0.98841\n",
      "[466]\tvalidation_0-mlogloss:0.98805\n",
      "[467]\tvalidation_0-mlogloss:0.98788\n",
      "[468]\tvalidation_0-mlogloss:0.98781\n",
      "[469]\tvalidation_0-mlogloss:0.98768\n",
      "[470]\tvalidation_0-mlogloss:0.98744\n",
      "[471]\tvalidation_0-mlogloss:0.98752\n",
      "[472]\tvalidation_0-mlogloss:0.98755\n",
      "[473]\tvalidation_0-mlogloss:0.98764\n",
      "[474]\tvalidation_0-mlogloss:0.98758\n",
      "[475]\tvalidation_0-mlogloss:0.98760\n",
      "[476]\tvalidation_0-mlogloss:0.98776\n",
      "[477]\tvalidation_0-mlogloss:0.98777\n",
      "[478]\tvalidation_0-mlogloss:0.98753\n",
      "[479]\tvalidation_0-mlogloss:0.98735\n",
      "[480]\tvalidation_0-mlogloss:0.98696\n",
      "[481]\tvalidation_0-mlogloss:0.98656\n",
      "[482]\tvalidation_0-mlogloss:0.98639\n",
      "[483]\tvalidation_0-mlogloss:0.98612\n",
      "[484]\tvalidation_0-mlogloss:0.98629\n",
      "[485]\tvalidation_0-mlogloss:0.98635\n",
      "[486]\tvalidation_0-mlogloss:0.98637\n",
      "[487]\tvalidation_0-mlogloss:0.98665\n",
      "[488]\tvalidation_0-mlogloss:0.98666\n",
      "[489]\tvalidation_0-mlogloss:0.98644\n",
      "[490]\tvalidation_0-mlogloss:0.98625\n",
      "[491]\tvalidation_0-mlogloss:0.98612\n",
      "[492]\tvalidation_0-mlogloss:0.98571\n",
      "[493]\tvalidation_0-mlogloss:0.98549\n",
      "[494]\tvalidation_0-mlogloss:0.98487\n",
      "[495]\tvalidation_0-mlogloss:0.98479\n",
      "[496]\tvalidation_0-mlogloss:0.98476\n",
      "[497]\tvalidation_0-mlogloss:0.98452\n",
      "[498]\tvalidation_0-mlogloss:0.98389\n",
      "[499]\tvalidation_0-mlogloss:0.98389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      1499\n",
      "           1       0.71      0.77      0.74      1499\n",
      "           2       0.69      0.70      0.69      1499\n",
      "           3       0.74      0.42      0.54      1499\n",
      "           4       0.92      0.95      0.93      1499\n",
      "           5       0.88      0.93      0.91      1499\n",
      "           6       0.83      0.96      0.89      1499\n",
      "           7       0.93      0.80      0.86      1499\n",
      "           8       0.45      0.85      0.59      1499\n",
      "           9       0.75      0.72      0.73      1499\n",
      "          10       0.96      0.91      0.93      1499\n",
      "          11       0.95      0.94      0.95      1499\n",
      "          12       0.67      0.86      0.76      1499\n",
      "          13       0.85      0.90      0.87      1499\n",
      "          14       0.58      0.78      0.66      1499\n",
      "          15       0.87      0.43      0.58      1499\n",
      "          16       0.29      0.16      0.21      1499\n",
      "          17       0.77      0.88      0.82      1499\n",
      "          18       0.73      0.75      0.74      1499\n",
      "          19       0.66      0.63      0.65      1499\n",
      "          20       0.96      0.99      0.97      1499\n",
      "          21       0.72      0.84      0.78      1499\n",
      "          22       0.69      0.77      0.73      1499\n",
      "          23       0.91      0.98      0.94      1499\n",
      "          24       0.94      0.95      0.94      1499\n",
      "          25       0.75      0.58      0.65      1499\n",
      "          26       0.62      0.49      0.55      1499\n",
      "          27       0.71      0.54      0.61      1499\n",
      "          28       0.81      0.48      0.60      1499\n",
      "          29       0.76      0.90      0.82      1499\n",
      "          30       0.71      0.85      0.77      1499\n",
      "          31       0.57      0.86      0.68      1499\n",
      "          32       0.89      0.90      0.90      1499\n",
      "          33       0.87      0.49      0.62      1499\n",
      "          34       0.91      0.99      0.95      1499\n",
      "          35       0.78      0.86      0.82      1499\n",
      "          36       0.87      0.79      0.83      1499\n",
      "          37       0.71      0.80      0.75      1499\n",
      "          38       0.66      0.33      0.44      1499\n",
      "          39       0.80      0.74      0.77      1499\n",
      "          40       0.88      0.70      0.78      1499\n",
      "          41       0.69      0.54      0.61      1499\n",
      "          42       0.67      0.54      0.60      1499\n",
      "          43       0.70      0.78      0.74      1499\n",
      "          44       0.86      0.95      0.90      1499\n",
      "          45       0.64      0.59      0.62      1499\n",
      "          46       0.62      0.90      0.74      1499\n",
      "          47       0.81      0.54      0.65      1499\n",
      "          48       0.78      0.91      0.84      1499\n",
      "          49       0.71      0.79      0.75      1499\n",
      "\n",
      "    accuracy                           0.75     74950\n",
      "   macro avg       0.76      0.75      0.74     74950\n",
      "weighted avg       0.76      0.75      0.74     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8f654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "300aa492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 12:46:40.351159: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 76800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 26s 1ms/step - loss: 1.1356 - val_loss: 1.8813\n",
      "Epoch 2/10\n",
      "15529/18750 [=======================>......] - ETA: 4s - loss: 0.4705"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.4570 - val_loss: 1.8390\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.3354 - val_loss: 1.6959\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.2775 - val_loss: 1.8002\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.2420 - val_loss: 1.6499\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.2172 - val_loss: 1.6240\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.1991 - val_loss: 1.6648\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.1846 - val_loss: 1.7991\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.1747 - val_loss: 1.8616\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.1666 - val_loss: 1.7579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc198d8ab30>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 2s 842us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.67      1499\n",
      "           1       0.62      0.80      0.70      1499\n",
      "           2       0.62      0.60      0.61      1499\n",
      "           3       0.54      0.62      0.57      1499\n",
      "           4       0.89      0.94      0.91      1499\n",
      "           5       0.84      0.94      0.89      1499\n",
      "           6       0.97      0.88      0.92      1499\n",
      "           7       0.87      0.77      0.82      1499\n",
      "           8       0.36      0.87      0.51      1499\n",
      "           9       0.71      0.80      0.75      1499\n",
      "          10       0.79      0.90      0.84      1499\n",
      "          11       0.90      0.97      0.93      1499\n",
      "          12       0.65      0.76      0.70      1499\n",
      "          13       0.64      0.85      0.73      1499\n",
      "          14       0.48      0.75      0.59      1499\n",
      "          15       0.97      0.09      0.17      1499\n",
      "          16       0.17      0.12      0.14      1499\n",
      "          17       0.96      0.60      0.74      1499\n",
      "          18       0.73      0.80      0.76      1499\n",
      "          19       0.57      0.43      0.49      1499\n",
      "          20       0.99      0.94      0.97      1499\n",
      "          21       0.84      0.74      0.79      1499\n",
      "          22       0.78      0.67      0.72      1499\n",
      "          23       0.89      0.97      0.93      1499\n",
      "          24       0.85      0.82      0.84      1499\n",
      "          25       0.54      0.17      0.25      1499\n",
      "          26       0.84      0.54      0.66      1499\n",
      "          27       0.65      0.47      0.54      1499\n",
      "          28       0.74      0.54      0.62      1499\n",
      "          29       0.75      0.87      0.81      1499\n",
      "          30       0.89      0.79      0.84      1499\n",
      "          31       0.70      0.64      0.67      1499\n",
      "          32       0.77      0.88      0.82      1499\n",
      "          33       0.80      0.44      0.57      1499\n",
      "          34       0.87      0.98      0.92      1499\n",
      "          35       0.83      0.79      0.81      1499\n",
      "          36       0.81      0.66      0.72      1499\n",
      "          37       0.62      0.74      0.67      1499\n",
      "          38       0.53      0.45      0.49      1499\n",
      "          39       0.59      0.77      0.67      1499\n",
      "          40       0.83      0.60      0.70      1499\n",
      "          41       0.85      0.68      0.75      1499\n",
      "          42       0.46      0.60      0.52      1499\n",
      "          43       0.66      0.72      0.69      1499\n",
      "          44       0.79      0.94      0.86      1499\n",
      "          45       0.57      0.58      0.57      1499\n",
      "          46       0.48      0.81      0.60      1499\n",
      "          47       0.81      0.53      0.64      1499\n",
      "          48       0.57      0.95      0.71      1499\n",
      "          49       0.80      0.56      0.66      1499\n",
      "\n",
      "    accuracy                           0.70     74950\n",
      "   macro avg       0.73      0.70      0.69     74950\n",
      "weighted avg       0.73      0.70      0.69     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:3.05913\n",
      "[1]\tvalidation_0-mlogloss:2.76593\n",
      "[2]\tvalidation_0-mlogloss:2.59051\n",
      "[3]\tvalidation_0-mlogloss:2.45501\n",
      "[4]\tvalidation_0-mlogloss:2.33595\n",
      "[5]\tvalidation_0-mlogloss:2.24296\n",
      "[6]\tvalidation_0-mlogloss:2.15791\n",
      "[7]\tvalidation_0-mlogloss:2.09009\n",
      "[8]\tvalidation_0-mlogloss:2.02049\n",
      "[9]\tvalidation_0-mlogloss:1.96296\n",
      "[10]\tvalidation_0-mlogloss:1.91300\n",
      "[11]\tvalidation_0-mlogloss:1.86588\n",
      "[12]\tvalidation_0-mlogloss:1.82345\n",
      "[13]\tvalidation_0-mlogloss:1.78351\n",
      "[14]\tvalidation_0-mlogloss:1.74389\n",
      "[15]\tvalidation_0-mlogloss:1.70949\n",
      "[16]\tvalidation_0-mlogloss:1.67419\n",
      "[17]\tvalidation_0-mlogloss:1.64566\n",
      "[18]\tvalidation_0-mlogloss:1.61226\n",
      "[19]\tvalidation_0-mlogloss:1.58593\n",
      "[20]\tvalidation_0-mlogloss:1.55834\n",
      "[21]\tvalidation_0-mlogloss:1.53123\n",
      "[22]\tvalidation_0-mlogloss:1.50894\n",
      "[23]\tvalidation_0-mlogloss:1.48734\n",
      "[24]\tvalidation_0-mlogloss:1.46692\n",
      "[25]\tvalidation_0-mlogloss:1.44579\n",
      "[26]\tvalidation_0-mlogloss:1.42670\n",
      "[27]\tvalidation_0-mlogloss:1.40704\n",
      "[28]\tvalidation_0-mlogloss:1.38974\n",
      "[29]\tvalidation_0-mlogloss:1.37204\n",
      "[30]\tvalidation_0-mlogloss:1.35498\n",
      "[31]\tvalidation_0-mlogloss:1.33906\n",
      "[32]\tvalidation_0-mlogloss:1.32268\n",
      "[33]\tvalidation_0-mlogloss:1.30504\n",
      "[34]\tvalidation_0-mlogloss:1.29104\n",
      "[35]\tvalidation_0-mlogloss:1.27883\n",
      "[36]\tvalidation_0-mlogloss:1.26577\n",
      "[37]\tvalidation_0-mlogloss:1.25349\n",
      "[38]\tvalidation_0-mlogloss:1.24172\n",
      "[39]\tvalidation_0-mlogloss:1.23007\n",
      "[40]\tvalidation_0-mlogloss:1.21745\n",
      "[41]\tvalidation_0-mlogloss:1.20522\n",
      "[42]\tvalidation_0-mlogloss:1.19382\n",
      "[43]\tvalidation_0-mlogloss:1.18379\n",
      "[44]\tvalidation_0-mlogloss:1.17282\n",
      "[45]\tvalidation_0-mlogloss:1.16162\n",
      "[46]\tvalidation_0-mlogloss:1.15080\n",
      "[47]\tvalidation_0-mlogloss:1.14159\n",
      "[48]\tvalidation_0-mlogloss:1.13010\n",
      "[49]\tvalidation_0-mlogloss:1.12170\n",
      "[50]\tvalidation_0-mlogloss:1.11048\n",
      "[51]\tvalidation_0-mlogloss:1.10195\n",
      "[52]\tvalidation_0-mlogloss:1.09378\n",
      "[53]\tvalidation_0-mlogloss:1.08526\n",
      "[54]\tvalidation_0-mlogloss:1.07746\n",
      "[55]\tvalidation_0-mlogloss:1.06875\n",
      "[56]\tvalidation_0-mlogloss:1.05975\n",
      "[57]\tvalidation_0-mlogloss:1.05200\n",
      "[58]\tvalidation_0-mlogloss:1.04293\n",
      "[59]\tvalidation_0-mlogloss:1.03422\n",
      "[60]\tvalidation_0-mlogloss:1.02586\n",
      "[61]\tvalidation_0-mlogloss:1.01949\n",
      "[62]\tvalidation_0-mlogloss:1.01268\n",
      "[63]\tvalidation_0-mlogloss:1.00512\n",
      "[64]\tvalidation_0-mlogloss:0.99888\n",
      "[65]\tvalidation_0-mlogloss:0.99320\n",
      "[66]\tvalidation_0-mlogloss:0.98594\n",
      "[67]\tvalidation_0-mlogloss:0.97943\n",
      "[68]\tvalidation_0-mlogloss:0.97165\n",
      "[69]\tvalidation_0-mlogloss:0.96553\n",
      "[70]\tvalidation_0-mlogloss:0.95821\n",
      "[71]\tvalidation_0-mlogloss:0.95162\n",
      "[72]\tvalidation_0-mlogloss:0.94537\n",
      "[73]\tvalidation_0-mlogloss:0.94004\n",
      "[74]\tvalidation_0-mlogloss:0.93415\n",
      "[75]\tvalidation_0-mlogloss:0.92951\n",
      "[76]\tvalidation_0-mlogloss:0.92436\n",
      "[77]\tvalidation_0-mlogloss:0.91904\n",
      "[78]\tvalidation_0-mlogloss:0.91534\n",
      "[79]\tvalidation_0-mlogloss:0.91016\n",
      "[80]\tvalidation_0-mlogloss:0.90555\n",
      "[81]\tvalidation_0-mlogloss:0.89954\n",
      "[82]\tvalidation_0-mlogloss:0.89439\n",
      "[83]\tvalidation_0-mlogloss:0.88901\n",
      "[84]\tvalidation_0-mlogloss:0.88387\n",
      "[85]\tvalidation_0-mlogloss:0.87794\n",
      "[86]\tvalidation_0-mlogloss:0.87335\n",
      "[87]\tvalidation_0-mlogloss:0.86921\n",
      "[88]\tvalidation_0-mlogloss:0.86517\n",
      "[89]\tvalidation_0-mlogloss:0.86140\n",
      "[90]\tvalidation_0-mlogloss:0.85685\n",
      "[91]\tvalidation_0-mlogloss:0.85254\n",
      "[92]\tvalidation_0-mlogloss:0.84978\n",
      "[93]\tvalidation_0-mlogloss:0.84616\n",
      "[94]\tvalidation_0-mlogloss:0.84325\n",
      "[95]\tvalidation_0-mlogloss:0.83841\n",
      "[96]\tvalidation_0-mlogloss:0.83461\n",
      "[97]\tvalidation_0-mlogloss:0.82994\n",
      "[98]\tvalidation_0-mlogloss:0.82653\n",
      "[99]\tvalidation_0-mlogloss:0.82278\n",
      "[100]\tvalidation_0-mlogloss:0.81952\n",
      "[101]\tvalidation_0-mlogloss:0.81644\n",
      "[102]\tvalidation_0-mlogloss:0.81427\n",
      "[103]\tvalidation_0-mlogloss:0.81056\n",
      "[104]\tvalidation_0-mlogloss:0.80693\n",
      "[105]\tvalidation_0-mlogloss:0.80398\n",
      "[106]\tvalidation_0-mlogloss:0.80122\n",
      "[107]\tvalidation_0-mlogloss:0.79823\n",
      "[108]\tvalidation_0-mlogloss:0.79427\n",
      "[109]\tvalidation_0-mlogloss:0.79174\n",
      "[110]\tvalidation_0-mlogloss:0.78916\n",
      "[111]\tvalidation_0-mlogloss:0.78662\n",
      "[112]\tvalidation_0-mlogloss:0.78475\n",
      "[113]\tvalidation_0-mlogloss:0.78182\n",
      "[114]\tvalidation_0-mlogloss:0.77924\n",
      "[115]\tvalidation_0-mlogloss:0.77707\n",
      "[116]\tvalidation_0-mlogloss:0.77309\n",
      "[117]\tvalidation_0-mlogloss:0.77077\n",
      "[118]\tvalidation_0-mlogloss:0.76761\n",
      "[119]\tvalidation_0-mlogloss:0.76502\n",
      "[120]\tvalidation_0-mlogloss:0.76198\n",
      "[121]\tvalidation_0-mlogloss:0.75925\n",
      "[122]\tvalidation_0-mlogloss:0.75594\n",
      "[123]\tvalidation_0-mlogloss:0.75292\n",
      "[124]\tvalidation_0-mlogloss:0.75066\n",
      "[125]\tvalidation_0-mlogloss:0.74952\n",
      "[126]\tvalidation_0-mlogloss:0.74672\n",
      "[127]\tvalidation_0-mlogloss:0.74398\n",
      "[128]\tvalidation_0-mlogloss:0.74119\n",
      "[129]\tvalidation_0-mlogloss:0.73899\n",
      "[130]\tvalidation_0-mlogloss:0.73729\n",
      "[131]\tvalidation_0-mlogloss:0.73431\n",
      "[132]\tvalidation_0-mlogloss:0.73235\n",
      "[133]\tvalidation_0-mlogloss:0.73121\n",
      "[134]\tvalidation_0-mlogloss:0.72986\n",
      "[135]\tvalidation_0-mlogloss:0.72788\n",
      "[136]\tvalidation_0-mlogloss:0.72600\n",
      "[137]\tvalidation_0-mlogloss:0.72375\n",
      "[138]\tvalidation_0-mlogloss:0.72167\n",
      "[139]\tvalidation_0-mlogloss:0.71986\n",
      "[140]\tvalidation_0-mlogloss:0.71795\n",
      "[141]\tvalidation_0-mlogloss:0.71608\n",
      "[142]\tvalidation_0-mlogloss:0.71420\n",
      "[143]\tvalidation_0-mlogloss:0.71261\n",
      "[144]\tvalidation_0-mlogloss:0.71120\n",
      "[145]\tvalidation_0-mlogloss:0.70904\n",
      "[146]\tvalidation_0-mlogloss:0.70685\n",
      "[147]\tvalidation_0-mlogloss:0.70499\n",
      "[148]\tvalidation_0-mlogloss:0.70378\n",
      "[149]\tvalidation_0-mlogloss:0.70222\n",
      "[150]\tvalidation_0-mlogloss:0.70062\n",
      "[151]\tvalidation_0-mlogloss:0.69891\n",
      "[152]\tvalidation_0-mlogloss:0.69735\n",
      "[153]\tvalidation_0-mlogloss:0.69555\n",
      "[154]\tvalidation_0-mlogloss:0.69385\n",
      "[155]\tvalidation_0-mlogloss:0.69256\n",
      "[156]\tvalidation_0-mlogloss:0.69020\n",
      "[157]\tvalidation_0-mlogloss:0.68904\n",
      "[158]\tvalidation_0-mlogloss:0.68703\n",
      "[159]\tvalidation_0-mlogloss:0.68551\n",
      "[160]\tvalidation_0-mlogloss:0.68380\n",
      "[161]\tvalidation_0-mlogloss:0.68243\n",
      "[162]\tvalidation_0-mlogloss:0.68148\n",
      "[163]\tvalidation_0-mlogloss:0.68029\n",
      "[164]\tvalidation_0-mlogloss:0.67864\n",
      "[165]\tvalidation_0-mlogloss:0.67724\n",
      "[166]\tvalidation_0-mlogloss:0.67628\n",
      "[167]\tvalidation_0-mlogloss:0.67488\n",
      "[168]\tvalidation_0-mlogloss:0.67370\n",
      "[169]\tvalidation_0-mlogloss:0.67219\n",
      "[170]\tvalidation_0-mlogloss:0.67095\n",
      "[171]\tvalidation_0-mlogloss:0.66935\n",
      "[172]\tvalidation_0-mlogloss:0.66819\n",
      "[173]\tvalidation_0-mlogloss:0.66664\n",
      "[174]\tvalidation_0-mlogloss:0.66502\n",
      "[175]\tvalidation_0-mlogloss:0.66360\n",
      "[176]\tvalidation_0-mlogloss:0.66251\n",
      "[177]\tvalidation_0-mlogloss:0.66203\n",
      "[178]\tvalidation_0-mlogloss:0.66034\n",
      "[179]\tvalidation_0-mlogloss:0.65926\n",
      "[180]\tvalidation_0-mlogloss:0.65843\n",
      "[181]\tvalidation_0-mlogloss:0.65613\n",
      "[182]\tvalidation_0-mlogloss:0.65592\n",
      "[183]\tvalidation_0-mlogloss:0.65484\n",
      "[184]\tvalidation_0-mlogloss:0.65298\n",
      "[185]\tvalidation_0-mlogloss:0.65184\n",
      "[186]\tvalidation_0-mlogloss:0.65020\n",
      "[187]\tvalidation_0-mlogloss:0.64946\n",
      "[188]\tvalidation_0-mlogloss:0.64880\n",
      "[189]\tvalidation_0-mlogloss:0.64799\n",
      "[190]\tvalidation_0-mlogloss:0.64636\n",
      "[191]\tvalidation_0-mlogloss:0.64568\n",
      "[192]\tvalidation_0-mlogloss:0.64469\n",
      "[193]\tvalidation_0-mlogloss:0.64346\n",
      "[194]\tvalidation_0-mlogloss:0.64230\n",
      "[195]\tvalidation_0-mlogloss:0.64199\n",
      "[196]\tvalidation_0-mlogloss:0.64097\n",
      "[197]\tvalidation_0-mlogloss:0.63985\n",
      "[198]\tvalidation_0-mlogloss:0.63843\n",
      "[199]\tvalidation_0-mlogloss:0.63690\n",
      "[200]\tvalidation_0-mlogloss:0.63623\n",
      "[201]\tvalidation_0-mlogloss:0.63557\n",
      "[202]\tvalidation_0-mlogloss:0.63515\n",
      "[203]\tvalidation_0-mlogloss:0.63406\n",
      "[204]\tvalidation_0-mlogloss:0.63274\n",
      "[205]\tvalidation_0-mlogloss:0.63178\n",
      "[206]\tvalidation_0-mlogloss:0.63070\n",
      "[207]\tvalidation_0-mlogloss:0.62982\n",
      "[208]\tvalidation_0-mlogloss:0.62870\n",
      "[209]\tvalidation_0-mlogloss:0.62742\n",
      "[210]\tvalidation_0-mlogloss:0.62664\n",
      "[211]\tvalidation_0-mlogloss:0.62602\n",
      "[212]\tvalidation_0-mlogloss:0.62557\n",
      "[213]\tvalidation_0-mlogloss:0.62458\n",
      "[214]\tvalidation_0-mlogloss:0.62381\n",
      "[215]\tvalidation_0-mlogloss:0.62255\n",
      "[216]\tvalidation_0-mlogloss:0.62154\n",
      "[217]\tvalidation_0-mlogloss:0.62086\n",
      "[218]\tvalidation_0-mlogloss:0.61935\n",
      "[219]\tvalidation_0-mlogloss:0.61812\n",
      "[220]\tvalidation_0-mlogloss:0.61701\n",
      "[221]\tvalidation_0-mlogloss:0.61643\n",
      "[222]\tvalidation_0-mlogloss:0.61540\n",
      "[223]\tvalidation_0-mlogloss:0.61420\n",
      "[224]\tvalidation_0-mlogloss:0.61337\n",
      "[225]\tvalidation_0-mlogloss:0.61288\n",
      "[226]\tvalidation_0-mlogloss:0.61231\n",
      "[227]\tvalidation_0-mlogloss:0.61131\n",
      "[228]\tvalidation_0-mlogloss:0.61091\n",
      "[229]\tvalidation_0-mlogloss:0.61021\n",
      "[230]\tvalidation_0-mlogloss:0.60942\n",
      "[231]\tvalidation_0-mlogloss:0.60898\n",
      "[232]\tvalidation_0-mlogloss:0.60824\n",
      "[233]\tvalidation_0-mlogloss:0.60775\n",
      "[234]\tvalidation_0-mlogloss:0.60700\n",
      "[235]\tvalidation_0-mlogloss:0.60608\n",
      "[236]\tvalidation_0-mlogloss:0.60475\n",
      "[237]\tvalidation_0-mlogloss:0.60441\n",
      "[238]\tvalidation_0-mlogloss:0.60391\n",
      "[239]\tvalidation_0-mlogloss:0.60312\n",
      "[240]\tvalidation_0-mlogloss:0.60247\n",
      "[241]\tvalidation_0-mlogloss:0.60164\n",
      "[242]\tvalidation_0-mlogloss:0.60127\n",
      "[243]\tvalidation_0-mlogloss:0.60115\n",
      "[244]\tvalidation_0-mlogloss:0.60086\n",
      "[245]\tvalidation_0-mlogloss:0.60046\n",
      "[246]\tvalidation_0-mlogloss:0.59961\n",
      "[247]\tvalidation_0-mlogloss:0.59908\n",
      "[248]\tvalidation_0-mlogloss:0.59827\n",
      "[249]\tvalidation_0-mlogloss:0.59760\n",
      "[250]\tvalidation_0-mlogloss:0.59732\n",
      "[251]\tvalidation_0-mlogloss:0.59633\n",
      "[252]\tvalidation_0-mlogloss:0.59542\n",
      "[253]\tvalidation_0-mlogloss:0.59481\n",
      "[254]\tvalidation_0-mlogloss:0.59433\n",
      "[255]\tvalidation_0-mlogloss:0.59371\n",
      "[256]\tvalidation_0-mlogloss:0.59286\n",
      "[257]\tvalidation_0-mlogloss:0.59232\n",
      "[258]\tvalidation_0-mlogloss:0.59202\n",
      "[259]\tvalidation_0-mlogloss:0.59107\n",
      "[260]\tvalidation_0-mlogloss:0.59041\n",
      "[261]\tvalidation_0-mlogloss:0.58990\n",
      "[262]\tvalidation_0-mlogloss:0.58967\n",
      "[263]\tvalidation_0-mlogloss:0.58906\n",
      "[264]\tvalidation_0-mlogloss:0.58815\n",
      "[265]\tvalidation_0-mlogloss:0.58794\n",
      "[266]\tvalidation_0-mlogloss:0.58748\n",
      "[267]\tvalidation_0-mlogloss:0.58649\n",
      "[268]\tvalidation_0-mlogloss:0.58613\n",
      "[269]\tvalidation_0-mlogloss:0.58526\n",
      "[270]\tvalidation_0-mlogloss:0.58496\n",
      "[271]\tvalidation_0-mlogloss:0.58440\n",
      "[272]\tvalidation_0-mlogloss:0.58410\n",
      "[273]\tvalidation_0-mlogloss:0.58380\n",
      "[274]\tvalidation_0-mlogloss:0.58357\n",
      "[275]\tvalidation_0-mlogloss:0.58326\n",
      "[276]\tvalidation_0-mlogloss:0.58267\n",
      "[277]\tvalidation_0-mlogloss:0.58229\n",
      "[278]\tvalidation_0-mlogloss:0.58207\n",
      "[279]\tvalidation_0-mlogloss:0.58161\n",
      "[280]\tvalidation_0-mlogloss:0.58137\n",
      "[281]\tvalidation_0-mlogloss:0.58110\n",
      "[282]\tvalidation_0-mlogloss:0.58059\n",
      "[283]\tvalidation_0-mlogloss:0.58053\n",
      "[284]\tvalidation_0-mlogloss:0.57974\n",
      "[285]\tvalidation_0-mlogloss:0.57907\n",
      "[286]\tvalidation_0-mlogloss:0.57853\n",
      "[287]\tvalidation_0-mlogloss:0.57815\n",
      "[288]\tvalidation_0-mlogloss:0.57777\n",
      "[289]\tvalidation_0-mlogloss:0.57749\n",
      "[290]\tvalidation_0-mlogloss:0.57735\n",
      "[291]\tvalidation_0-mlogloss:0.57686\n",
      "[292]\tvalidation_0-mlogloss:0.57651\n",
      "[293]\tvalidation_0-mlogloss:0.57606\n",
      "[294]\tvalidation_0-mlogloss:0.57557\n",
      "[295]\tvalidation_0-mlogloss:0.57521\n",
      "[296]\tvalidation_0-mlogloss:0.57484\n",
      "[297]\tvalidation_0-mlogloss:0.57453\n",
      "[298]\tvalidation_0-mlogloss:0.57439\n",
      "[299]\tvalidation_0-mlogloss:0.57434\n",
      "[300]\tvalidation_0-mlogloss:0.57374\n",
      "[301]\tvalidation_0-mlogloss:0.57299\n",
      "[302]\tvalidation_0-mlogloss:0.57259\n",
      "[303]\tvalidation_0-mlogloss:0.57237\n",
      "[304]\tvalidation_0-mlogloss:0.57201\n",
      "[305]\tvalidation_0-mlogloss:0.57174\n",
      "[306]\tvalidation_0-mlogloss:0.57124\n",
      "[307]\tvalidation_0-mlogloss:0.57074\n",
      "[308]\tvalidation_0-mlogloss:0.57014\n",
      "[309]\tvalidation_0-mlogloss:0.56958\n",
      "[310]\tvalidation_0-mlogloss:0.56958\n",
      "[311]\tvalidation_0-mlogloss:0.56937\n",
      "[312]\tvalidation_0-mlogloss:0.56894\n",
      "[313]\tvalidation_0-mlogloss:0.56836\n",
      "[314]\tvalidation_0-mlogloss:0.56779\n",
      "[315]\tvalidation_0-mlogloss:0.56762\n",
      "[316]\tvalidation_0-mlogloss:0.56716\n",
      "[317]\tvalidation_0-mlogloss:0.56666\n",
      "[318]\tvalidation_0-mlogloss:0.56601\n",
      "[319]\tvalidation_0-mlogloss:0.56593\n",
      "[320]\tvalidation_0-mlogloss:0.56540\n",
      "[321]\tvalidation_0-mlogloss:0.56485\n",
      "[322]\tvalidation_0-mlogloss:0.56447\n",
      "[323]\tvalidation_0-mlogloss:0.56412\n",
      "[324]\tvalidation_0-mlogloss:0.56393\n",
      "[325]\tvalidation_0-mlogloss:0.56380\n",
      "[326]\tvalidation_0-mlogloss:0.56351\n",
      "[327]\tvalidation_0-mlogloss:0.56322\n",
      "[328]\tvalidation_0-mlogloss:0.56249\n",
      "[329]\tvalidation_0-mlogloss:0.56228\n",
      "[330]\tvalidation_0-mlogloss:0.56200\n",
      "[331]\tvalidation_0-mlogloss:0.56215\n",
      "[332]\tvalidation_0-mlogloss:0.56178\n",
      "[333]\tvalidation_0-mlogloss:0.56148\n",
      "[334]\tvalidation_0-mlogloss:0.56091\n",
      "[335]\tvalidation_0-mlogloss:0.56087\n",
      "[336]\tvalidation_0-mlogloss:0.56078\n",
      "[337]\tvalidation_0-mlogloss:0.56033\n",
      "[338]\tvalidation_0-mlogloss:0.55979\n",
      "[339]\tvalidation_0-mlogloss:0.55920\n",
      "[340]\tvalidation_0-mlogloss:0.55898\n",
      "[341]\tvalidation_0-mlogloss:0.55848\n",
      "[342]\tvalidation_0-mlogloss:0.55809\n",
      "[343]\tvalidation_0-mlogloss:0.55819\n",
      "[344]\tvalidation_0-mlogloss:0.55769\n",
      "[345]\tvalidation_0-mlogloss:0.55736\n",
      "[346]\tvalidation_0-mlogloss:0.55720\n",
      "[347]\tvalidation_0-mlogloss:0.55686\n",
      "[348]\tvalidation_0-mlogloss:0.55654\n",
      "[349]\tvalidation_0-mlogloss:0.55609\n",
      "[350]\tvalidation_0-mlogloss:0.55577\n",
      "[351]\tvalidation_0-mlogloss:0.55513\n",
      "[352]\tvalidation_0-mlogloss:0.55471\n",
      "[353]\tvalidation_0-mlogloss:0.55446\n",
      "[354]\tvalidation_0-mlogloss:0.55414\n",
      "[355]\tvalidation_0-mlogloss:0.55364\n",
      "[356]\tvalidation_0-mlogloss:0.55326\n",
      "[357]\tvalidation_0-mlogloss:0.55325\n",
      "[358]\tvalidation_0-mlogloss:0.55320\n",
      "[359]\tvalidation_0-mlogloss:0.55295\n",
      "[360]\tvalidation_0-mlogloss:0.55263\n",
      "[361]\tvalidation_0-mlogloss:0.55240\n",
      "[362]\tvalidation_0-mlogloss:0.55223\n",
      "[363]\tvalidation_0-mlogloss:0.55206\n",
      "[364]\tvalidation_0-mlogloss:0.55186\n",
      "[365]\tvalidation_0-mlogloss:0.55164\n",
      "[366]\tvalidation_0-mlogloss:0.55167\n",
      "[367]\tvalidation_0-mlogloss:0.55145\n",
      "[368]\tvalidation_0-mlogloss:0.55147\n",
      "[369]\tvalidation_0-mlogloss:0.55121\n",
      "[370]\tvalidation_0-mlogloss:0.55106\n",
      "[371]\tvalidation_0-mlogloss:0.55091\n",
      "[372]\tvalidation_0-mlogloss:0.55054\n",
      "[373]\tvalidation_0-mlogloss:0.55004\n",
      "[374]\tvalidation_0-mlogloss:0.54979\n",
      "[375]\tvalidation_0-mlogloss:0.54964\n",
      "[376]\tvalidation_0-mlogloss:0.54951\n",
      "[377]\tvalidation_0-mlogloss:0.54924\n",
      "[378]\tvalidation_0-mlogloss:0.54913\n",
      "[379]\tvalidation_0-mlogloss:0.54906\n",
      "[380]\tvalidation_0-mlogloss:0.54875\n",
      "[381]\tvalidation_0-mlogloss:0.54865\n",
      "[382]\tvalidation_0-mlogloss:0.54834\n",
      "[383]\tvalidation_0-mlogloss:0.54799\n",
      "[384]\tvalidation_0-mlogloss:0.54767\n",
      "[385]\tvalidation_0-mlogloss:0.54748\n",
      "[386]\tvalidation_0-mlogloss:0.54724\n",
      "[387]\tvalidation_0-mlogloss:0.54676\n",
      "[388]\tvalidation_0-mlogloss:0.54683\n",
      "[389]\tvalidation_0-mlogloss:0.54683\n",
      "[390]\tvalidation_0-mlogloss:0.54665\n",
      "[391]\tvalidation_0-mlogloss:0.54637\n",
      "[392]\tvalidation_0-mlogloss:0.54617\n",
      "[393]\tvalidation_0-mlogloss:0.54606\n",
      "[394]\tvalidation_0-mlogloss:0.54565\n",
      "[395]\tvalidation_0-mlogloss:0.54560\n",
      "[396]\tvalidation_0-mlogloss:0.54561\n",
      "[397]\tvalidation_0-mlogloss:0.54536\n",
      "[398]\tvalidation_0-mlogloss:0.54524\n",
      "[399]\tvalidation_0-mlogloss:0.54494\n",
      "[400]\tvalidation_0-mlogloss:0.54481\n",
      "[401]\tvalidation_0-mlogloss:0.54451\n",
      "[402]\tvalidation_0-mlogloss:0.54452\n",
      "[403]\tvalidation_0-mlogloss:0.54445\n",
      "[404]\tvalidation_0-mlogloss:0.54420\n",
      "[405]\tvalidation_0-mlogloss:0.54412\n",
      "[406]\tvalidation_0-mlogloss:0.54391\n",
      "[407]\tvalidation_0-mlogloss:0.54376\n",
      "[408]\tvalidation_0-mlogloss:0.54371\n",
      "[409]\tvalidation_0-mlogloss:0.54360\n",
      "[410]\tvalidation_0-mlogloss:0.54339\n",
      "[411]\tvalidation_0-mlogloss:0.54321\n",
      "[412]\tvalidation_0-mlogloss:0.54289\n",
      "[413]\tvalidation_0-mlogloss:0.54284\n",
      "[414]\tvalidation_0-mlogloss:0.54270\n",
      "[415]\tvalidation_0-mlogloss:0.54257\n",
      "[416]\tvalidation_0-mlogloss:0.54254\n",
      "[417]\tvalidation_0-mlogloss:0.54245\n",
      "[418]\tvalidation_0-mlogloss:0.54231\n",
      "[419]\tvalidation_0-mlogloss:0.54211\n",
      "[420]\tvalidation_0-mlogloss:0.54180\n",
      "[421]\tvalidation_0-mlogloss:0.54178\n",
      "[422]\tvalidation_0-mlogloss:0.54170\n",
      "[423]\tvalidation_0-mlogloss:0.54148\n",
      "[424]\tvalidation_0-mlogloss:0.54129\n",
      "[425]\tvalidation_0-mlogloss:0.54107\n",
      "[426]\tvalidation_0-mlogloss:0.54117\n",
      "[427]\tvalidation_0-mlogloss:0.54110\n",
      "[428]\tvalidation_0-mlogloss:0.54087\n",
      "[429]\tvalidation_0-mlogloss:0.54086\n",
      "[430]\tvalidation_0-mlogloss:0.54086\n",
      "[431]\tvalidation_0-mlogloss:0.54046\n",
      "[432]\tvalidation_0-mlogloss:0.54019\n",
      "[433]\tvalidation_0-mlogloss:0.53989\n",
      "[434]\tvalidation_0-mlogloss:0.53978\n",
      "[435]\tvalidation_0-mlogloss:0.53994\n",
      "[436]\tvalidation_0-mlogloss:0.53964\n",
      "[437]\tvalidation_0-mlogloss:0.53940\n",
      "[438]\tvalidation_0-mlogloss:0.53922\n",
      "[439]\tvalidation_0-mlogloss:0.53883\n",
      "[440]\tvalidation_0-mlogloss:0.53868\n",
      "[441]\tvalidation_0-mlogloss:0.53836\n",
      "[442]\tvalidation_0-mlogloss:0.53818\n",
      "[443]\tvalidation_0-mlogloss:0.53795\n",
      "[444]\tvalidation_0-mlogloss:0.53800\n",
      "[445]\tvalidation_0-mlogloss:0.53794\n",
      "[446]\tvalidation_0-mlogloss:0.53793\n",
      "[447]\tvalidation_0-mlogloss:0.53795\n",
      "[448]\tvalidation_0-mlogloss:0.53773\n",
      "[449]\tvalidation_0-mlogloss:0.53754\n",
      "[450]\tvalidation_0-mlogloss:0.53743\n",
      "[451]\tvalidation_0-mlogloss:0.53729\n",
      "[452]\tvalidation_0-mlogloss:0.53728\n",
      "[453]\tvalidation_0-mlogloss:0.53725\n",
      "[454]\tvalidation_0-mlogloss:0.53721\n",
      "[455]\tvalidation_0-mlogloss:0.53705\n",
      "[456]\tvalidation_0-mlogloss:0.53693\n",
      "[457]\tvalidation_0-mlogloss:0.53676\n",
      "[458]\tvalidation_0-mlogloss:0.53675\n",
      "[459]\tvalidation_0-mlogloss:0.53670\n",
      "[460]\tvalidation_0-mlogloss:0.53662\n",
      "[461]\tvalidation_0-mlogloss:0.53638\n",
      "[462]\tvalidation_0-mlogloss:0.53605\n",
      "[463]\tvalidation_0-mlogloss:0.53581\n",
      "[464]\tvalidation_0-mlogloss:0.53545\n",
      "[465]\tvalidation_0-mlogloss:0.53508\n",
      "[466]\tvalidation_0-mlogloss:0.53506\n",
      "[467]\tvalidation_0-mlogloss:0.53482\n",
      "[468]\tvalidation_0-mlogloss:0.53482\n",
      "[469]\tvalidation_0-mlogloss:0.53478\n",
      "[470]\tvalidation_0-mlogloss:0.53488\n",
      "[471]\tvalidation_0-mlogloss:0.53491\n",
      "[472]\tvalidation_0-mlogloss:0.53485\n",
      "[473]\tvalidation_0-mlogloss:0.53478\n",
      "[474]\tvalidation_0-mlogloss:0.53471\n",
      "[475]\tvalidation_0-mlogloss:0.53457\n",
      "[476]\tvalidation_0-mlogloss:0.53439\n",
      "[477]\tvalidation_0-mlogloss:0.53434\n",
      "[478]\tvalidation_0-mlogloss:0.53407\n",
      "[479]\tvalidation_0-mlogloss:0.53400\n",
      "[480]\tvalidation_0-mlogloss:0.53372\n",
      "[481]\tvalidation_0-mlogloss:0.53389\n",
      "[482]\tvalidation_0-mlogloss:0.53348\n",
      "[483]\tvalidation_0-mlogloss:0.53346\n",
      "[484]\tvalidation_0-mlogloss:0.53337\n",
      "[485]\tvalidation_0-mlogloss:0.53330\n",
      "[486]\tvalidation_0-mlogloss:0.53307\n",
      "[487]\tvalidation_0-mlogloss:0.53294\n",
      "[488]\tvalidation_0-mlogloss:0.53275\n",
      "[489]\tvalidation_0-mlogloss:0.53250\n",
      "[490]\tvalidation_0-mlogloss:0.53232\n",
      "[491]\tvalidation_0-mlogloss:0.53229\n",
      "[492]\tvalidation_0-mlogloss:0.53216\n",
      "[493]\tvalidation_0-mlogloss:0.53207\n",
      "[494]\tvalidation_0-mlogloss:0.53189\n",
      "[495]\tvalidation_0-mlogloss:0.53142\n",
      "[496]\tvalidation_0-mlogloss:0.53144\n",
      "[497]\tvalidation_0-mlogloss:0.53150\n",
      "[498]\tvalidation_0-mlogloss:0.53145\n",
      "[499]\tvalidation_0-mlogloss:0.53142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83      1499\n",
      "           1       0.86      0.97      0.91      1499\n",
      "           2       0.71      0.77      0.74      1499\n",
      "           3       0.97      0.83      0.89      1499\n",
      "           4       0.97      0.99      0.98      1499\n",
      "           5       0.99      0.99      0.99      1499\n",
      "           6       0.96      0.98      0.97      1499\n",
      "           7       0.96      0.94      0.95      1499\n",
      "           8       0.53      0.92      0.67      1499\n",
      "           9       0.88      0.83      0.85      1499\n",
      "          10       0.98      0.94      0.96      1499\n",
      "          11       0.98      0.98      0.98      1499\n",
      "          12       0.84      0.94      0.89      1499\n",
      "          13       0.94      0.94      0.94      1499\n",
      "          14       0.75      0.93      0.83      1499\n",
      "          15       0.97      0.71      0.82      1499\n",
      "          16       0.47      0.35      0.40      1499\n",
      "          17       0.86      0.89      0.87      1499\n",
      "          18       0.92      0.88      0.90      1499\n",
      "          19       0.89      0.78      0.83      1499\n",
      "          20       0.98      1.00      0.99      1499\n",
      "          21       0.80      0.94      0.86      1499\n",
      "          22       0.79      0.78      0.78      1499\n",
      "          23       0.98      1.00      0.99      1499\n",
      "          24       0.96      0.97      0.97      1499\n",
      "          25       0.96      0.78      0.86      1499\n",
      "          26       0.60      0.41      0.48      1499\n",
      "          27       0.82      0.83      0.82      1499\n",
      "          28       0.77      0.57      0.65      1499\n",
      "          29       0.80      0.96      0.88      1499\n",
      "          30       0.90      0.95      0.93      1499\n",
      "          31       0.77      0.97      0.86      1499\n",
      "          32       0.94      0.97      0.96      1499\n",
      "          33       0.98      0.52      0.68      1499\n",
      "          34       0.99      1.00      0.99      1499\n",
      "          35       0.85      0.93      0.89      1499\n",
      "          36       0.88      0.91      0.89      1499\n",
      "          37       0.85      0.83      0.84      1499\n",
      "          38       0.63      0.12      0.21      1499\n",
      "          39       0.87      0.88      0.88      1499\n",
      "          40       0.94      0.89      0.91      1499\n",
      "          41       0.87      0.87      0.87      1499\n",
      "          42       0.89      0.87      0.88      1499\n",
      "          43       0.92      0.88      0.90      1499\n",
      "          44       0.92      0.99      0.95      1499\n",
      "          45       0.63      0.80      0.71      1499\n",
      "          46       0.71      0.90      0.79      1499\n",
      "          47       0.78      0.57      0.66      1499\n",
      "          48       0.77      0.95      0.85      1499\n",
      "          49       0.82      0.91      0.86      1499\n",
      "\n",
      "    accuracy                           0.85     74950\n",
      "   macro avg       0.85      0.85      0.84     74950\n",
      "weighted avg       0.85      0.85      0.84     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1dad4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(50, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3361defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 12:51:00.484688: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 153600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18742/18750 [============================>.] - ETA: 0s - loss: 0.7319"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 27s 1ms/step - loss: 0.7317 - val_loss: 1.6029\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 27s 1ms/step - loss: 0.2781 - val_loss: 1.7273\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.2062 - val_loss: 1.6858\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.1697 - val_loss: 1.6180\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.1450 - val_loss: 1.7696\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.1274 - val_loss: 1.7156\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.1157 - val_loss: 1.4283\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.1043 - val_loss: 1.5649\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.0968 - val_loss: 1.6063\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 26s 1ms/step - loss: 0.0890 - val_loss: 1.6680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc199406710>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 2s 843us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.76      1499\n",
      "           1       0.93      0.85      0.89      1499\n",
      "           2       0.69      0.67      0.68      1499\n",
      "           3       0.85      0.95      0.90      1499\n",
      "           4       0.92      0.98      0.95      1499\n",
      "           5       0.92      0.98      0.95      1499\n",
      "           6       0.92      0.97      0.94      1499\n",
      "           7       0.97      0.48      0.65      1499\n",
      "           8       0.45      0.82      0.58      1499\n",
      "           9       0.87      0.69      0.77      1499\n",
      "          10       0.82      0.69      0.75      1499\n",
      "          11       0.92      0.92      0.92      1499\n",
      "          12       0.84      0.74      0.79      1499\n",
      "          13       0.75      0.94      0.83      1499\n",
      "          14       0.59      0.87      0.70      1499\n",
      "          15       0.89      0.51      0.64      1499\n",
      "          16       0.75      0.45      0.56      1499\n",
      "          17       0.87      0.82      0.84      1499\n",
      "          18       0.86      0.83      0.84      1499\n",
      "          19       0.64      0.79      0.70      1499\n",
      "          20       1.00      0.98      0.99      1499\n",
      "          21       0.71      0.87      0.78      1499\n",
      "          22       0.64      0.85      0.73      1499\n",
      "          23       0.97      0.98      0.98      1499\n",
      "          24       0.96      0.95      0.96      1499\n",
      "          25       0.82      0.39      0.53      1499\n",
      "          26       0.59      0.42      0.49      1499\n",
      "          27       0.85      0.73      0.79      1499\n",
      "          28       0.77      0.29      0.42      1499\n",
      "          29       0.60      0.89      0.72      1499\n",
      "          30       0.89      0.90      0.89      1499\n",
      "          31       0.78      0.94      0.85      1499\n",
      "          32       0.81      0.95      0.88      1499\n",
      "          33       0.78      0.50      0.61      1499\n",
      "          34       0.97      1.00      0.99      1499\n",
      "          35       0.87      0.81      0.84      1499\n",
      "          36       0.61      0.87      0.72      1499\n",
      "          37       0.75      0.78      0.77      1499\n",
      "          38       0.67      0.28      0.39      1499\n",
      "          39       0.43      0.81      0.56      1499\n",
      "          40       0.72      0.73      0.72      1499\n",
      "          41       0.77      0.55      0.64      1499\n",
      "          42       0.46      0.74      0.57      1499\n",
      "          43       0.80      0.83      0.81      1499\n",
      "          44       0.84      0.94      0.89      1499\n",
      "          45       0.83      0.48      0.61      1499\n",
      "          46       0.60      0.57      0.59      1499\n",
      "          47       0.79      0.36      0.49      1499\n",
      "          48       0.62      0.90      0.73      1499\n",
      "          49       0.70      0.60      0.65      1499\n",
      "\n",
      "    accuracy                           0.75     74950\n",
      "   macro avg       0.78      0.75      0.74     74950\n",
      "weighted avg       0.78      0.75      0.74     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c862b2bf",
   "metadata": {},
   "source": [
    "# 25 persons\n",
    "## electrodes : 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ce0f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 05:19:48.171147: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 05:19:50.615587: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 05:19:50.615690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 05:19:50.988227: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 05:19:51.740005: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 05:19:51.740999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 05:19:54.862511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081a3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.6.3 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.7.0)\n",
      "Requirement already satisfied: tqdm in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (4.64.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (1.7.0)\n",
      "Requirement already satisfied: decorator in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: packaging in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (22.0)\n",
      "Requirement already satisfied: jinja2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (3.8.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from pooch>=1.5->mne) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from jinja2->mne) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saikeerthana/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d4276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9085fdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files3/S001R05.edf',\n",
       " 'files3/S002R05.edf',\n",
       " 'files3/S003R05.edf',\n",
       " 'files3/S004R05.edf',\n",
       " 'files3/S005R05.edf',\n",
       " 'files3/S006R05.edf',\n",
       " 'files3/S007R05.edf',\n",
       " 'files3/S008R05.edf',\n",
       " 'files3/S009R05.edf',\n",
       " 'files3/S010R05.edf',\n",
       " 'files3/S011R05.edf',\n",
       " 'files3/S012R05.edf',\n",
       " 'files3/S013R05.edf',\n",
       " 'files3/S014R05.edf',\n",
       " 'files3/S015R05.edf',\n",
       " 'files3/S016R05.edf',\n",
       " 'files3/S017R05.edf',\n",
       " 'files3/S018R05.edf',\n",
       " 'files3/S019R05.edf',\n",
       " 'files3/S020R05.edf',\n",
       " 'files3/S021R05.edf',\n",
       " 'files3/S022R05.edf',\n",
       " 'files3/S023R05.edf',\n",
       " 'files3/S024R05.edf',\n",
       " 'files3/S025R05.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=sorted(glob('files3/*.edf'))\n",
    "train=train[:no_of_patients]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb63fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "    \n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    eeg_df.sample(frac=1)\n",
    "    \n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65857e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aeb31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47802ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705fd1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 37475, 300000, 37475)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain),len(xtest),len(ytrain),len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6d73ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.01e-04  1.30e-05 -1.50e-05 ...  6.00e-06  3.00e-06 -7.00e-06]\n"
     ]
    }
   ],
   "source": [
    "print(xtest.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8738bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000010   0.000020   0.000007  -0.000018   0.000000   0.000004   \n",
       "1        0.000010   0.000050   0.000049   0.000022   0.000038   0.000031   \n",
       "2        0.000017   0.000055   0.000059   0.000030   0.000037   0.000025   \n",
       "3        0.000021   0.000063   0.000066   0.000035   0.000040   0.000029   \n",
       "4        0.000027   0.000072   0.000079   0.000048   0.000052   0.000042   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "299995  -0.000021  -0.000036  -0.000044  -0.000012  -0.000038  -0.000038   \n",
       "299996  -0.000033  -0.000040  -0.000046  -0.000014  -0.000040  -0.000039   \n",
       "299997  -0.000023  -0.000035  -0.000041  -0.000005  -0.000035  -0.000034   \n",
       "299998  -0.000020  -0.000024  -0.000032   0.000004  -0.000025  -0.000026   \n",
       "299999  -0.000025  -0.000030  -0.000036   0.000003  -0.000022  -0.000021   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000060   0.000004   0.000012   0.000006  ...   -0.000006   \n",
       "1        0.000084   0.000025   0.000043   0.000049  ...    0.000001   \n",
       "2        0.000079   0.000030   0.000048   0.000053  ...    0.000006   \n",
       "3        0.000076   0.000035   0.000055   0.000065  ...    0.000029   \n",
       "4        0.000091   0.000043   0.000060   0.000076  ...    0.000035   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "299995  -0.000030  -0.000011  -0.000027  -0.000023  ...   -0.000021   \n",
       "299996  -0.000030  -0.000012  -0.000029  -0.000027  ...   -0.000017   \n",
       "299997  -0.000028  -0.000012  -0.000029  -0.000023  ...   -0.000015   \n",
       "299998  -0.000019  -0.000007  -0.000023  -0.000019  ...   -0.000011   \n",
       "299999  -0.000015  -0.000008  -0.000028  -0.000025  ...   -0.000012   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000030    0.000025    0.000022    0.000013   -0.000009   \n",
       "1         0.000044    0.000031    0.000027    0.000025    0.000012   \n",
       "2         0.000048    0.000036    0.000033    0.000034    0.000034   \n",
       "3         0.000048    0.000041    0.000050    0.000059    0.000063   \n",
       "4         0.000029    0.000022    0.000042    0.000062    0.000071   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "299995   -0.000011   -0.000026   -0.000012   -0.000030   -0.000024   \n",
       "299996   -0.000011   -0.000025   -0.000010   -0.000024   -0.000009   \n",
       "299997   -0.000010   -0.000024   -0.000007   -0.000024   -0.000010   \n",
       "299998   -0.000003   -0.000019   -0.000004   -0.000018    0.000000   \n",
       "299999   -0.000006   -0.000025   -0.000015   -0.000031   -0.000021   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000020    0.000024    0.000089    0.000068  \n",
       "1         0.000030    0.000025    0.000083    0.000065  \n",
       "2         0.000041    0.000034    0.000092    0.000078  \n",
       "3         0.000045    0.000047    0.000121    0.000086  \n",
       "4         0.000037    0.000051    0.000143    0.000090  \n",
       "...            ...         ...         ...         ...  \n",
       "299995   -0.000012   -0.000024   -0.000022   -0.000012  \n",
       "299996   -0.000011   -0.000018   -0.000017   -0.000009  \n",
       "299997   -0.000007   -0.000016   -0.000015   -0.000008  \n",
       "299998    0.000001   -0.000007    0.000002    0.000000  \n",
       "299999   -0.000007   -0.000015   -0.000019   -0.000003  \n",
       "\n",
       "[300000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459d7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe.iloc[:,:-1].values\n",
    "    y=dataframe.iloc[:,-1].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabeaa2",
   "metadata": {},
   "source": [
    "## 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3172e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain8=xtrain.iloc[:,:8]\n",
    "xvalid8=xvalid.iloc[:,:8]\n",
    "xtest8=xtest.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904c30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3400/394288547.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain8['id']=ytrain\n",
      "/tmp/ipykernel_3400/394288547.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest8['id']=ytest\n",
      "/tmp/ipykernel_3400/394288547.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid8['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000010   0.000020   0.000007  -0.000018   0.000000   0.000004   \n",
       "1        0.000010   0.000050   0.000049   0.000022   0.000038   0.000031   \n",
       "2        0.000017   0.000055   0.000059   0.000030   0.000037   0.000025   \n",
       "3        0.000021   0.000063   0.000066   0.000035   0.000040   0.000029   \n",
       "4        0.000027   0.000072   0.000079   0.000048   0.000052   0.000042   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "299995  -0.000021  -0.000036  -0.000044  -0.000012  -0.000038  -0.000038   \n",
       "299996  -0.000033  -0.000040  -0.000046  -0.000014  -0.000040  -0.000039   \n",
       "299997  -0.000023  -0.000035  -0.000041  -0.000005  -0.000035  -0.000034   \n",
       "299998  -0.000020  -0.000024  -0.000032   0.000004  -0.000025  -0.000026   \n",
       "299999  -0.000025  -0.000030  -0.000036   0.000003  -0.000022  -0.000021   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000060   0.000004   0.000012   0.000006  ...   -0.000006   \n",
       "1        0.000084   0.000025   0.000043   0.000049  ...    0.000001   \n",
       "2        0.000079   0.000030   0.000048   0.000053  ...    0.000006   \n",
       "3        0.000076   0.000035   0.000055   0.000065  ...    0.000029   \n",
       "4        0.000091   0.000043   0.000060   0.000076  ...    0.000035   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "299995  -0.000030  -0.000011  -0.000027  -0.000023  ...   -0.000021   \n",
       "299996  -0.000030  -0.000012  -0.000029  -0.000027  ...   -0.000017   \n",
       "299997  -0.000028  -0.000012  -0.000029  -0.000023  ...   -0.000015   \n",
       "299998  -0.000019  -0.000007  -0.000023  -0.000019  ...   -0.000011   \n",
       "299999  -0.000015  -0.000008  -0.000028  -0.000025  ...   -0.000012   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000030    0.000025    0.000022    0.000013   -0.000009   \n",
       "1         0.000044    0.000031    0.000027    0.000025    0.000012   \n",
       "2         0.000048    0.000036    0.000033    0.000034    0.000034   \n",
       "3         0.000048    0.000041    0.000050    0.000059    0.000063   \n",
       "4         0.000029    0.000022    0.000042    0.000062    0.000071   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "299995   -0.000011   -0.000026   -0.000012   -0.000030   -0.000024   \n",
       "299996   -0.000011   -0.000025   -0.000010   -0.000024   -0.000009   \n",
       "299997   -0.000010   -0.000024   -0.000007   -0.000024   -0.000010   \n",
       "299998   -0.000003   -0.000019   -0.000004   -0.000018    0.000000   \n",
       "299999   -0.000006   -0.000025   -0.000015   -0.000031   -0.000021   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000020    0.000024    0.000089    0.000068  \n",
       "1         0.000030    0.000025    0.000083    0.000065  \n",
       "2         0.000041    0.000034    0.000092    0.000078  \n",
       "3         0.000045    0.000047    0.000121    0.000086  \n",
       "4         0.000037    0.000051    0.000143    0.000090  \n",
       "...            ...         ...         ...         ...  \n",
       "299995   -0.000012   -0.000024   -0.000022   -0.000012  \n",
       "299996   -0.000011   -0.000018   -0.000017   -0.000009  \n",
       "299997   -0.000007   -0.000016   -0.000015   -0.000008  \n",
       "299998    0.000001   -0.000007    0.000002    0.000000  \n",
       "299999   -0.000007   -0.000015   -0.000019   -0.000003  \n",
       "\n",
       "[300000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain8['id']=ytrain\n",
    "xtest8['id']=ytest\n",
    "xvalid8['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b732f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x8,y8=scale_dataset(xtrain8)\n",
    "xt8,yt8=scale_dataset(xtest8)\n",
    "xv8,yv8=scale_dataset(xvalid8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.91089\n",
      "[1]\tvalidation_0-mlogloss:2.78557\n",
      "[2]\tvalidation_0-mlogloss:2.70545\n",
      "[3]\tvalidation_0-mlogloss:2.64570\n",
      "[4]\tvalidation_0-mlogloss:2.59139\n",
      "[5]\tvalidation_0-mlogloss:2.55158\n",
      "[6]\tvalidation_0-mlogloss:2.51291\n",
      "[7]\tvalidation_0-mlogloss:2.48394\n",
      "[8]\tvalidation_0-mlogloss:2.45068\n",
      "[9]\tvalidation_0-mlogloss:2.42893\n",
      "[10]\tvalidation_0-mlogloss:2.40568\n",
      "[11]\tvalidation_0-mlogloss:2.38613\n",
      "[12]\tvalidation_0-mlogloss:2.36814\n",
      "[13]\tvalidation_0-mlogloss:2.35298\n",
      "[14]\tvalidation_0-mlogloss:2.33874\n",
      "[15]\tvalidation_0-mlogloss:2.32130\n",
      "[16]\tvalidation_0-mlogloss:2.30208\n",
      "[17]\tvalidation_0-mlogloss:2.28768\n",
      "[18]\tvalidation_0-mlogloss:2.27454\n",
      "[19]\tvalidation_0-mlogloss:2.26282\n",
      "[20]\tvalidation_0-mlogloss:2.25096\n",
      "[21]\tvalidation_0-mlogloss:2.23579\n",
      "[22]\tvalidation_0-mlogloss:2.22535\n",
      "[23]\tvalidation_0-mlogloss:2.21720\n",
      "[24]\tvalidation_0-mlogloss:2.20857\n",
      "[25]\tvalidation_0-mlogloss:2.19888\n",
      "[26]\tvalidation_0-mlogloss:2.19166\n",
      "[27]\tvalidation_0-mlogloss:2.18132\n",
      "[28]\tvalidation_0-mlogloss:2.17500\n",
      "[29]\tvalidation_0-mlogloss:2.16719\n",
      "[30]\tvalidation_0-mlogloss:2.15670\n",
      "[31]\tvalidation_0-mlogloss:2.14864\n",
      "[32]\tvalidation_0-mlogloss:2.14036\n",
      "[33]\tvalidation_0-mlogloss:2.13275\n",
      "[34]\tvalidation_0-mlogloss:2.12515\n",
      "[35]\tvalidation_0-mlogloss:2.11971\n",
      "[36]\tvalidation_0-mlogloss:2.11065\n",
      "[37]\tvalidation_0-mlogloss:2.10563\n",
      "[38]\tvalidation_0-mlogloss:2.09983\n",
      "[39]\tvalidation_0-mlogloss:2.09449\n",
      "[40]\tvalidation_0-mlogloss:2.08814\n",
      "[41]\tvalidation_0-mlogloss:2.08386\n",
      "[42]\tvalidation_0-mlogloss:2.07961\n",
      "[43]\tvalidation_0-mlogloss:2.07568\n",
      "[44]\tvalidation_0-mlogloss:2.06806\n",
      "[45]\tvalidation_0-mlogloss:2.06222\n",
      "[46]\tvalidation_0-mlogloss:2.05826\n",
      "[47]\tvalidation_0-mlogloss:2.05383\n",
      "[48]\tvalidation_0-mlogloss:2.05083\n",
      "[49]\tvalidation_0-mlogloss:2.04709\n",
      "[50]\tvalidation_0-mlogloss:2.04466\n",
      "[51]\tvalidation_0-mlogloss:2.03988\n",
      "[52]\tvalidation_0-mlogloss:2.03651\n",
      "[53]\tvalidation_0-mlogloss:2.03155\n",
      "[54]\tvalidation_0-mlogloss:2.02622\n",
      "[55]\tvalidation_0-mlogloss:2.02417\n",
      "[56]\tvalidation_0-mlogloss:2.02270\n",
      "[57]\tvalidation_0-mlogloss:2.01905\n",
      "[58]\tvalidation_0-mlogloss:2.01535\n",
      "[59]\tvalidation_0-mlogloss:2.01333\n",
      "[60]\tvalidation_0-mlogloss:2.01041\n",
      "[61]\tvalidation_0-mlogloss:2.00841\n",
      "[62]\tvalidation_0-mlogloss:2.00632\n",
      "[63]\tvalidation_0-mlogloss:2.00348\n",
      "[64]\tvalidation_0-mlogloss:2.00149\n",
      "[65]\tvalidation_0-mlogloss:1.99814\n",
      "[66]\tvalidation_0-mlogloss:1.99299\n",
      "[67]\tvalidation_0-mlogloss:1.98932\n",
      "[68]\tvalidation_0-mlogloss:1.98734\n",
      "[69]\tvalidation_0-mlogloss:1.98582\n",
      "[70]\tvalidation_0-mlogloss:1.98227\n",
      "[71]\tvalidation_0-mlogloss:1.97984\n",
      "[72]\tvalidation_0-mlogloss:1.97715\n",
      "[73]\tvalidation_0-mlogloss:1.97445\n",
      "[74]\tvalidation_0-mlogloss:1.97084\n",
      "[75]\tvalidation_0-mlogloss:1.97049\n",
      "[76]\tvalidation_0-mlogloss:1.96853\n",
      "[77]\tvalidation_0-mlogloss:1.96845\n",
      "[78]\tvalidation_0-mlogloss:1.96726\n",
      "[79]\tvalidation_0-mlogloss:1.96629\n",
      "[80]\tvalidation_0-mlogloss:1.96351\n",
      "[81]\tvalidation_0-mlogloss:1.96292\n",
      "[82]\tvalidation_0-mlogloss:1.96048\n",
      "[83]\tvalidation_0-mlogloss:1.95817\n",
      "[84]\tvalidation_0-mlogloss:1.95583\n",
      "[85]\tvalidation_0-mlogloss:1.95202\n",
      "[86]\tvalidation_0-mlogloss:1.94899\n",
      "[87]\tvalidation_0-mlogloss:1.94512\n",
      "[88]\tvalidation_0-mlogloss:1.94306\n",
      "[89]\tvalidation_0-mlogloss:1.94142\n",
      "[90]\tvalidation_0-mlogloss:1.94024\n",
      "[91]\tvalidation_0-mlogloss:1.93716\n",
      "[92]\tvalidation_0-mlogloss:1.93552\n",
      "[93]\tvalidation_0-mlogloss:1.93287\n",
      "[94]\tvalidation_0-mlogloss:1.93060\n",
      "[95]\tvalidation_0-mlogloss:1.92874\n",
      "[96]\tvalidation_0-mlogloss:1.92727\n",
      "[97]\tvalidation_0-mlogloss:1.92592\n",
      "[98]\tvalidation_0-mlogloss:1.92457\n",
      "[99]\tvalidation_0-mlogloss:1.92297\n",
      "[100]\tvalidation_0-mlogloss:1.92175\n",
      "[101]\tvalidation_0-mlogloss:1.92086\n",
      "[102]\tvalidation_0-mlogloss:1.91991\n",
      "[103]\tvalidation_0-mlogloss:1.91847\n",
      "[104]\tvalidation_0-mlogloss:1.91675\n",
      "[105]\tvalidation_0-mlogloss:1.91569\n",
      "[106]\tvalidation_0-mlogloss:1.91361\n",
      "[107]\tvalidation_0-mlogloss:1.91171\n",
      "[108]\tvalidation_0-mlogloss:1.91099\n",
      "[109]\tvalidation_0-mlogloss:1.90971\n",
      "[110]\tvalidation_0-mlogloss:1.90847\n",
      "[111]\tvalidation_0-mlogloss:1.90894\n",
      "[112]\tvalidation_0-mlogloss:1.90907\n",
      "[113]\tvalidation_0-mlogloss:1.90711\n",
      "[114]\tvalidation_0-mlogloss:1.90655\n",
      "[115]\tvalidation_0-mlogloss:1.90473\n",
      "[116]\tvalidation_0-mlogloss:1.90184\n",
      "[117]\tvalidation_0-mlogloss:1.90147\n",
      "[118]\tvalidation_0-mlogloss:1.89805\n",
      "[119]\tvalidation_0-mlogloss:1.89561\n",
      "[120]\tvalidation_0-mlogloss:1.89499\n",
      "[121]\tvalidation_0-mlogloss:1.89320\n",
      "[122]\tvalidation_0-mlogloss:1.89332\n",
      "[123]\tvalidation_0-mlogloss:1.89113\n",
      "[124]\tvalidation_0-mlogloss:1.88933\n",
      "[125]\tvalidation_0-mlogloss:1.88943\n",
      "[126]\tvalidation_0-mlogloss:1.88781\n",
      "[127]\tvalidation_0-mlogloss:1.88532\n",
      "[128]\tvalidation_0-mlogloss:1.88413\n",
      "[129]\tvalidation_0-mlogloss:1.88313\n",
      "[130]\tvalidation_0-mlogloss:1.88153\n",
      "[131]\tvalidation_0-mlogloss:1.87952\n",
      "[132]\tvalidation_0-mlogloss:1.87918\n",
      "[133]\tvalidation_0-mlogloss:1.87933\n",
      "[134]\tvalidation_0-mlogloss:1.87839\n",
      "[135]\tvalidation_0-mlogloss:1.87697\n",
      "[136]\tvalidation_0-mlogloss:1.87607\n",
      "[137]\tvalidation_0-mlogloss:1.87452\n",
      "[138]\tvalidation_0-mlogloss:1.87218\n",
      "[139]\tvalidation_0-mlogloss:1.87126\n",
      "[140]\tvalidation_0-mlogloss:1.87003\n",
      "[141]\tvalidation_0-mlogloss:1.86759\n",
      "[142]\tvalidation_0-mlogloss:1.86608\n",
      "[143]\tvalidation_0-mlogloss:1.86365\n",
      "[144]\tvalidation_0-mlogloss:1.86264\n",
      "[145]\tvalidation_0-mlogloss:1.86147\n",
      "[146]\tvalidation_0-mlogloss:1.85984\n",
      "[147]\tvalidation_0-mlogloss:1.85924\n",
      "[148]\tvalidation_0-mlogloss:1.85852\n",
      "[149]\tvalidation_0-mlogloss:1.85846\n",
      "[150]\tvalidation_0-mlogloss:1.85710\n",
      "[151]\tvalidation_0-mlogloss:1.85575\n",
      "[152]\tvalidation_0-mlogloss:1.85509\n",
      "[153]\tvalidation_0-mlogloss:1.85406\n",
      "[154]\tvalidation_0-mlogloss:1.85321\n",
      "[155]\tvalidation_0-mlogloss:1.85234\n",
      "[156]\tvalidation_0-mlogloss:1.85119\n",
      "[157]\tvalidation_0-mlogloss:1.85100\n",
      "[158]\tvalidation_0-mlogloss:1.85045\n",
      "[159]\tvalidation_0-mlogloss:1.84873\n",
      "[160]\tvalidation_0-mlogloss:1.84833\n",
      "[161]\tvalidation_0-mlogloss:1.84804\n",
      "[162]\tvalidation_0-mlogloss:1.84788\n",
      "[163]\tvalidation_0-mlogloss:1.84728\n",
      "[164]\tvalidation_0-mlogloss:1.84601\n",
      "[165]\tvalidation_0-mlogloss:1.84627\n",
      "[166]\tvalidation_0-mlogloss:1.84533\n",
      "[167]\tvalidation_0-mlogloss:1.84493\n",
      "[168]\tvalidation_0-mlogloss:1.84486\n",
      "[169]\tvalidation_0-mlogloss:1.84524\n",
      "[170]\tvalidation_0-mlogloss:1.84428\n",
      "[171]\tvalidation_0-mlogloss:1.84398\n",
      "[172]\tvalidation_0-mlogloss:1.84399\n",
      "[173]\tvalidation_0-mlogloss:1.84315\n",
      "[174]\tvalidation_0-mlogloss:1.84318\n",
      "[175]\tvalidation_0-mlogloss:1.84329\n",
      "[176]\tvalidation_0-mlogloss:1.84354\n",
      "[177]\tvalidation_0-mlogloss:1.84257\n",
      "[178]\tvalidation_0-mlogloss:1.84176\n",
      "[179]\tvalidation_0-mlogloss:1.84215\n",
      "[180]\tvalidation_0-mlogloss:1.84258\n",
      "[181]\tvalidation_0-mlogloss:1.84217\n",
      "[182]\tvalidation_0-mlogloss:1.84146\n",
      "[183]\tvalidation_0-mlogloss:1.83997\n",
      "[184]\tvalidation_0-mlogloss:1.83985\n",
      "[185]\tvalidation_0-mlogloss:1.83869\n",
      "[186]\tvalidation_0-mlogloss:1.83952\n",
      "[187]\tvalidation_0-mlogloss:1.84049\n",
      "[188]\tvalidation_0-mlogloss:1.84056\n",
      "[189]\tvalidation_0-mlogloss:1.84065\n",
      "[190]\tvalidation_0-mlogloss:1.84103\n",
      "[191]\tvalidation_0-mlogloss:1.84012\n",
      "[192]\tvalidation_0-mlogloss:1.84001\n",
      "[193]\tvalidation_0-mlogloss:1.83924\n",
      "[194]\tvalidation_0-mlogloss:1.83859\n",
      "[195]\tvalidation_0-mlogloss:1.83897\n",
      "[196]\tvalidation_0-mlogloss:1.83911\n",
      "[197]\tvalidation_0-mlogloss:1.83924\n",
      "[198]\tvalidation_0-mlogloss:1.83877\n",
      "[199]\tvalidation_0-mlogloss:1.83809\n",
      "[200]\tvalidation_0-mlogloss:1.83743\n",
      "[201]\tvalidation_0-mlogloss:1.83686\n",
      "[202]\tvalidation_0-mlogloss:1.83695\n",
      "[203]\tvalidation_0-mlogloss:1.83745\n",
      "[204]\tvalidation_0-mlogloss:1.83744\n",
      "[205]\tvalidation_0-mlogloss:1.83800\n",
      "[206]\tvalidation_0-mlogloss:1.83809\n",
      "[207]\tvalidation_0-mlogloss:1.83895\n",
      "[208]\tvalidation_0-mlogloss:1.83999\n",
      "[209]\tvalidation_0-mlogloss:1.84044\n",
      "[210]\tvalidation_0-mlogloss:1.84148\n",
      "[211]\tvalidation_0-mlogloss:1.84120\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.15      0.17      1499\n",
      "           1       0.20      0.28      0.23      1499\n",
      "           2       0.44      0.42      0.43      1499\n",
      "           3       0.37      0.28      0.32      1499\n",
      "           4       0.78      0.74      0.76      1499\n",
      "           5       0.39      0.64      0.48      1499\n",
      "           6       0.32      0.53      0.40      1499\n",
      "           7       0.40      0.29      0.33      1499\n",
      "           8       0.64      0.71      0.67      1499\n",
      "           9       0.60      0.60      0.60      1499\n",
      "          10       0.40      0.34      0.37      1499\n",
      "          11       0.40      0.34      0.37      1499\n",
      "          12       0.51      0.50      0.50      1499\n",
      "          13       0.31      0.37      0.34      1499\n",
      "          14       0.24      0.20      0.22      1499\n",
      "          15       0.67      0.70      0.68      1499\n",
      "          16       0.51      0.37      0.43      1499\n",
      "          17       0.52      0.47      0.49      1499\n",
      "          18       0.25      0.20      0.22      1499\n",
      "          19       0.30      0.37      0.33      1499\n",
      "          20       0.61      0.68      0.65      1499\n",
      "          21       0.73      0.65      0.68      1499\n",
      "          22       0.40      0.25      0.30      1499\n",
      "          23       0.75      0.73      0.74      1499\n",
      "          24       0.63      0.64      0.64      1499\n",
      "\n",
      "    accuracy                           0.46     37475\n",
      "   macro avg       0.46      0.46      0.45     37475\n",
      "weighted avg       0.46      0.46      0.45     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=XGBClassifier(n_estimators=500)\n",
    "model.fit(x8,y8,early_stopping_rounds=10, eval_set=[(xv8, yv8)])\n",
    "y_pred=model.predict(xt8)\n",
    "print(classification_report(yt8,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b271d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(8, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(25, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aee87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9375/9375 [==============================] - 9s 950us/step - loss: 1.8388 - val_loss: 2.0449\n",
      "Epoch 2/10\n",
      "9375/9375 [==============================] - 9s 938us/step - loss: 1.2869 - val_loss: 2.0205\n",
      "Epoch 3/10\n",
      "9375/9375 [==============================] - 9s 996us/step - loss: 1.1744 - val_loss: 2.0770\n",
      "Epoch 4/10\n",
      "9375/9375 [==============================] - 6s 648us/step - loss: 1.0994 - val_loss: 2.1018\n",
      "Epoch 5/10\n",
      "9375/9375 [==============================] - 6s 668us/step - loss: 1.0556 - val_loss: 2.3199\n",
      "Epoch 6/10\n",
      "9375/9375 [==============================] - 6s 669us/step - loss: 1.0297 - val_loss: 2.2398\n",
      "Epoch 7/10\n",
      "9375/9375 [==============================] - 6s 675us/step - loss: 1.0090 - val_loss: 2.3246\n",
      "Epoch 8/10\n",
      "9375/9375 [==============================] - 6s 665us/step - loss: 0.9954 - val_loss: 2.2565\n",
      "Epoch 9/10\n",
      "9375/9375 [==============================] - 6s 655us/step - loss: 0.9814 - val_loss: 2.4668\n",
      "Epoch 10/10\n",
      "9375/9375 [==============================] - 6s 666us/step - loss: 0.9703 - val_loss: 2.2286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d5ae2560>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x8,y8,epochs=10,validation_data=(xv8,yv8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "badca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172/1172 [==============================] - 0s 287us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.19      0.20      1499\n",
      "           1       0.20      0.40      0.26      1499\n",
      "           2       0.36      0.55      0.44      1499\n",
      "           3       0.44      0.28      0.34      1499\n",
      "           4       0.79      0.69      0.74      1499\n",
      "           5       0.56      0.60      0.58      1499\n",
      "           6       0.28      0.52      0.37      1499\n",
      "           7       0.63      0.34      0.44      1499\n",
      "           8       0.58      0.71      0.64      1499\n",
      "           9       0.52      0.65      0.58      1499\n",
      "          10       0.54      0.29      0.38      1499\n",
      "          11       0.41      0.44      0.42      1499\n",
      "          12       0.50      0.43      0.46      1499\n",
      "          13       0.29      0.40      0.34      1499\n",
      "          14       0.21      0.20      0.21      1499\n",
      "          15       0.68      0.74      0.71      1499\n",
      "          16       0.44      0.20      0.27      1499\n",
      "          17       0.39      0.06      0.11      1499\n",
      "          18       0.26      0.23      0.25      1499\n",
      "          19       0.30      0.40      0.34      1499\n",
      "          20       0.64      0.67      0.66      1499\n",
      "          21       0.63      0.70      0.66      1499\n",
      "          22       0.37      0.35      0.36      1499\n",
      "          23       0.82      0.73      0.78      1499\n",
      "          24       0.68      0.34      0.46      1499\n",
      "\n",
      "    accuracy                           0.44     37475\n",
      "   macro avg       0.47      0.44      0.44     37475\n",
      "weighted avg       0.47      0.44      0.44     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model.predict(xt8)).numpy(),axis=1)\n",
    "print(classification_report(yt8,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d010",
   "metadata": {},
   "source": [
    "## 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "268bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain16=xtrain.iloc[:,:16]\n",
    "xtest16=xtest.iloc[:,:16]\n",
    "xvalid16=xvalid.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "000e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3400/4039718790.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain16['id']=ytrain\n",
      "/tmp/ipykernel_3400/4039718790.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest16['id']=ytest\n",
      "/tmp/ipykernel_3400/4039718790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid16['id']=yvalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_0</th>\n",
       "      <th>Channel_1</th>\n",
       "      <th>Channel_2</th>\n",
       "      <th>Channel_3</th>\n",
       "      <th>Channel_4</th>\n",
       "      <th>Channel_5</th>\n",
       "      <th>Channel_6</th>\n",
       "      <th>Channel_7</th>\n",
       "      <th>Channel_8</th>\n",
       "      <th>Channel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel_54</th>\n",
       "      <th>Channel_55</th>\n",
       "      <th>Channel_56</th>\n",
       "      <th>Channel_57</th>\n",
       "      <th>Channel_58</th>\n",
       "      <th>Channel_59</th>\n",
       "      <th>Channel_60</th>\n",
       "      <th>Channel_61</th>\n",
       "      <th>Channel_62</th>\n",
       "      <th>Channel_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Channel_0  Channel_1  Channel_2  Channel_3  Channel_4  Channel_5  \\\n",
       "0       -0.000010   0.000020   0.000007  -0.000018   0.000000   0.000004   \n",
       "1        0.000010   0.000050   0.000049   0.000022   0.000038   0.000031   \n",
       "2        0.000017   0.000055   0.000059   0.000030   0.000037   0.000025   \n",
       "3        0.000021   0.000063   0.000066   0.000035   0.000040   0.000029   \n",
       "4        0.000027   0.000072   0.000079   0.000048   0.000052   0.000042   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "299995  -0.000021  -0.000036  -0.000044  -0.000012  -0.000038  -0.000038   \n",
       "299996  -0.000033  -0.000040  -0.000046  -0.000014  -0.000040  -0.000039   \n",
       "299997  -0.000023  -0.000035  -0.000041  -0.000005  -0.000035  -0.000034   \n",
       "299998  -0.000020  -0.000024  -0.000032   0.000004  -0.000025  -0.000026   \n",
       "299999  -0.000025  -0.000030  -0.000036   0.000003  -0.000022  -0.000021   \n",
       "\n",
       "        Channel_6  Channel_7  Channel_8  Channel_9  ...  Channel_54  \\\n",
       "0        0.000060   0.000004   0.000012   0.000006  ...   -0.000006   \n",
       "1        0.000084   0.000025   0.000043   0.000049  ...    0.000001   \n",
       "2        0.000079   0.000030   0.000048   0.000053  ...    0.000006   \n",
       "3        0.000076   0.000035   0.000055   0.000065  ...    0.000029   \n",
       "4        0.000091   0.000043   0.000060   0.000076  ...    0.000035   \n",
       "...           ...        ...        ...        ...  ...         ...   \n",
       "299995  -0.000030  -0.000011  -0.000027  -0.000023  ...   -0.000021   \n",
       "299996  -0.000030  -0.000012  -0.000029  -0.000027  ...   -0.000017   \n",
       "299997  -0.000028  -0.000012  -0.000029  -0.000023  ...   -0.000015   \n",
       "299998  -0.000019  -0.000007  -0.000023  -0.000019  ...   -0.000011   \n",
       "299999  -0.000015  -0.000008  -0.000028  -0.000025  ...   -0.000012   \n",
       "\n",
       "        Channel_55  Channel_56  Channel_57  Channel_58  Channel_59  \\\n",
       "0         0.000030    0.000025    0.000022    0.000013   -0.000009   \n",
       "1         0.000044    0.000031    0.000027    0.000025    0.000012   \n",
       "2         0.000048    0.000036    0.000033    0.000034    0.000034   \n",
       "3         0.000048    0.000041    0.000050    0.000059    0.000063   \n",
       "4         0.000029    0.000022    0.000042    0.000062    0.000071   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "299995   -0.000011   -0.000026   -0.000012   -0.000030   -0.000024   \n",
       "299996   -0.000011   -0.000025   -0.000010   -0.000024   -0.000009   \n",
       "299997   -0.000010   -0.000024   -0.000007   -0.000024   -0.000010   \n",
       "299998   -0.000003   -0.000019   -0.000004   -0.000018    0.000000   \n",
       "299999   -0.000006   -0.000025   -0.000015   -0.000031   -0.000021   \n",
       "\n",
       "        Channel_60  Channel_61  Channel_62  Channel_63  \n",
       "0         0.000020    0.000024    0.000089    0.000068  \n",
       "1         0.000030    0.000025    0.000083    0.000065  \n",
       "2         0.000041    0.000034    0.000092    0.000078  \n",
       "3         0.000045    0.000047    0.000121    0.000086  \n",
       "4         0.000037    0.000051    0.000143    0.000090  \n",
       "...            ...         ...         ...         ...  \n",
       "299995   -0.000012   -0.000024   -0.000022   -0.000012  \n",
       "299996   -0.000011   -0.000018   -0.000017   -0.000009  \n",
       "299997   -0.000007   -0.000016   -0.000015   -0.000008  \n",
       "299998    0.000001   -0.000007    0.000002    0.000000  \n",
       "299999   -0.000007   -0.000015   -0.000019   -0.000003  \n",
       "\n",
       "[300000 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain16['id']=ytrain\n",
    "xtest16['id']=ytest\n",
    "xvalid16['id']=yvalid\n",
    "display(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x16,y16=scale_dataset(xtrain16)\n",
    "xt16,yt16=scale_dataset(xtest16)\n",
    "xv16,yv16=scale_dataset(xvalid16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.85250\n",
      "[1]\tvalidation_0-mlogloss:2.71715\n",
      "[2]\tvalidation_0-mlogloss:2.61865\n",
      "[3]\tvalidation_0-mlogloss:2.53303\n",
      "[4]\tvalidation_0-mlogloss:2.46961\n",
      "[5]\tvalidation_0-mlogloss:2.41320\n",
      "[6]\tvalidation_0-mlogloss:2.36357\n",
      "[7]\tvalidation_0-mlogloss:2.32255\n",
      "[8]\tvalidation_0-mlogloss:2.28311\n",
      "[9]\tvalidation_0-mlogloss:2.24802\n",
      "[10]\tvalidation_0-mlogloss:2.21245\n",
      "[11]\tvalidation_0-mlogloss:2.18665\n",
      "[12]\tvalidation_0-mlogloss:2.16105\n",
      "[13]\tvalidation_0-mlogloss:2.13499\n",
      "[14]\tvalidation_0-mlogloss:2.11104\n",
      "[15]\tvalidation_0-mlogloss:2.08618\n",
      "[16]\tvalidation_0-mlogloss:2.06465\n",
      "[17]\tvalidation_0-mlogloss:2.04719\n",
      "[18]\tvalidation_0-mlogloss:2.02764\n",
      "[19]\tvalidation_0-mlogloss:2.01236\n",
      "[20]\tvalidation_0-mlogloss:1.99196\n",
      "[21]\tvalidation_0-mlogloss:1.97649\n",
      "[22]\tvalidation_0-mlogloss:1.95769\n",
      "[23]\tvalidation_0-mlogloss:1.94225\n",
      "[24]\tvalidation_0-mlogloss:1.92823\n",
      "[25]\tvalidation_0-mlogloss:1.91229\n",
      "[26]\tvalidation_0-mlogloss:1.89965\n",
      "[27]\tvalidation_0-mlogloss:1.88623\n",
      "[28]\tvalidation_0-mlogloss:1.87472\n",
      "[29]\tvalidation_0-mlogloss:1.86493\n",
      "[30]\tvalidation_0-mlogloss:1.85438\n",
      "[31]\tvalidation_0-mlogloss:1.83965\n",
      "[32]\tvalidation_0-mlogloss:1.82782\n",
      "[33]\tvalidation_0-mlogloss:1.81313\n",
      "[34]\tvalidation_0-mlogloss:1.80024\n",
      "[35]\tvalidation_0-mlogloss:1.79316\n",
      "[36]\tvalidation_0-mlogloss:1.78398\n",
      "[37]\tvalidation_0-mlogloss:1.77543\n",
      "[38]\tvalidation_0-mlogloss:1.76665\n",
      "[39]\tvalidation_0-mlogloss:1.75616\n",
      "[40]\tvalidation_0-mlogloss:1.74807\n",
      "[41]\tvalidation_0-mlogloss:1.74077\n",
      "[42]\tvalidation_0-mlogloss:1.73292\n",
      "[43]\tvalidation_0-mlogloss:1.72394\n",
      "[44]\tvalidation_0-mlogloss:1.71796\n",
      "[45]\tvalidation_0-mlogloss:1.70803\n",
      "[46]\tvalidation_0-mlogloss:1.70024\n",
      "[47]\tvalidation_0-mlogloss:1.69412\n",
      "[48]\tvalidation_0-mlogloss:1.68561\n",
      "[49]\tvalidation_0-mlogloss:1.67760\n",
      "[50]\tvalidation_0-mlogloss:1.67167\n",
      "[51]\tvalidation_0-mlogloss:1.66714\n",
      "[52]\tvalidation_0-mlogloss:1.66195\n",
      "[53]\tvalidation_0-mlogloss:1.65500\n",
      "[54]\tvalidation_0-mlogloss:1.64798\n",
      "[55]\tvalidation_0-mlogloss:1.64228\n",
      "[56]\tvalidation_0-mlogloss:1.63811\n",
      "[57]\tvalidation_0-mlogloss:1.63030\n",
      "[58]\tvalidation_0-mlogloss:1.62473\n",
      "[59]\tvalidation_0-mlogloss:1.61887\n",
      "[60]\tvalidation_0-mlogloss:1.61229\n",
      "[61]\tvalidation_0-mlogloss:1.60695\n",
      "[62]\tvalidation_0-mlogloss:1.60177\n",
      "[63]\tvalidation_0-mlogloss:1.59929\n",
      "[64]\tvalidation_0-mlogloss:1.59667\n",
      "[65]\tvalidation_0-mlogloss:1.59193\n",
      "[66]\tvalidation_0-mlogloss:1.58941\n",
      "[67]\tvalidation_0-mlogloss:1.58532\n",
      "[68]\tvalidation_0-mlogloss:1.58118\n",
      "[69]\tvalidation_0-mlogloss:1.57650\n",
      "[70]\tvalidation_0-mlogloss:1.57222\n",
      "[71]\tvalidation_0-mlogloss:1.56584\n",
      "[72]\tvalidation_0-mlogloss:1.56189\n",
      "[73]\tvalidation_0-mlogloss:1.55795\n",
      "[74]\tvalidation_0-mlogloss:1.55237\n",
      "[75]\tvalidation_0-mlogloss:1.54666\n",
      "[76]\tvalidation_0-mlogloss:1.54175\n",
      "[77]\tvalidation_0-mlogloss:1.53905\n",
      "[78]\tvalidation_0-mlogloss:1.53555\n",
      "[79]\tvalidation_0-mlogloss:1.53123\n",
      "[80]\tvalidation_0-mlogloss:1.52643\n",
      "[81]\tvalidation_0-mlogloss:1.52407\n",
      "[82]\tvalidation_0-mlogloss:1.52069\n",
      "[83]\tvalidation_0-mlogloss:1.51732\n",
      "[84]\tvalidation_0-mlogloss:1.51634\n",
      "[85]\tvalidation_0-mlogloss:1.51435\n",
      "[86]\tvalidation_0-mlogloss:1.51016\n",
      "[87]\tvalidation_0-mlogloss:1.50509\n",
      "[88]\tvalidation_0-mlogloss:1.50139\n",
      "[89]\tvalidation_0-mlogloss:1.49703\n",
      "[90]\tvalidation_0-mlogloss:1.49457\n",
      "[91]\tvalidation_0-mlogloss:1.49227\n",
      "[92]\tvalidation_0-mlogloss:1.49071\n",
      "[93]\tvalidation_0-mlogloss:1.48838\n",
      "[94]\tvalidation_0-mlogloss:1.48335\n",
      "[95]\tvalidation_0-mlogloss:1.47994\n",
      "[96]\tvalidation_0-mlogloss:1.47709\n",
      "[97]\tvalidation_0-mlogloss:1.47233\n",
      "[98]\tvalidation_0-mlogloss:1.46958\n",
      "[99]\tvalidation_0-mlogloss:1.46776\n",
      "[100]\tvalidation_0-mlogloss:1.46620\n",
      "[101]\tvalidation_0-mlogloss:1.46268\n",
      "[102]\tvalidation_0-mlogloss:1.46147\n",
      "[103]\tvalidation_0-mlogloss:1.45872\n",
      "[104]\tvalidation_0-mlogloss:1.45718\n",
      "[105]\tvalidation_0-mlogloss:1.45509\n",
      "[106]\tvalidation_0-mlogloss:1.45208\n",
      "[107]\tvalidation_0-mlogloss:1.44998\n",
      "[108]\tvalidation_0-mlogloss:1.44777\n",
      "[109]\tvalidation_0-mlogloss:1.44541\n",
      "[110]\tvalidation_0-mlogloss:1.44411\n",
      "[111]\tvalidation_0-mlogloss:1.44179\n",
      "[112]\tvalidation_0-mlogloss:1.43993\n",
      "[113]\tvalidation_0-mlogloss:1.43821\n",
      "[114]\tvalidation_0-mlogloss:1.43676\n",
      "[115]\tvalidation_0-mlogloss:1.43482\n",
      "[116]\tvalidation_0-mlogloss:1.43279\n",
      "[117]\tvalidation_0-mlogloss:1.43110\n",
      "[118]\tvalidation_0-mlogloss:1.43039\n",
      "[119]\tvalidation_0-mlogloss:1.42783\n",
      "[120]\tvalidation_0-mlogloss:1.42539\n",
      "[121]\tvalidation_0-mlogloss:1.42302\n",
      "[122]\tvalidation_0-mlogloss:1.42118\n",
      "[123]\tvalidation_0-mlogloss:1.41836\n",
      "[124]\tvalidation_0-mlogloss:1.41660\n",
      "[125]\tvalidation_0-mlogloss:1.41511\n",
      "[126]\tvalidation_0-mlogloss:1.41331\n",
      "[127]\tvalidation_0-mlogloss:1.41120\n",
      "[128]\tvalidation_0-mlogloss:1.41043\n",
      "[129]\tvalidation_0-mlogloss:1.40924\n",
      "[130]\tvalidation_0-mlogloss:1.40720\n",
      "[131]\tvalidation_0-mlogloss:1.40497\n",
      "[132]\tvalidation_0-mlogloss:1.40383\n",
      "[133]\tvalidation_0-mlogloss:1.40148\n",
      "[134]\tvalidation_0-mlogloss:1.40036\n",
      "[135]\tvalidation_0-mlogloss:1.39782\n",
      "[136]\tvalidation_0-mlogloss:1.39523\n",
      "[137]\tvalidation_0-mlogloss:1.39355\n",
      "[138]\tvalidation_0-mlogloss:1.39238\n",
      "[139]\tvalidation_0-mlogloss:1.39050\n",
      "[140]\tvalidation_0-mlogloss:1.38959\n",
      "[141]\tvalidation_0-mlogloss:1.38720\n",
      "[142]\tvalidation_0-mlogloss:1.38546\n",
      "[143]\tvalidation_0-mlogloss:1.38451\n",
      "[144]\tvalidation_0-mlogloss:1.38268\n",
      "[145]\tvalidation_0-mlogloss:1.38198\n",
      "[146]\tvalidation_0-mlogloss:1.38116\n",
      "[147]\tvalidation_0-mlogloss:1.38001\n",
      "[148]\tvalidation_0-mlogloss:1.37707\n",
      "[149]\tvalidation_0-mlogloss:1.37439\n",
      "[150]\tvalidation_0-mlogloss:1.37387\n",
      "[151]\tvalidation_0-mlogloss:1.37187\n",
      "[152]\tvalidation_0-mlogloss:1.37057\n",
      "[153]\tvalidation_0-mlogloss:1.37026\n",
      "[154]\tvalidation_0-mlogloss:1.36943\n",
      "[155]\tvalidation_0-mlogloss:1.36841\n",
      "[156]\tvalidation_0-mlogloss:1.36813\n",
      "[157]\tvalidation_0-mlogloss:1.36714\n",
      "[158]\tvalidation_0-mlogloss:1.36582\n",
      "[159]\tvalidation_0-mlogloss:1.36444\n",
      "[160]\tvalidation_0-mlogloss:1.36255\n",
      "[161]\tvalidation_0-mlogloss:1.36167\n",
      "[162]\tvalidation_0-mlogloss:1.36052\n",
      "[163]\tvalidation_0-mlogloss:1.35904\n",
      "[164]\tvalidation_0-mlogloss:1.35846\n",
      "[165]\tvalidation_0-mlogloss:1.35641\n",
      "[166]\tvalidation_0-mlogloss:1.35495\n",
      "[167]\tvalidation_0-mlogloss:1.35409\n",
      "[168]\tvalidation_0-mlogloss:1.35378\n",
      "[169]\tvalidation_0-mlogloss:1.35305\n",
      "[170]\tvalidation_0-mlogloss:1.35216\n",
      "[171]\tvalidation_0-mlogloss:1.35077\n",
      "[172]\tvalidation_0-mlogloss:1.34985\n",
      "[173]\tvalidation_0-mlogloss:1.34935\n",
      "[174]\tvalidation_0-mlogloss:1.34814\n",
      "[175]\tvalidation_0-mlogloss:1.34808\n",
      "[176]\tvalidation_0-mlogloss:1.34681\n",
      "[177]\tvalidation_0-mlogloss:1.34581\n",
      "[178]\tvalidation_0-mlogloss:1.34513\n",
      "[179]\tvalidation_0-mlogloss:1.34487\n",
      "[180]\tvalidation_0-mlogloss:1.34431\n",
      "[181]\tvalidation_0-mlogloss:1.34419\n",
      "[182]\tvalidation_0-mlogloss:1.34392\n",
      "[183]\tvalidation_0-mlogloss:1.34406\n",
      "[184]\tvalidation_0-mlogloss:1.34386\n",
      "[185]\tvalidation_0-mlogloss:1.34352\n",
      "[186]\tvalidation_0-mlogloss:1.34104\n",
      "[187]\tvalidation_0-mlogloss:1.34106\n",
      "[188]\tvalidation_0-mlogloss:1.34072\n",
      "[189]\tvalidation_0-mlogloss:1.33984\n",
      "[190]\tvalidation_0-mlogloss:1.34019\n",
      "[191]\tvalidation_0-mlogloss:1.34013\n",
      "[192]\tvalidation_0-mlogloss:1.33993\n",
      "[193]\tvalidation_0-mlogloss:1.33939\n",
      "[194]\tvalidation_0-mlogloss:1.33890\n",
      "[195]\tvalidation_0-mlogloss:1.33865\n",
      "[196]\tvalidation_0-mlogloss:1.33738\n",
      "[197]\tvalidation_0-mlogloss:1.33664\n",
      "[198]\tvalidation_0-mlogloss:1.33625\n",
      "[199]\tvalidation_0-mlogloss:1.33552\n",
      "[200]\tvalidation_0-mlogloss:1.33465\n",
      "[201]\tvalidation_0-mlogloss:1.33344\n",
      "[202]\tvalidation_0-mlogloss:1.33156\n",
      "[203]\tvalidation_0-mlogloss:1.33214\n",
      "[204]\tvalidation_0-mlogloss:1.33183\n",
      "[205]\tvalidation_0-mlogloss:1.33137\n",
      "[206]\tvalidation_0-mlogloss:1.33064\n",
      "[207]\tvalidation_0-mlogloss:1.33029\n",
      "[208]\tvalidation_0-mlogloss:1.32970\n",
      "[209]\tvalidation_0-mlogloss:1.32957\n",
      "[210]\tvalidation_0-mlogloss:1.32989\n",
      "[211]\tvalidation_0-mlogloss:1.32910\n",
      "[212]\tvalidation_0-mlogloss:1.32870\n",
      "[213]\tvalidation_0-mlogloss:1.32781\n",
      "[214]\tvalidation_0-mlogloss:1.32841\n",
      "[215]\tvalidation_0-mlogloss:1.32825\n",
      "[216]\tvalidation_0-mlogloss:1.32718\n",
      "[217]\tvalidation_0-mlogloss:1.32648\n",
      "[218]\tvalidation_0-mlogloss:1.32539\n",
      "[219]\tvalidation_0-mlogloss:1.32477\n",
      "[220]\tvalidation_0-mlogloss:1.32387\n",
      "[221]\tvalidation_0-mlogloss:1.32348\n",
      "[222]\tvalidation_0-mlogloss:1.32277\n",
      "[223]\tvalidation_0-mlogloss:1.32257\n",
      "[224]\tvalidation_0-mlogloss:1.32207\n",
      "[225]\tvalidation_0-mlogloss:1.32182\n",
      "[226]\tvalidation_0-mlogloss:1.32101\n",
      "[227]\tvalidation_0-mlogloss:1.32094\n",
      "[228]\tvalidation_0-mlogloss:1.32108\n",
      "[229]\tvalidation_0-mlogloss:1.32015\n",
      "[230]\tvalidation_0-mlogloss:1.31992\n",
      "[231]\tvalidation_0-mlogloss:1.31904\n",
      "[232]\tvalidation_0-mlogloss:1.31962\n",
      "[233]\tvalidation_0-mlogloss:1.32016\n",
      "[234]\tvalidation_0-mlogloss:1.31982\n",
      "[235]\tvalidation_0-mlogloss:1.31936\n",
      "[236]\tvalidation_0-mlogloss:1.31913\n",
      "[237]\tvalidation_0-mlogloss:1.31835\n",
      "[238]\tvalidation_0-mlogloss:1.31831\n",
      "[239]\tvalidation_0-mlogloss:1.31829\n",
      "[240]\tvalidation_0-mlogloss:1.31712\n",
      "[241]\tvalidation_0-mlogloss:1.31734\n",
      "[242]\tvalidation_0-mlogloss:1.31701\n",
      "[243]\tvalidation_0-mlogloss:1.31651\n",
      "[244]\tvalidation_0-mlogloss:1.31575\n",
      "[245]\tvalidation_0-mlogloss:1.31523\n",
      "[246]\tvalidation_0-mlogloss:1.31618\n",
      "[247]\tvalidation_0-mlogloss:1.31607\n",
      "[248]\tvalidation_0-mlogloss:1.31591\n",
      "[249]\tvalidation_0-mlogloss:1.31573\n",
      "[250]\tvalidation_0-mlogloss:1.31565\n",
      "[251]\tvalidation_0-mlogloss:1.31494\n",
      "[252]\tvalidation_0-mlogloss:1.31591\n",
      "[253]\tvalidation_0-mlogloss:1.31564\n",
      "[254]\tvalidation_0-mlogloss:1.31566\n",
      "[255]\tvalidation_0-mlogloss:1.31551\n",
      "[256]\tvalidation_0-mlogloss:1.31547\n",
      "[257]\tvalidation_0-mlogloss:1.31642\n",
      "[258]\tvalidation_0-mlogloss:1.31554\n",
      "[259]\tvalidation_0-mlogloss:1.31635\n",
      "[260]\tvalidation_0-mlogloss:1.31599\n",
      "[261]\tvalidation_0-mlogloss:1.31564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.36      0.38      1499\n",
      "           1       0.32      0.41      0.36      1499\n",
      "           2       0.54      0.52      0.53      1499\n",
      "           3       0.54      0.27      0.36      1499\n",
      "           4       0.88      0.88      0.88      1499\n",
      "           5       0.57      0.75      0.65      1499\n",
      "           6       0.62      0.77      0.69      1499\n",
      "           7       0.90      0.65      0.75      1499\n",
      "           8       0.76      0.85      0.80      1499\n",
      "           9       0.86      0.70      0.77      1499\n",
      "          10       0.66      0.58      0.62      1499\n",
      "          11       0.62      0.70      0.66      1499\n",
      "          12       0.67      0.80      0.73      1499\n",
      "          13       0.36      0.56      0.44      1499\n",
      "          14       0.36      0.42      0.39      1499\n",
      "          15       0.83      0.79      0.81      1499\n",
      "          16       0.75      0.64      0.69      1499\n",
      "          17       0.91      0.62      0.74      1499\n",
      "          18       0.46      0.61      0.52      1499\n",
      "          19       0.54      0.41      0.47      1499\n",
      "          20       0.79      0.84      0.81      1499\n",
      "          21       0.73      0.73      0.73      1499\n",
      "          22       0.49      0.35      0.41      1499\n",
      "          23       0.84      0.85      0.84      1499\n",
      "          24       0.90      0.78      0.83      1499\n",
      "\n",
      "    accuracy                           0.63     37475\n",
      "   macro avg       0.65      0.63      0.63     37475\n",
      "weighted avg       0.65      0.63      0.63     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=XGBClassifier(n_estimators=500)\n",
    "model2.fit(x16,y16,early_stopping_rounds=10, eval_set=[(xv16, yv16)])\n",
    "y_pred=model2.predict(xt16)\n",
    "print(classification_report(yt16,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7844baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(25, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7980b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9375/9375 [==============================] - 6s 674us/step - loss: 1.2939 - val_loss: 1.4125\n",
      "Epoch 2/10\n",
      "9375/9375 [==============================] - 6s 644us/step - loss: 0.6974 - val_loss: 1.5488\n",
      "Epoch 3/10\n",
      "9375/9375 [==============================] - 6s 659us/step - loss: 0.5811 - val_loss: 1.5663\n",
      "Epoch 4/10\n",
      "9375/9375 [==============================] - 6s 659us/step - loss: 0.5209 - val_loss: 1.6327\n",
      "Epoch 5/10\n",
      "9375/9375 [==============================] - 6s 664us/step - loss: 0.4784 - val_loss: 1.8563\n",
      "Epoch 6/10\n",
      "9375/9375 [==============================] - 6s 665us/step - loss: 0.4405 - val_loss: 1.7205\n",
      "Epoch 7/10\n",
      "9375/9375 [==============================] - 6s 660us/step - loss: 0.4116 - val_loss: 1.9137\n",
      "Epoch 8/10\n",
      "9375/9375 [==============================] - 6s 669us/step - loss: 0.3898 - val_loss: 1.7058\n",
      "Epoch 9/10\n",
      "9375/9375 [==============================] - 6s 659us/step - loss: 0.3745 - val_loss: 1.9133\n",
      "Epoch 10/10\n",
      "9375/9375 [==============================] - 6s 668us/step - loss: 0.3623 - val_loss: 1.9206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d63a0eb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model1.fit(\n",
    "    x16,y16,epochs=10,validation_data=(xv16,yv16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07cf8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172/1172 [==============================] - 0s 290us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.31      0.38      1499\n",
      "           1       0.36      0.55      0.44      1499\n",
      "           2       0.46      0.54      0.50      1499\n",
      "           3       0.65      0.38      0.48      1499\n",
      "           4       0.88      0.91      0.90      1499\n",
      "           5       0.72      0.87      0.79      1499\n",
      "           6       0.78      0.67      0.72      1499\n",
      "           7       0.93      0.68      0.79      1499\n",
      "           8       0.64      0.97      0.77      1499\n",
      "           9       0.82      0.78      0.80      1499\n",
      "          10       0.90      0.43      0.58      1499\n",
      "          11       0.83      0.78      0.81      1499\n",
      "          12       0.76      0.75      0.75      1499\n",
      "          13       0.30      0.69      0.42      1499\n",
      "          14       0.35      0.49      0.41      1499\n",
      "          15       0.94      0.76      0.84      1499\n",
      "          16       0.78      0.47      0.59      1499\n",
      "          17       0.98      0.58      0.73      1499\n",
      "          18       0.60      0.59      0.60      1499\n",
      "          19       0.67      0.51      0.58      1499\n",
      "          20       0.82      0.89      0.85      1499\n",
      "          21       0.77      0.80      0.79      1499\n",
      "          22       0.47      0.55      0.50      1499\n",
      "          23       0.89      0.86      0.87      1499\n",
      "          24       0.92      0.58      0.71      1499\n",
      "\n",
      "    accuracy                           0.66     37475\n",
      "   macro avg       0.71      0.66      0.66     37475\n",
      "weighted avg       0.71      0.66      0.66     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model1.predict(xt16)).numpy(),axis=1)\n",
    "print(classification_report(yt16,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e57e7",
   "metadata": {},
   "source": [
    "## 0-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a408e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain32=xtrain.iloc[:,:32]\n",
    "xtest32=xtest.iloc[:,:32]\n",
    "xvalid32=xvalid.iloc[:,:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3400/1675347936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain32['id']=ytrain\n",
      "/tmp/ipykernel_3400/1675347936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest32['id']=ytest\n",
      "/tmp/ipykernel_3400/1675347936.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xvalid32['id']=yvalid\n"
     ]
    }
   ],
   "source": [
    "xtrain32['id']=ytrain\n",
    "xtest32['id']=ytest\n",
    "xvalid32['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ddddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32,y32=scale_dataset(xtrain32)\n",
    "xt32,yt32=scale_dataset(xtest32)\n",
    "xv32,yv32=scale_dataset(xvalid32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.67182\n",
      "[1]\tvalidation_0-mlogloss:2.46507\n",
      "[2]\tvalidation_0-mlogloss:2.32055\n",
      "[3]\tvalidation_0-mlogloss:2.20414\n",
      "[4]\tvalidation_0-mlogloss:2.11896\n",
      "[5]\tvalidation_0-mlogloss:2.03793\n",
      "[6]\tvalidation_0-mlogloss:1.97623\n",
      "[7]\tvalidation_0-mlogloss:1.91616\n",
      "[8]\tvalidation_0-mlogloss:1.86422\n",
      "[9]\tvalidation_0-mlogloss:1.81560\n",
      "[10]\tvalidation_0-mlogloss:1.76531\n",
      "[11]\tvalidation_0-mlogloss:1.73049\n",
      "[12]\tvalidation_0-mlogloss:1.69274\n",
      "[13]\tvalidation_0-mlogloss:1.65740\n",
      "[14]\tvalidation_0-mlogloss:1.62544\n",
      "[15]\tvalidation_0-mlogloss:1.59777\n",
      "[16]\tvalidation_0-mlogloss:1.57087\n",
      "[17]\tvalidation_0-mlogloss:1.54381\n",
      "[18]\tvalidation_0-mlogloss:1.52039\n",
      "[19]\tvalidation_0-mlogloss:1.49558\n",
      "[20]\tvalidation_0-mlogloss:1.47630\n",
      "[21]\tvalidation_0-mlogloss:1.45636\n",
      "[22]\tvalidation_0-mlogloss:1.44021\n",
      "[23]\tvalidation_0-mlogloss:1.41743\n",
      "[24]\tvalidation_0-mlogloss:1.39870\n",
      "[25]\tvalidation_0-mlogloss:1.38071\n",
      "[26]\tvalidation_0-mlogloss:1.36630\n",
      "[27]\tvalidation_0-mlogloss:1.35541\n",
      "[28]\tvalidation_0-mlogloss:1.33867\n",
      "[29]\tvalidation_0-mlogloss:1.32527\n",
      "[30]\tvalidation_0-mlogloss:1.31179\n",
      "[31]\tvalidation_0-mlogloss:1.29852\n",
      "[32]\tvalidation_0-mlogloss:1.28677\n",
      "[33]\tvalidation_0-mlogloss:1.27511\n",
      "[34]\tvalidation_0-mlogloss:1.26291\n",
      "[35]\tvalidation_0-mlogloss:1.25483\n",
      "[36]\tvalidation_0-mlogloss:1.24087\n",
      "[37]\tvalidation_0-mlogloss:1.22700\n",
      "[38]\tvalidation_0-mlogloss:1.21420\n",
      "[39]\tvalidation_0-mlogloss:1.20561\n",
      "[40]\tvalidation_0-mlogloss:1.19317\n",
      "[41]\tvalidation_0-mlogloss:1.18492\n",
      "[42]\tvalidation_0-mlogloss:1.17165\n",
      "[43]\tvalidation_0-mlogloss:1.16141\n",
      "[44]\tvalidation_0-mlogloss:1.15428\n",
      "[45]\tvalidation_0-mlogloss:1.14544\n",
      "[46]\tvalidation_0-mlogloss:1.13702\n",
      "[47]\tvalidation_0-mlogloss:1.12815\n",
      "[48]\tvalidation_0-mlogloss:1.12030\n",
      "[49]\tvalidation_0-mlogloss:1.11142\n",
      "[50]\tvalidation_0-mlogloss:1.10259\n",
      "[51]\tvalidation_0-mlogloss:1.09443\n",
      "[52]\tvalidation_0-mlogloss:1.08881\n",
      "[53]\tvalidation_0-mlogloss:1.08164\n",
      "[54]\tvalidation_0-mlogloss:1.07412\n",
      "[55]\tvalidation_0-mlogloss:1.06743\n",
      "[56]\tvalidation_0-mlogloss:1.06084\n",
      "[57]\tvalidation_0-mlogloss:1.05316\n",
      "[58]\tvalidation_0-mlogloss:1.04591\n",
      "[59]\tvalidation_0-mlogloss:1.03814\n",
      "[60]\tvalidation_0-mlogloss:1.03127\n",
      "[61]\tvalidation_0-mlogloss:1.02560\n",
      "[62]\tvalidation_0-mlogloss:1.01760\n",
      "[63]\tvalidation_0-mlogloss:1.01058\n",
      "[64]\tvalidation_0-mlogloss:1.00351\n",
      "[65]\tvalidation_0-mlogloss:0.99821\n",
      "[66]\tvalidation_0-mlogloss:0.99210\n",
      "[67]\tvalidation_0-mlogloss:0.98597\n",
      "[68]\tvalidation_0-mlogloss:0.98028\n",
      "[69]\tvalidation_0-mlogloss:0.97490\n",
      "[70]\tvalidation_0-mlogloss:0.96937\n",
      "[71]\tvalidation_0-mlogloss:0.96500\n",
      "[72]\tvalidation_0-mlogloss:0.95961\n",
      "[73]\tvalidation_0-mlogloss:0.95485\n",
      "[74]\tvalidation_0-mlogloss:0.94844\n",
      "[75]\tvalidation_0-mlogloss:0.94418\n",
      "[76]\tvalidation_0-mlogloss:0.93970\n",
      "[77]\tvalidation_0-mlogloss:0.93386\n",
      "[78]\tvalidation_0-mlogloss:0.92861\n",
      "[79]\tvalidation_0-mlogloss:0.92469\n",
      "[80]\tvalidation_0-mlogloss:0.91983\n",
      "[81]\tvalidation_0-mlogloss:0.91339\n",
      "[82]\tvalidation_0-mlogloss:0.91032\n",
      "[83]\tvalidation_0-mlogloss:0.90637\n",
      "[84]\tvalidation_0-mlogloss:0.90378\n",
      "[85]\tvalidation_0-mlogloss:0.89839\n",
      "[86]\tvalidation_0-mlogloss:0.89356\n",
      "[87]\tvalidation_0-mlogloss:0.89085\n",
      "[88]\tvalidation_0-mlogloss:0.88669\n",
      "[89]\tvalidation_0-mlogloss:0.88268\n",
      "[90]\tvalidation_0-mlogloss:0.87896\n",
      "[91]\tvalidation_0-mlogloss:0.87624\n",
      "[92]\tvalidation_0-mlogloss:0.87267\n",
      "[93]\tvalidation_0-mlogloss:0.86881\n",
      "[94]\tvalidation_0-mlogloss:0.86322\n",
      "[95]\tvalidation_0-mlogloss:0.85952\n",
      "[96]\tvalidation_0-mlogloss:0.85454\n",
      "[97]\tvalidation_0-mlogloss:0.85174\n",
      "[98]\tvalidation_0-mlogloss:0.84835\n",
      "[99]\tvalidation_0-mlogloss:0.84582\n",
      "[100]\tvalidation_0-mlogloss:0.84295\n",
      "[101]\tvalidation_0-mlogloss:0.84081\n",
      "[102]\tvalidation_0-mlogloss:0.83758\n",
      "[103]\tvalidation_0-mlogloss:0.83426\n",
      "[104]\tvalidation_0-mlogloss:0.83233\n",
      "[105]\tvalidation_0-mlogloss:0.83049\n",
      "[106]\tvalidation_0-mlogloss:0.82691\n",
      "[107]\tvalidation_0-mlogloss:0.82491\n",
      "[108]\tvalidation_0-mlogloss:0.82239\n",
      "[109]\tvalidation_0-mlogloss:0.82024\n",
      "[110]\tvalidation_0-mlogloss:0.81871\n",
      "[111]\tvalidation_0-mlogloss:0.81506\n",
      "[112]\tvalidation_0-mlogloss:0.81135\n",
      "[113]\tvalidation_0-mlogloss:0.80892\n",
      "[114]\tvalidation_0-mlogloss:0.80735\n",
      "[115]\tvalidation_0-mlogloss:0.80420\n",
      "[116]\tvalidation_0-mlogloss:0.80138\n",
      "[117]\tvalidation_0-mlogloss:0.79871\n",
      "[118]\tvalidation_0-mlogloss:0.79615\n",
      "[119]\tvalidation_0-mlogloss:0.79365\n",
      "[120]\tvalidation_0-mlogloss:0.79230\n",
      "[121]\tvalidation_0-mlogloss:0.78956\n",
      "[122]\tvalidation_0-mlogloss:0.78676\n",
      "[123]\tvalidation_0-mlogloss:0.78491\n",
      "[124]\tvalidation_0-mlogloss:0.78267\n",
      "[125]\tvalidation_0-mlogloss:0.78022\n",
      "[126]\tvalidation_0-mlogloss:0.77817\n",
      "[127]\tvalidation_0-mlogloss:0.77595\n",
      "[128]\tvalidation_0-mlogloss:0.77385\n",
      "[129]\tvalidation_0-mlogloss:0.77372\n",
      "[130]\tvalidation_0-mlogloss:0.77223\n",
      "[131]\tvalidation_0-mlogloss:0.77207\n",
      "[132]\tvalidation_0-mlogloss:0.77090\n",
      "[133]\tvalidation_0-mlogloss:0.76939\n",
      "[134]\tvalidation_0-mlogloss:0.76698\n",
      "[135]\tvalidation_0-mlogloss:0.76458\n",
      "[136]\tvalidation_0-mlogloss:0.76436\n",
      "[137]\tvalidation_0-mlogloss:0.76272\n",
      "[138]\tvalidation_0-mlogloss:0.76152\n",
      "[139]\tvalidation_0-mlogloss:0.75988\n",
      "[140]\tvalidation_0-mlogloss:0.75810\n",
      "[141]\tvalidation_0-mlogloss:0.75758\n",
      "[142]\tvalidation_0-mlogloss:0.75576\n",
      "[143]\tvalidation_0-mlogloss:0.75405\n",
      "[144]\tvalidation_0-mlogloss:0.75242\n",
      "[145]\tvalidation_0-mlogloss:0.74994\n",
      "[146]\tvalidation_0-mlogloss:0.74807\n",
      "[147]\tvalidation_0-mlogloss:0.74619\n",
      "[148]\tvalidation_0-mlogloss:0.74433\n",
      "[149]\tvalidation_0-mlogloss:0.74215\n",
      "[150]\tvalidation_0-mlogloss:0.74128\n",
      "[151]\tvalidation_0-mlogloss:0.73873\n",
      "[152]\tvalidation_0-mlogloss:0.73820\n",
      "[153]\tvalidation_0-mlogloss:0.73750\n",
      "[154]\tvalidation_0-mlogloss:0.73642\n",
      "[155]\tvalidation_0-mlogloss:0.73643\n",
      "[156]\tvalidation_0-mlogloss:0.73448\n",
      "[157]\tvalidation_0-mlogloss:0.73300\n",
      "[158]\tvalidation_0-mlogloss:0.73084\n",
      "[159]\tvalidation_0-mlogloss:0.72968\n",
      "[160]\tvalidation_0-mlogloss:0.72792\n",
      "[161]\tvalidation_0-mlogloss:0.72718\n",
      "[162]\tvalidation_0-mlogloss:0.72514\n",
      "[163]\tvalidation_0-mlogloss:0.72405\n",
      "[164]\tvalidation_0-mlogloss:0.72299\n",
      "[165]\tvalidation_0-mlogloss:0.72164\n",
      "[166]\tvalidation_0-mlogloss:0.72084\n",
      "[167]\tvalidation_0-mlogloss:0.71992\n",
      "[168]\tvalidation_0-mlogloss:0.71825\n",
      "[169]\tvalidation_0-mlogloss:0.71812\n",
      "[170]\tvalidation_0-mlogloss:0.71678\n",
      "[171]\tvalidation_0-mlogloss:0.71590\n",
      "[172]\tvalidation_0-mlogloss:0.71472\n",
      "[173]\tvalidation_0-mlogloss:0.71283\n",
      "[174]\tvalidation_0-mlogloss:0.71189\n",
      "[175]\tvalidation_0-mlogloss:0.71042\n",
      "[176]\tvalidation_0-mlogloss:0.70934\n",
      "[177]\tvalidation_0-mlogloss:0.70737\n",
      "[178]\tvalidation_0-mlogloss:0.70637\n",
      "[179]\tvalidation_0-mlogloss:0.70510\n",
      "[180]\tvalidation_0-mlogloss:0.70477\n",
      "[181]\tvalidation_0-mlogloss:0.70352\n",
      "[182]\tvalidation_0-mlogloss:0.70193\n",
      "[183]\tvalidation_0-mlogloss:0.70061\n",
      "[184]\tvalidation_0-mlogloss:0.69994\n",
      "[185]\tvalidation_0-mlogloss:0.69890\n",
      "[186]\tvalidation_0-mlogloss:0.69794\n",
      "[187]\tvalidation_0-mlogloss:0.69736\n",
      "[188]\tvalidation_0-mlogloss:0.69640\n",
      "[189]\tvalidation_0-mlogloss:0.69501\n",
      "[190]\tvalidation_0-mlogloss:0.69430\n",
      "[191]\tvalidation_0-mlogloss:0.69302\n",
      "[192]\tvalidation_0-mlogloss:0.69191\n",
      "[193]\tvalidation_0-mlogloss:0.69080\n",
      "[194]\tvalidation_0-mlogloss:0.68941\n",
      "[195]\tvalidation_0-mlogloss:0.68998\n",
      "[196]\tvalidation_0-mlogloss:0.68962\n",
      "[197]\tvalidation_0-mlogloss:0.68834\n",
      "[198]\tvalidation_0-mlogloss:0.68821\n",
      "[199]\tvalidation_0-mlogloss:0.68692\n",
      "[200]\tvalidation_0-mlogloss:0.68670\n",
      "[201]\tvalidation_0-mlogloss:0.68619\n",
      "[202]\tvalidation_0-mlogloss:0.68486\n",
      "[203]\tvalidation_0-mlogloss:0.68389\n",
      "[204]\tvalidation_0-mlogloss:0.68344\n",
      "[205]\tvalidation_0-mlogloss:0.68323\n",
      "[206]\tvalidation_0-mlogloss:0.68154\n",
      "[207]\tvalidation_0-mlogloss:0.68035\n",
      "[208]\tvalidation_0-mlogloss:0.67952\n",
      "[209]\tvalidation_0-mlogloss:0.67866\n",
      "[210]\tvalidation_0-mlogloss:0.67766\n",
      "[211]\tvalidation_0-mlogloss:0.67667\n",
      "[212]\tvalidation_0-mlogloss:0.67589\n",
      "[213]\tvalidation_0-mlogloss:0.67501\n",
      "[214]\tvalidation_0-mlogloss:0.67485\n",
      "[215]\tvalidation_0-mlogloss:0.67370\n",
      "[216]\tvalidation_0-mlogloss:0.67293\n",
      "[217]\tvalidation_0-mlogloss:0.67172\n",
      "[218]\tvalidation_0-mlogloss:0.67142\n",
      "[219]\tvalidation_0-mlogloss:0.67133\n",
      "[220]\tvalidation_0-mlogloss:0.67101\n",
      "[221]\tvalidation_0-mlogloss:0.67095\n",
      "[222]\tvalidation_0-mlogloss:0.67083\n",
      "[223]\tvalidation_0-mlogloss:0.66938\n",
      "[224]\tvalidation_0-mlogloss:0.66915\n",
      "[225]\tvalidation_0-mlogloss:0.66866\n",
      "[226]\tvalidation_0-mlogloss:0.66844\n",
      "[227]\tvalidation_0-mlogloss:0.66769\n",
      "[228]\tvalidation_0-mlogloss:0.66739\n",
      "[229]\tvalidation_0-mlogloss:0.66658\n",
      "[230]\tvalidation_0-mlogloss:0.66592\n",
      "[231]\tvalidation_0-mlogloss:0.66571\n",
      "[232]\tvalidation_0-mlogloss:0.66504\n",
      "[233]\tvalidation_0-mlogloss:0.66445\n",
      "[234]\tvalidation_0-mlogloss:0.66428\n",
      "[235]\tvalidation_0-mlogloss:0.66532\n",
      "[236]\tvalidation_0-mlogloss:0.66499\n",
      "[237]\tvalidation_0-mlogloss:0.66468\n",
      "[238]\tvalidation_0-mlogloss:0.66412\n",
      "[239]\tvalidation_0-mlogloss:0.66468\n",
      "[240]\tvalidation_0-mlogloss:0.66438\n",
      "[241]\tvalidation_0-mlogloss:0.66364\n",
      "[242]\tvalidation_0-mlogloss:0.66285\n",
      "[243]\tvalidation_0-mlogloss:0.66180\n",
      "[244]\tvalidation_0-mlogloss:0.66113\n",
      "[245]\tvalidation_0-mlogloss:0.66124\n",
      "[246]\tvalidation_0-mlogloss:0.66071\n",
      "[247]\tvalidation_0-mlogloss:0.65996\n",
      "[248]\tvalidation_0-mlogloss:0.65864\n",
      "[249]\tvalidation_0-mlogloss:0.65795\n",
      "[250]\tvalidation_0-mlogloss:0.65723\n",
      "[251]\tvalidation_0-mlogloss:0.65752\n",
      "[252]\tvalidation_0-mlogloss:0.65722\n",
      "[253]\tvalidation_0-mlogloss:0.65679\n",
      "[254]\tvalidation_0-mlogloss:0.65653\n",
      "[255]\tvalidation_0-mlogloss:0.65592\n",
      "[256]\tvalidation_0-mlogloss:0.65549\n",
      "[257]\tvalidation_0-mlogloss:0.65490\n",
      "[258]\tvalidation_0-mlogloss:0.65433\n",
      "[259]\tvalidation_0-mlogloss:0.65372\n",
      "[260]\tvalidation_0-mlogloss:0.65325\n",
      "[261]\tvalidation_0-mlogloss:0.65288\n",
      "[262]\tvalidation_0-mlogloss:0.65224\n",
      "[263]\tvalidation_0-mlogloss:0.65195\n",
      "[264]\tvalidation_0-mlogloss:0.65226\n",
      "[265]\tvalidation_0-mlogloss:0.65237\n",
      "[266]\tvalidation_0-mlogloss:0.65191\n",
      "[267]\tvalidation_0-mlogloss:0.65154\n",
      "[268]\tvalidation_0-mlogloss:0.65156\n",
      "[269]\tvalidation_0-mlogloss:0.65110\n",
      "[270]\tvalidation_0-mlogloss:0.65064\n",
      "[271]\tvalidation_0-mlogloss:0.64951\n",
      "[272]\tvalidation_0-mlogloss:0.64893\n",
      "[273]\tvalidation_0-mlogloss:0.64794\n",
      "[274]\tvalidation_0-mlogloss:0.64751\n",
      "[275]\tvalidation_0-mlogloss:0.64743\n",
      "[276]\tvalidation_0-mlogloss:0.64679\n",
      "[277]\tvalidation_0-mlogloss:0.64633\n",
      "[278]\tvalidation_0-mlogloss:0.64540\n",
      "[279]\tvalidation_0-mlogloss:0.64543\n",
      "[280]\tvalidation_0-mlogloss:0.64500\n",
      "[281]\tvalidation_0-mlogloss:0.64499\n",
      "[282]\tvalidation_0-mlogloss:0.64463\n",
      "[283]\tvalidation_0-mlogloss:0.64400\n",
      "[284]\tvalidation_0-mlogloss:0.64318\n",
      "[285]\tvalidation_0-mlogloss:0.64307\n",
      "[286]\tvalidation_0-mlogloss:0.64196\n",
      "[287]\tvalidation_0-mlogloss:0.64153\n",
      "[288]\tvalidation_0-mlogloss:0.64091\n",
      "[289]\tvalidation_0-mlogloss:0.64064\n",
      "[290]\tvalidation_0-mlogloss:0.64049\n",
      "[291]\tvalidation_0-mlogloss:0.64016\n",
      "[292]\tvalidation_0-mlogloss:0.63994\n",
      "[293]\tvalidation_0-mlogloss:0.63959\n",
      "[294]\tvalidation_0-mlogloss:0.63899\n",
      "[295]\tvalidation_0-mlogloss:0.63921\n",
      "[296]\tvalidation_0-mlogloss:0.63998\n",
      "[297]\tvalidation_0-mlogloss:0.64014\n",
      "[298]\tvalidation_0-mlogloss:0.63976\n",
      "[299]\tvalidation_0-mlogloss:0.63949\n",
      "[300]\tvalidation_0-mlogloss:0.63891\n",
      "[301]\tvalidation_0-mlogloss:0.63846\n",
      "[302]\tvalidation_0-mlogloss:0.63829\n",
      "[303]\tvalidation_0-mlogloss:0.63748\n",
      "[304]\tvalidation_0-mlogloss:0.63723\n",
      "[305]\tvalidation_0-mlogloss:0.63760\n",
      "[306]\tvalidation_0-mlogloss:0.63682\n",
      "[307]\tvalidation_0-mlogloss:0.63620\n",
      "[308]\tvalidation_0-mlogloss:0.63611\n",
      "[309]\tvalidation_0-mlogloss:0.63552\n",
      "[310]\tvalidation_0-mlogloss:0.63500\n",
      "[311]\tvalidation_0-mlogloss:0.63488\n",
      "[312]\tvalidation_0-mlogloss:0.63408\n",
      "[313]\tvalidation_0-mlogloss:0.63356\n",
      "[314]\tvalidation_0-mlogloss:0.63332\n",
      "[315]\tvalidation_0-mlogloss:0.63305\n",
      "[316]\tvalidation_0-mlogloss:0.63239\n",
      "[317]\tvalidation_0-mlogloss:0.63299\n",
      "[318]\tvalidation_0-mlogloss:0.63331\n",
      "[319]\tvalidation_0-mlogloss:0.63300\n",
      "[320]\tvalidation_0-mlogloss:0.63278\n",
      "[321]\tvalidation_0-mlogloss:0.63293\n",
      "[322]\tvalidation_0-mlogloss:0.63244\n",
      "[323]\tvalidation_0-mlogloss:0.63204\n",
      "[324]\tvalidation_0-mlogloss:0.63203\n",
      "[325]\tvalidation_0-mlogloss:0.63217\n",
      "[326]\tvalidation_0-mlogloss:0.63258\n",
      "[327]\tvalidation_0-mlogloss:0.63237\n",
      "[328]\tvalidation_0-mlogloss:0.63197\n",
      "[329]\tvalidation_0-mlogloss:0.63191\n",
      "[330]\tvalidation_0-mlogloss:0.63193\n",
      "[331]\tvalidation_0-mlogloss:0.63234\n",
      "[332]\tvalidation_0-mlogloss:0.63321\n",
      "[333]\tvalidation_0-mlogloss:0.63307\n",
      "[334]\tvalidation_0-mlogloss:0.63307\n",
      "[335]\tvalidation_0-mlogloss:0.63245\n",
      "[336]\tvalidation_0-mlogloss:0.63174\n",
      "[337]\tvalidation_0-mlogloss:0.63172\n",
      "[338]\tvalidation_0-mlogloss:0.63139\n",
      "[339]\tvalidation_0-mlogloss:0.63155\n",
      "[340]\tvalidation_0-mlogloss:0.63163\n",
      "[341]\tvalidation_0-mlogloss:0.63168\n",
      "[342]\tvalidation_0-mlogloss:0.63080\n",
      "[343]\tvalidation_0-mlogloss:0.63109\n",
      "[344]\tvalidation_0-mlogloss:0.63080\n",
      "[345]\tvalidation_0-mlogloss:0.63031\n",
      "[346]\tvalidation_0-mlogloss:0.63031\n",
      "[347]\tvalidation_0-mlogloss:0.63029\n",
      "[348]\tvalidation_0-mlogloss:0.63068\n",
      "[349]\tvalidation_0-mlogloss:0.62993\n",
      "[350]\tvalidation_0-mlogloss:0.62974\n",
      "[351]\tvalidation_0-mlogloss:0.62963\n",
      "[352]\tvalidation_0-mlogloss:0.62985\n",
      "[353]\tvalidation_0-mlogloss:0.62971\n",
      "[354]\tvalidation_0-mlogloss:0.63008\n",
      "[355]\tvalidation_0-mlogloss:0.63008\n",
      "[356]\tvalidation_0-mlogloss:0.62987\n",
      "[357]\tvalidation_0-mlogloss:0.62981\n",
      "[358]\tvalidation_0-mlogloss:0.62897\n",
      "[359]\tvalidation_0-mlogloss:0.62880\n",
      "[360]\tvalidation_0-mlogloss:0.62824\n",
      "[361]\tvalidation_0-mlogloss:0.62779\n",
      "[362]\tvalidation_0-mlogloss:0.62774\n",
      "[363]\tvalidation_0-mlogloss:0.62766\n",
      "[364]\tvalidation_0-mlogloss:0.62772\n",
      "[365]\tvalidation_0-mlogloss:0.62754\n",
      "[366]\tvalidation_0-mlogloss:0.62759\n",
      "[367]\tvalidation_0-mlogloss:0.62724\n",
      "[368]\tvalidation_0-mlogloss:0.62665\n",
      "[369]\tvalidation_0-mlogloss:0.62689\n",
      "[370]\tvalidation_0-mlogloss:0.62604\n",
      "[371]\tvalidation_0-mlogloss:0.62617\n",
      "[372]\tvalidation_0-mlogloss:0.62570\n",
      "[373]\tvalidation_0-mlogloss:0.62598\n",
      "[374]\tvalidation_0-mlogloss:0.62565\n",
      "[375]\tvalidation_0-mlogloss:0.62530\n",
      "[376]\tvalidation_0-mlogloss:0.62527\n",
      "[377]\tvalidation_0-mlogloss:0.62511\n",
      "[378]\tvalidation_0-mlogloss:0.62526\n",
      "[379]\tvalidation_0-mlogloss:0.62556\n",
      "[380]\tvalidation_0-mlogloss:0.62550\n",
      "[381]\tvalidation_0-mlogloss:0.62576\n",
      "[382]\tvalidation_0-mlogloss:0.62592\n",
      "[383]\tvalidation_0-mlogloss:0.62578\n",
      "[384]\tvalidation_0-mlogloss:0.62561\n",
      "[385]\tvalidation_0-mlogloss:0.62594\n",
      "[386]\tvalidation_0-mlogloss:0.62552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      1499\n",
      "           1       0.75      0.65      0.70      1499\n",
      "           2       0.83      0.87      0.85      1499\n",
      "           3       0.86      0.46      0.60      1499\n",
      "           4       0.97      0.96      0.96      1499\n",
      "           5       0.94      0.90      0.92      1499\n",
      "           6       0.87      0.96      0.92      1499\n",
      "           7       0.97      0.80      0.88      1499\n",
      "           8       0.85      0.92      0.89      1499\n",
      "           9       0.94      0.81      0.87      1499\n",
      "          10       0.98      0.96      0.97      1499\n",
      "          11       0.94      0.98      0.96      1499\n",
      "          12       0.85      0.94      0.90      1499\n",
      "          13       0.73      0.90      0.81      1499\n",
      "          14       0.65      0.80      0.72      1499\n",
      "          15       0.94      0.98      0.96      1499\n",
      "          16       0.94      0.82      0.88      1499\n",
      "          17       0.98      0.89      0.94      1499\n",
      "          18       0.70      0.95      0.80      1499\n",
      "          19       0.90      0.65      0.76      1499\n",
      "          20       0.95      0.97      0.96      1499\n",
      "          21       0.88      0.89      0.88      1499\n",
      "          22       0.70      0.78      0.73      1499\n",
      "          23       0.96      0.97      0.97      1499\n",
      "          24       0.97      0.93      0.95      1499\n",
      "\n",
      "    accuracy                           0.86     37475\n",
      "   macro avg       0.87      0.86      0.86     37475\n",
      "weighted avg       0.87      0.86      0.86     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3=XGBClassifier(n_estimators=500)\n",
    "model3.fit(x32,y32,early_stopping_rounds=10, eval_set=[(xv32, yv32)])\n",
    "y_pred=model3.predict(xt32)\n",
    "print(classification_report(yt32,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8f654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Dense(32, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(25, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "300aa492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9375/9375 [==============================] - 7s 711us/step - loss: 0.7572 - val_loss: 1.0026\n",
      "Epoch 2/10\n",
      "9375/9375 [==============================] - 7s 699us/step - loss: 0.2478 - val_loss: 0.9522\n",
      "Epoch 3/10\n",
      "9375/9375 [==============================] - 7s 708us/step - loss: 0.1755 - val_loss: 0.8248\n",
      "Epoch 4/10\n",
      "9375/9375 [==============================] - 7s 709us/step - loss: 0.1439 - val_loss: 0.8511\n",
      "Epoch 5/10\n",
      "9375/9375 [==============================] - 7s 699us/step - loss: 0.1243 - val_loss: 0.9427\n",
      "Epoch 6/10\n",
      "9375/9375 [==============================] - 7s 708us/step - loss: 0.1116 - val_loss: 0.9574\n",
      "Epoch 7/10\n",
      "9375/9375 [==============================] - 7s 699us/step - loss: 0.1031 - val_loss: 0.8959\n",
      "Epoch 8/10\n",
      "9375/9375 [==============================] - 7s 695us/step - loss: 0.0962 - val_loss: 0.8762\n",
      "Epoch 9/10\n",
      "9375/9375 [==============================] - 7s 715us/step - loss: 0.0884 - val_loss: 0.9605\n",
      "Epoch 10/10\n",
      "9375/9375 [==============================] - 7s 698us/step - loss: 0.0836 - val_loss: 0.9324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2dfc7bf70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    x32,y32,epochs=10,validation_data=(xv32,yv32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eedffd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172/1172 [==============================] - 0s 290us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.74      1499\n",
      "           1       0.64      0.72      0.68      1499\n",
      "           2       0.86      0.74      0.80      1499\n",
      "           3       0.72      0.44      0.55      1499\n",
      "           4       0.96      0.98      0.97      1499\n",
      "           5       0.97      0.87      0.92      1499\n",
      "           6       0.93      0.97      0.95      1499\n",
      "           7       0.95      0.95      0.95      1499\n",
      "           8       0.72      0.90      0.80      1499\n",
      "           9       0.86      0.93      0.89      1499\n",
      "          10       0.94      0.90      0.92      1499\n",
      "          11       0.95      0.98      0.97      1499\n",
      "          12       0.79      0.96      0.86      1499\n",
      "          13       0.77      0.79      0.78      1499\n",
      "          14       0.67      0.82      0.74      1499\n",
      "          15       0.93      0.99      0.96      1499\n",
      "          16       0.93      0.62      0.74      1499\n",
      "          17       0.99      0.94      0.96      1499\n",
      "          18       0.71      0.94      0.81      1499\n",
      "          19       0.85      0.46      0.60      1499\n",
      "          20       0.94      1.00      0.97      1499\n",
      "          21       0.75      0.94      0.83      1499\n",
      "          22       0.66      0.78      0.72      1499\n",
      "          23       0.96      0.97      0.96      1499\n",
      "          24       0.96      0.76      0.85      1499\n",
      "\n",
      "    accuracy                           0.84     37475\n",
      "   macro avg       0.85      0.84      0.84     37475\n",
      "weighted avg       0.85      0.84      0.84     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model2.predict(xt32)).numpy(),axis=1)\n",
    "print(classification_report(yt32,y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead93bfd",
   "metadata": {},
   "source": [
    "## 0-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c5c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21e76096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.39933\n",
      "[1]\tvalidation_0-mlogloss:2.13371\n",
      "[2]\tvalidation_0-mlogloss:1.96682\n",
      "[3]\tvalidation_0-mlogloss:1.82947\n",
      "[4]\tvalidation_0-mlogloss:1.71458\n",
      "[5]\tvalidation_0-mlogloss:1.62135\n",
      "[6]\tvalidation_0-mlogloss:1.53765\n",
      "[7]\tvalidation_0-mlogloss:1.46627\n",
      "[8]\tvalidation_0-mlogloss:1.40357\n",
      "[9]\tvalidation_0-mlogloss:1.34584\n",
      "[10]\tvalidation_0-mlogloss:1.29171\n",
      "[11]\tvalidation_0-mlogloss:1.24807\n",
      "[12]\tvalidation_0-mlogloss:1.21102\n",
      "[13]\tvalidation_0-mlogloss:1.16864\n",
      "[14]\tvalidation_0-mlogloss:1.13320\n",
      "[15]\tvalidation_0-mlogloss:1.09936\n",
      "[16]\tvalidation_0-mlogloss:1.06757\n",
      "[17]\tvalidation_0-mlogloss:1.03871\n",
      "[18]\tvalidation_0-mlogloss:1.01180\n",
      "[19]\tvalidation_0-mlogloss:0.98949\n",
      "[20]\tvalidation_0-mlogloss:0.96749\n",
      "[21]\tvalidation_0-mlogloss:0.94527\n",
      "[22]\tvalidation_0-mlogloss:0.92945\n",
      "[23]\tvalidation_0-mlogloss:0.91330\n",
      "[24]\tvalidation_0-mlogloss:0.89708\n",
      "[25]\tvalidation_0-mlogloss:0.88087\n",
      "[26]\tvalidation_0-mlogloss:0.86842\n",
      "[27]\tvalidation_0-mlogloss:0.85007\n",
      "[28]\tvalidation_0-mlogloss:0.83587\n",
      "[29]\tvalidation_0-mlogloss:0.82385\n",
      "[30]\tvalidation_0-mlogloss:0.80967\n",
      "[31]\tvalidation_0-mlogloss:0.79498\n",
      "[32]\tvalidation_0-mlogloss:0.78096\n",
      "[33]\tvalidation_0-mlogloss:0.76848\n",
      "[34]\tvalidation_0-mlogloss:0.75866\n",
      "[35]\tvalidation_0-mlogloss:0.75077\n",
      "[36]\tvalidation_0-mlogloss:0.74042\n",
      "[37]\tvalidation_0-mlogloss:0.73160\n",
      "[38]\tvalidation_0-mlogloss:0.72143\n",
      "[39]\tvalidation_0-mlogloss:0.71048\n",
      "[40]\tvalidation_0-mlogloss:0.70089\n",
      "[41]\tvalidation_0-mlogloss:0.69178\n",
      "[42]\tvalidation_0-mlogloss:0.68258\n",
      "[43]\tvalidation_0-mlogloss:0.67624\n",
      "[44]\tvalidation_0-mlogloss:0.66757\n",
      "[45]\tvalidation_0-mlogloss:0.65854\n",
      "[46]\tvalidation_0-mlogloss:0.65187\n",
      "[47]\tvalidation_0-mlogloss:0.64478\n",
      "[48]\tvalidation_0-mlogloss:0.63756\n",
      "[49]\tvalidation_0-mlogloss:0.63006\n",
      "[50]\tvalidation_0-mlogloss:0.62556\n",
      "[51]\tvalidation_0-mlogloss:0.61768\n",
      "[52]\tvalidation_0-mlogloss:0.61126\n",
      "[53]\tvalidation_0-mlogloss:0.60599\n",
      "[54]\tvalidation_0-mlogloss:0.59894\n",
      "[55]\tvalidation_0-mlogloss:0.59332\n",
      "[56]\tvalidation_0-mlogloss:0.58935\n",
      "[57]\tvalidation_0-mlogloss:0.58574\n",
      "[58]\tvalidation_0-mlogloss:0.58157\n",
      "[59]\tvalidation_0-mlogloss:0.57646\n",
      "[60]\tvalidation_0-mlogloss:0.57225\n",
      "[61]\tvalidation_0-mlogloss:0.56816\n",
      "[62]\tvalidation_0-mlogloss:0.56621\n",
      "[63]\tvalidation_0-mlogloss:0.56105\n",
      "[64]\tvalidation_0-mlogloss:0.55722\n",
      "[65]\tvalidation_0-mlogloss:0.55356\n",
      "[66]\tvalidation_0-mlogloss:0.54918\n",
      "[67]\tvalidation_0-mlogloss:0.54475\n",
      "[68]\tvalidation_0-mlogloss:0.53971\n",
      "[69]\tvalidation_0-mlogloss:0.53521\n",
      "[70]\tvalidation_0-mlogloss:0.53089\n",
      "[71]\tvalidation_0-mlogloss:0.52706\n",
      "[72]\tvalidation_0-mlogloss:0.52554\n",
      "[73]\tvalidation_0-mlogloss:0.52104\n",
      "[74]\tvalidation_0-mlogloss:0.51843\n",
      "[75]\tvalidation_0-mlogloss:0.51736\n",
      "[76]\tvalidation_0-mlogloss:0.51376\n",
      "[77]\tvalidation_0-mlogloss:0.50991\n",
      "[78]\tvalidation_0-mlogloss:0.50554\n",
      "[79]\tvalidation_0-mlogloss:0.50164\n",
      "[80]\tvalidation_0-mlogloss:0.49837\n",
      "[81]\tvalidation_0-mlogloss:0.49516\n",
      "[82]\tvalidation_0-mlogloss:0.49215\n",
      "[83]\tvalidation_0-mlogloss:0.48933\n",
      "[84]\tvalidation_0-mlogloss:0.48678\n",
      "[85]\tvalidation_0-mlogloss:0.48299\n",
      "[86]\tvalidation_0-mlogloss:0.47982\n",
      "[87]\tvalidation_0-mlogloss:0.47694\n",
      "[88]\tvalidation_0-mlogloss:0.47429\n",
      "[89]\tvalidation_0-mlogloss:0.47156\n",
      "[90]\tvalidation_0-mlogloss:0.46898\n",
      "[91]\tvalidation_0-mlogloss:0.46651\n",
      "[92]\tvalidation_0-mlogloss:0.46392\n",
      "[93]\tvalidation_0-mlogloss:0.46131\n",
      "[94]\tvalidation_0-mlogloss:0.45870\n",
      "[95]\tvalidation_0-mlogloss:0.45674\n",
      "[96]\tvalidation_0-mlogloss:0.45539\n",
      "[97]\tvalidation_0-mlogloss:0.45333\n",
      "[98]\tvalidation_0-mlogloss:0.45117\n",
      "[99]\tvalidation_0-mlogloss:0.44908\n",
      "[100]\tvalidation_0-mlogloss:0.44721\n",
      "[101]\tvalidation_0-mlogloss:0.44464\n",
      "[102]\tvalidation_0-mlogloss:0.44321\n",
      "[103]\tvalidation_0-mlogloss:0.44127\n",
      "[104]\tvalidation_0-mlogloss:0.43899\n",
      "[105]\tvalidation_0-mlogloss:0.43803\n",
      "[106]\tvalidation_0-mlogloss:0.43657\n",
      "[107]\tvalidation_0-mlogloss:0.43532\n",
      "[108]\tvalidation_0-mlogloss:0.43307\n",
      "[109]\tvalidation_0-mlogloss:0.43067\n",
      "[110]\tvalidation_0-mlogloss:0.42830\n",
      "[111]\tvalidation_0-mlogloss:0.42769\n",
      "[112]\tvalidation_0-mlogloss:0.42537\n",
      "[113]\tvalidation_0-mlogloss:0.42302\n",
      "[114]\tvalidation_0-mlogloss:0.42144\n",
      "[115]\tvalidation_0-mlogloss:0.42061\n",
      "[116]\tvalidation_0-mlogloss:0.41857\n",
      "[117]\tvalidation_0-mlogloss:0.41721\n",
      "[118]\tvalidation_0-mlogloss:0.41549\n",
      "[119]\tvalidation_0-mlogloss:0.41378\n",
      "[120]\tvalidation_0-mlogloss:0.41158\n",
      "[121]\tvalidation_0-mlogloss:0.40988\n",
      "[122]\tvalidation_0-mlogloss:0.40871\n",
      "[123]\tvalidation_0-mlogloss:0.40672\n",
      "[124]\tvalidation_0-mlogloss:0.40571\n",
      "[125]\tvalidation_0-mlogloss:0.40432\n",
      "[126]\tvalidation_0-mlogloss:0.40372\n",
      "[127]\tvalidation_0-mlogloss:0.40207\n",
      "[128]\tvalidation_0-mlogloss:0.40093\n",
      "[129]\tvalidation_0-mlogloss:0.39972\n",
      "[130]\tvalidation_0-mlogloss:0.39880\n",
      "[131]\tvalidation_0-mlogloss:0.39699\n",
      "[132]\tvalidation_0-mlogloss:0.39551\n",
      "[133]\tvalidation_0-mlogloss:0.39494\n",
      "[134]\tvalidation_0-mlogloss:0.39452\n",
      "[135]\tvalidation_0-mlogloss:0.39313\n",
      "[136]\tvalidation_0-mlogloss:0.39230\n",
      "[137]\tvalidation_0-mlogloss:0.39160\n",
      "[138]\tvalidation_0-mlogloss:0.39135\n",
      "[139]\tvalidation_0-mlogloss:0.39067\n",
      "[140]\tvalidation_0-mlogloss:0.38881\n",
      "[141]\tvalidation_0-mlogloss:0.38864\n",
      "[142]\tvalidation_0-mlogloss:0.38774\n",
      "[143]\tvalidation_0-mlogloss:0.38663\n",
      "[144]\tvalidation_0-mlogloss:0.38560\n",
      "[145]\tvalidation_0-mlogloss:0.38409\n",
      "[146]\tvalidation_0-mlogloss:0.38289\n",
      "[147]\tvalidation_0-mlogloss:0.38239\n",
      "[148]\tvalidation_0-mlogloss:0.38092\n",
      "[149]\tvalidation_0-mlogloss:0.37958\n",
      "[150]\tvalidation_0-mlogloss:0.37844\n",
      "[151]\tvalidation_0-mlogloss:0.37759\n",
      "[152]\tvalidation_0-mlogloss:0.37752\n",
      "[153]\tvalidation_0-mlogloss:0.37595\n",
      "[154]\tvalidation_0-mlogloss:0.37556\n",
      "[155]\tvalidation_0-mlogloss:0.37479\n",
      "[156]\tvalidation_0-mlogloss:0.37424\n",
      "[157]\tvalidation_0-mlogloss:0.37202\n",
      "[158]\tvalidation_0-mlogloss:0.37124\n",
      "[159]\tvalidation_0-mlogloss:0.36965\n",
      "[160]\tvalidation_0-mlogloss:0.36873\n",
      "[161]\tvalidation_0-mlogloss:0.36801\n",
      "[162]\tvalidation_0-mlogloss:0.36736\n",
      "[163]\tvalidation_0-mlogloss:0.36754\n",
      "[164]\tvalidation_0-mlogloss:0.36633\n",
      "[165]\tvalidation_0-mlogloss:0.36559\n",
      "[166]\tvalidation_0-mlogloss:0.36479\n",
      "[167]\tvalidation_0-mlogloss:0.36392\n",
      "[168]\tvalidation_0-mlogloss:0.36352\n",
      "[169]\tvalidation_0-mlogloss:0.36237\n",
      "[170]\tvalidation_0-mlogloss:0.36059\n",
      "[171]\tvalidation_0-mlogloss:0.35975\n",
      "[172]\tvalidation_0-mlogloss:0.35857\n",
      "[173]\tvalidation_0-mlogloss:0.35781\n",
      "[174]\tvalidation_0-mlogloss:0.35780\n",
      "[175]\tvalidation_0-mlogloss:0.35656\n",
      "[176]\tvalidation_0-mlogloss:0.35632\n",
      "[177]\tvalidation_0-mlogloss:0.35536\n",
      "[178]\tvalidation_0-mlogloss:0.35436\n",
      "[179]\tvalidation_0-mlogloss:0.35337\n",
      "[180]\tvalidation_0-mlogloss:0.35273\n",
      "[181]\tvalidation_0-mlogloss:0.35288\n",
      "[182]\tvalidation_0-mlogloss:0.35190\n",
      "[183]\tvalidation_0-mlogloss:0.35152\n",
      "[184]\tvalidation_0-mlogloss:0.35200\n",
      "[185]\tvalidation_0-mlogloss:0.35084\n",
      "[186]\tvalidation_0-mlogloss:0.34999\n",
      "[187]\tvalidation_0-mlogloss:0.34939\n",
      "[188]\tvalidation_0-mlogloss:0.34909\n",
      "[189]\tvalidation_0-mlogloss:0.34882\n",
      "[190]\tvalidation_0-mlogloss:0.34849\n",
      "[191]\tvalidation_0-mlogloss:0.34820\n",
      "[192]\tvalidation_0-mlogloss:0.34725\n",
      "[193]\tvalidation_0-mlogloss:0.34656\n",
      "[194]\tvalidation_0-mlogloss:0.34643\n",
      "[195]\tvalidation_0-mlogloss:0.34621\n",
      "[196]\tvalidation_0-mlogloss:0.34599\n",
      "[197]\tvalidation_0-mlogloss:0.34508\n",
      "[198]\tvalidation_0-mlogloss:0.34423\n",
      "[199]\tvalidation_0-mlogloss:0.34494\n",
      "[200]\tvalidation_0-mlogloss:0.34386\n",
      "[201]\tvalidation_0-mlogloss:0.34347\n",
      "[202]\tvalidation_0-mlogloss:0.34223\n",
      "[203]\tvalidation_0-mlogloss:0.34136\n",
      "[204]\tvalidation_0-mlogloss:0.34091\n",
      "[205]\tvalidation_0-mlogloss:0.33951\n",
      "[206]\tvalidation_0-mlogloss:0.33868\n",
      "[207]\tvalidation_0-mlogloss:0.33866\n",
      "[208]\tvalidation_0-mlogloss:0.33830\n",
      "[209]\tvalidation_0-mlogloss:0.33834\n",
      "[210]\tvalidation_0-mlogloss:0.33802\n",
      "[211]\tvalidation_0-mlogloss:0.33754\n",
      "[212]\tvalidation_0-mlogloss:0.33669\n",
      "[213]\tvalidation_0-mlogloss:0.33632\n",
      "[214]\tvalidation_0-mlogloss:0.33573\n",
      "[215]\tvalidation_0-mlogloss:0.33575\n",
      "[216]\tvalidation_0-mlogloss:0.33530\n",
      "[217]\tvalidation_0-mlogloss:0.33477\n",
      "[218]\tvalidation_0-mlogloss:0.33434\n",
      "[219]\tvalidation_0-mlogloss:0.33412\n",
      "[220]\tvalidation_0-mlogloss:0.33329\n",
      "[221]\tvalidation_0-mlogloss:0.33337\n",
      "[222]\tvalidation_0-mlogloss:0.33315\n",
      "[223]\tvalidation_0-mlogloss:0.33335\n",
      "[224]\tvalidation_0-mlogloss:0.33255\n",
      "[225]\tvalidation_0-mlogloss:0.33286\n",
      "[226]\tvalidation_0-mlogloss:0.33216\n",
      "[227]\tvalidation_0-mlogloss:0.33146\n",
      "[228]\tvalidation_0-mlogloss:0.33096\n",
      "[229]\tvalidation_0-mlogloss:0.33060\n",
      "[230]\tvalidation_0-mlogloss:0.33026\n",
      "[231]\tvalidation_0-mlogloss:0.33028\n",
      "[232]\tvalidation_0-mlogloss:0.33021\n",
      "[233]\tvalidation_0-mlogloss:0.33003\n",
      "[234]\tvalidation_0-mlogloss:0.33001\n",
      "[235]\tvalidation_0-mlogloss:0.32945\n",
      "[236]\tvalidation_0-mlogloss:0.32925\n",
      "[237]\tvalidation_0-mlogloss:0.32877\n",
      "[238]\tvalidation_0-mlogloss:0.32837\n",
      "[239]\tvalidation_0-mlogloss:0.32826\n",
      "[240]\tvalidation_0-mlogloss:0.32752\n",
      "[241]\tvalidation_0-mlogloss:0.32688\n",
      "[242]\tvalidation_0-mlogloss:0.32620\n",
      "[243]\tvalidation_0-mlogloss:0.32570\n",
      "[244]\tvalidation_0-mlogloss:0.32529\n",
      "[245]\tvalidation_0-mlogloss:0.32491\n",
      "[246]\tvalidation_0-mlogloss:0.32488\n",
      "[247]\tvalidation_0-mlogloss:0.32442\n",
      "[248]\tvalidation_0-mlogloss:0.32418\n",
      "[249]\tvalidation_0-mlogloss:0.32356\n",
      "[250]\tvalidation_0-mlogloss:0.32344\n",
      "[251]\tvalidation_0-mlogloss:0.32308\n",
      "[252]\tvalidation_0-mlogloss:0.32235\n",
      "[253]\tvalidation_0-mlogloss:0.32224\n",
      "[254]\tvalidation_0-mlogloss:0.32236\n",
      "[255]\tvalidation_0-mlogloss:0.32213\n",
      "[256]\tvalidation_0-mlogloss:0.32182\n",
      "[257]\tvalidation_0-mlogloss:0.32137\n",
      "[258]\tvalidation_0-mlogloss:0.32162\n",
      "[259]\tvalidation_0-mlogloss:0.32168\n",
      "[260]\tvalidation_0-mlogloss:0.32099\n",
      "[261]\tvalidation_0-mlogloss:0.32103\n",
      "[262]\tvalidation_0-mlogloss:0.32070\n",
      "[263]\tvalidation_0-mlogloss:0.32070\n",
      "[264]\tvalidation_0-mlogloss:0.32046\n",
      "[265]\tvalidation_0-mlogloss:0.32014\n",
      "[266]\tvalidation_0-mlogloss:0.32007\n",
      "[267]\tvalidation_0-mlogloss:0.31960\n",
      "[268]\tvalidation_0-mlogloss:0.31928\n",
      "[269]\tvalidation_0-mlogloss:0.31858\n",
      "[270]\tvalidation_0-mlogloss:0.31823\n",
      "[271]\tvalidation_0-mlogloss:0.31820\n",
      "[272]\tvalidation_0-mlogloss:0.31769\n",
      "[273]\tvalidation_0-mlogloss:0.31713\n",
      "[274]\tvalidation_0-mlogloss:0.31674\n",
      "[275]\tvalidation_0-mlogloss:0.31686\n",
      "[276]\tvalidation_0-mlogloss:0.31689\n",
      "[277]\tvalidation_0-mlogloss:0.31712\n",
      "[278]\tvalidation_0-mlogloss:0.31655\n",
      "[279]\tvalidation_0-mlogloss:0.31655\n",
      "[280]\tvalidation_0-mlogloss:0.31634\n",
      "[281]\tvalidation_0-mlogloss:0.31658\n",
      "[282]\tvalidation_0-mlogloss:0.31631\n",
      "[283]\tvalidation_0-mlogloss:0.31596\n",
      "[284]\tvalidation_0-mlogloss:0.31517\n",
      "[285]\tvalidation_0-mlogloss:0.31553\n",
      "[286]\tvalidation_0-mlogloss:0.31517\n",
      "[287]\tvalidation_0-mlogloss:0.31476\n",
      "[288]\tvalidation_0-mlogloss:0.31446\n",
      "[289]\tvalidation_0-mlogloss:0.31466\n",
      "[290]\tvalidation_0-mlogloss:0.31404\n",
      "[291]\tvalidation_0-mlogloss:0.31415\n",
      "[292]\tvalidation_0-mlogloss:0.31394\n",
      "[293]\tvalidation_0-mlogloss:0.31378\n",
      "[294]\tvalidation_0-mlogloss:0.31350\n",
      "[295]\tvalidation_0-mlogloss:0.31365\n",
      "[296]\tvalidation_0-mlogloss:0.31332\n",
      "[297]\tvalidation_0-mlogloss:0.31299\n",
      "[298]\tvalidation_0-mlogloss:0.31267\n",
      "[299]\tvalidation_0-mlogloss:0.31207\n",
      "[300]\tvalidation_0-mlogloss:0.31191\n",
      "[301]\tvalidation_0-mlogloss:0.31141\n",
      "[302]\tvalidation_0-mlogloss:0.31113\n",
      "[303]\tvalidation_0-mlogloss:0.31089\n",
      "[304]\tvalidation_0-mlogloss:0.31078\n",
      "[305]\tvalidation_0-mlogloss:0.31095\n",
      "[306]\tvalidation_0-mlogloss:0.31061\n",
      "[307]\tvalidation_0-mlogloss:0.31064\n",
      "[308]\tvalidation_0-mlogloss:0.31024\n",
      "[309]\tvalidation_0-mlogloss:0.31061\n",
      "[310]\tvalidation_0-mlogloss:0.31013\n",
      "[311]\tvalidation_0-mlogloss:0.31003\n",
      "[312]\tvalidation_0-mlogloss:0.30965\n",
      "[313]\tvalidation_0-mlogloss:0.30963\n",
      "[314]\tvalidation_0-mlogloss:0.30946\n",
      "[315]\tvalidation_0-mlogloss:0.30927\n",
      "[316]\tvalidation_0-mlogloss:0.30872\n",
      "[317]\tvalidation_0-mlogloss:0.30919\n",
      "[318]\tvalidation_0-mlogloss:0.30917\n",
      "[319]\tvalidation_0-mlogloss:0.30879\n",
      "[320]\tvalidation_0-mlogloss:0.30876\n",
      "[321]\tvalidation_0-mlogloss:0.30853\n",
      "[322]\tvalidation_0-mlogloss:0.30837\n",
      "[323]\tvalidation_0-mlogloss:0.30800\n",
      "[324]\tvalidation_0-mlogloss:0.30763\n",
      "[325]\tvalidation_0-mlogloss:0.30751\n",
      "[326]\tvalidation_0-mlogloss:0.30741\n",
      "[327]\tvalidation_0-mlogloss:0.30765\n",
      "[328]\tvalidation_0-mlogloss:0.30724\n",
      "[329]\tvalidation_0-mlogloss:0.30706\n",
      "[330]\tvalidation_0-mlogloss:0.30697\n",
      "[331]\tvalidation_0-mlogloss:0.30660\n",
      "[332]\tvalidation_0-mlogloss:0.30652\n",
      "[333]\tvalidation_0-mlogloss:0.30664\n",
      "[334]\tvalidation_0-mlogloss:0.30651\n",
      "[335]\tvalidation_0-mlogloss:0.30621\n",
      "[336]\tvalidation_0-mlogloss:0.30619\n",
      "[337]\tvalidation_0-mlogloss:0.30603\n",
      "[338]\tvalidation_0-mlogloss:0.30565\n",
      "[339]\tvalidation_0-mlogloss:0.30523\n",
      "[340]\tvalidation_0-mlogloss:0.30496\n",
      "[341]\tvalidation_0-mlogloss:0.30479\n",
      "[342]\tvalidation_0-mlogloss:0.30481\n",
      "[343]\tvalidation_0-mlogloss:0.30497\n",
      "[344]\tvalidation_0-mlogloss:0.30497\n",
      "[345]\tvalidation_0-mlogloss:0.30517\n",
      "[346]\tvalidation_0-mlogloss:0.30486\n",
      "[347]\tvalidation_0-mlogloss:0.30430\n",
      "[348]\tvalidation_0-mlogloss:0.30427\n",
      "[349]\tvalidation_0-mlogloss:0.30436\n",
      "[350]\tvalidation_0-mlogloss:0.30391\n",
      "[351]\tvalidation_0-mlogloss:0.30396\n",
      "[352]\tvalidation_0-mlogloss:0.30409\n",
      "[353]\tvalidation_0-mlogloss:0.30397\n",
      "[354]\tvalidation_0-mlogloss:0.30389\n",
      "[355]\tvalidation_0-mlogloss:0.30386\n",
      "[356]\tvalidation_0-mlogloss:0.30353\n",
      "[357]\tvalidation_0-mlogloss:0.30335\n",
      "[358]\tvalidation_0-mlogloss:0.30330\n",
      "[359]\tvalidation_0-mlogloss:0.30329\n",
      "[360]\tvalidation_0-mlogloss:0.30325\n",
      "[361]\tvalidation_0-mlogloss:0.30329\n",
      "[362]\tvalidation_0-mlogloss:0.30299\n",
      "[363]\tvalidation_0-mlogloss:0.30292\n",
      "[364]\tvalidation_0-mlogloss:0.30285\n",
      "[365]\tvalidation_0-mlogloss:0.30252\n",
      "[366]\tvalidation_0-mlogloss:0.30236\n",
      "[367]\tvalidation_0-mlogloss:0.30220\n",
      "[368]\tvalidation_0-mlogloss:0.30203\n",
      "[369]\tvalidation_0-mlogloss:0.30219\n",
      "[370]\tvalidation_0-mlogloss:0.30182\n",
      "[371]\tvalidation_0-mlogloss:0.30171\n",
      "[372]\tvalidation_0-mlogloss:0.30156\n",
      "[373]\tvalidation_0-mlogloss:0.30131\n",
      "[374]\tvalidation_0-mlogloss:0.30146\n",
      "[375]\tvalidation_0-mlogloss:0.30102\n",
      "[376]\tvalidation_0-mlogloss:0.30077\n",
      "[377]\tvalidation_0-mlogloss:0.30070\n",
      "[378]\tvalidation_0-mlogloss:0.30062\n",
      "[379]\tvalidation_0-mlogloss:0.30053\n",
      "[380]\tvalidation_0-mlogloss:0.30018\n",
      "[381]\tvalidation_0-mlogloss:0.30029\n",
      "[382]\tvalidation_0-mlogloss:0.30010\n",
      "[383]\tvalidation_0-mlogloss:0.30024\n",
      "[384]\tvalidation_0-mlogloss:0.29999\n",
      "[385]\tvalidation_0-mlogloss:0.29992\n",
      "[386]\tvalidation_0-mlogloss:0.29976\n",
      "[387]\tvalidation_0-mlogloss:0.29969\n",
      "[388]\tvalidation_0-mlogloss:0.29942\n",
      "[389]\tvalidation_0-mlogloss:0.29942\n",
      "[390]\tvalidation_0-mlogloss:0.29936\n",
      "[391]\tvalidation_0-mlogloss:0.29907\n",
      "[392]\tvalidation_0-mlogloss:0.29908\n",
      "[393]\tvalidation_0-mlogloss:0.29883\n",
      "[394]\tvalidation_0-mlogloss:0.29882\n",
      "[395]\tvalidation_0-mlogloss:0.29830\n",
      "[396]\tvalidation_0-mlogloss:0.29806\n",
      "[397]\tvalidation_0-mlogloss:0.29796\n",
      "[398]\tvalidation_0-mlogloss:0.29822\n",
      "[399]\tvalidation_0-mlogloss:0.29802\n",
      "[400]\tvalidation_0-mlogloss:0.29801\n",
      "[401]\tvalidation_0-mlogloss:0.29795\n",
      "[402]\tvalidation_0-mlogloss:0.29782\n",
      "[403]\tvalidation_0-mlogloss:0.29773\n",
      "[404]\tvalidation_0-mlogloss:0.29766\n",
      "[405]\tvalidation_0-mlogloss:0.29756\n",
      "[406]\tvalidation_0-mlogloss:0.29749\n",
      "[407]\tvalidation_0-mlogloss:0.29772\n",
      "[408]\tvalidation_0-mlogloss:0.29747\n",
      "[409]\tvalidation_0-mlogloss:0.29732\n",
      "[410]\tvalidation_0-mlogloss:0.29732\n",
      "[411]\tvalidation_0-mlogloss:0.29730\n",
      "[412]\tvalidation_0-mlogloss:0.29715\n",
      "[413]\tvalidation_0-mlogloss:0.29703\n",
      "[414]\tvalidation_0-mlogloss:0.29719\n",
      "[415]\tvalidation_0-mlogloss:0.29711\n",
      "[416]\tvalidation_0-mlogloss:0.29712\n",
      "[417]\tvalidation_0-mlogloss:0.29696\n",
      "[418]\tvalidation_0-mlogloss:0.29689\n",
      "[419]\tvalidation_0-mlogloss:0.29695\n",
      "[420]\tvalidation_0-mlogloss:0.29693\n",
      "[421]\tvalidation_0-mlogloss:0.29707\n",
      "[422]\tvalidation_0-mlogloss:0.29706\n",
      "[423]\tvalidation_0-mlogloss:0.29696\n",
      "[424]\tvalidation_0-mlogloss:0.29710\n",
      "[425]\tvalidation_0-mlogloss:0.29682\n",
      "[426]\tvalidation_0-mlogloss:0.29674\n",
      "[427]\tvalidation_0-mlogloss:0.29676\n",
      "[428]\tvalidation_0-mlogloss:0.29666\n",
      "[429]\tvalidation_0-mlogloss:0.29642\n",
      "[430]\tvalidation_0-mlogloss:0.29659\n",
      "[431]\tvalidation_0-mlogloss:0.29634\n",
      "[432]\tvalidation_0-mlogloss:0.29621\n",
      "[433]\tvalidation_0-mlogloss:0.29623\n",
      "[434]\tvalidation_0-mlogloss:0.29647\n",
      "[435]\tvalidation_0-mlogloss:0.29629\n",
      "[436]\tvalidation_0-mlogloss:0.29638\n",
      "[437]\tvalidation_0-mlogloss:0.29623\n",
      "[438]\tvalidation_0-mlogloss:0.29631\n",
      "[439]\tvalidation_0-mlogloss:0.29612\n",
      "[440]\tvalidation_0-mlogloss:0.29606\n",
      "[441]\tvalidation_0-mlogloss:0.29608\n",
      "[442]\tvalidation_0-mlogloss:0.29599\n",
      "[443]\tvalidation_0-mlogloss:0.29582\n",
      "[444]\tvalidation_0-mlogloss:0.29587\n",
      "[445]\tvalidation_0-mlogloss:0.29588\n",
      "[446]\tvalidation_0-mlogloss:0.29585\n",
      "[447]\tvalidation_0-mlogloss:0.29584\n",
      "[448]\tvalidation_0-mlogloss:0.29587\n",
      "[449]\tvalidation_0-mlogloss:0.29596\n",
      "[450]\tvalidation_0-mlogloss:0.29602\n",
      "[451]\tvalidation_0-mlogloss:0.29610\n",
      "[452]\tvalidation_0-mlogloss:0.29611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88      1499\n",
      "           1       0.95      0.94      0.94      1499\n",
      "           2       0.96      0.95      0.95      1499\n",
      "           3       0.98      0.94      0.96      1499\n",
      "           4       1.00      1.00      1.00      1499\n",
      "           5       0.98      0.98      0.98      1499\n",
      "           6       0.97      0.99      0.98      1499\n",
      "           7       0.97      0.94      0.95      1499\n",
      "           8       0.92      0.93      0.93      1499\n",
      "           9       0.96      0.82      0.88      1499\n",
      "          10       0.98      0.97      0.97      1499\n",
      "          11       0.99      1.00      0.99      1499\n",
      "          12       0.95      0.97      0.96      1499\n",
      "          13       0.91      0.97      0.94      1499\n",
      "          14       0.92      0.93      0.92      1499\n",
      "          15       0.97      0.98      0.98      1499\n",
      "          16       0.96      0.90      0.93      1499\n",
      "          17       0.98      0.96      0.97      1499\n",
      "          18       0.89      0.95      0.92      1499\n",
      "          19       0.99      0.85      0.92      1499\n",
      "          20       0.99      0.99      0.99      1499\n",
      "          21       0.95      0.97      0.96      1499\n",
      "          22       0.85      0.88      0.86      1499\n",
      "          23       0.99      0.99      0.99      1499\n",
      "          24       0.99      0.99      0.99      1499\n",
      "\n",
      "    accuracy                           0.95     37475\n",
      "   macro avg       0.95      0.95      0.95     37475\n",
      "weighted avg       0.95      0.95      0.95     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4=XGBClassifier(n_estimators=500)\n",
    "model4.fit(x,y,early_stopping_rounds=10, eval_set=[(xv, yv)])\n",
    "y_pred=model4.predict(xt)\n",
    "print(classification_report(yt,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1dad4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(256, activation = 'relu', name = \"L2\"),\n",
    "        Dense(128, activation = 'relu', name = \"L3\"),\n",
    "        Dense(64, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(32, activation = 'relu', name = \"L5\"),\n",
    "        Dense(25, activation = 'linear', name = \"L6\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3361defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9375/9375 [==============================] - 7s 749us/step - loss: 0.4753 - val_loss: 0.7057\n",
      "Epoch 2/10\n",
      "9375/9375 [==============================] - 7s 745us/step - loss: 0.1528 - val_loss: 0.6195\n",
      "Epoch 3/10\n",
      "9375/9375 [==============================] - 7s 736us/step - loss: 0.1112 - val_loss: 0.6615\n",
      "Epoch 4/10\n",
      "9375/9375 [==============================] - 7s 740us/step - loss: 0.0883 - val_loss: 0.7001\n",
      "Epoch 5/10\n",
      "9375/9375 [==============================] - 7s 743us/step - loss: 0.0773 - val_loss: 0.7987\n",
      "Epoch 6/10\n",
      "9375/9375 [==============================] - 7s 736us/step - loss: 0.0694 - val_loss: 0.6665\n",
      "Epoch 7/10\n",
      "9375/9375 [==============================] - 7s 743us/step - loss: 0.0628 - val_loss: 0.6586\n",
      "Epoch 8/10\n",
      "9375/9375 [==============================] - 7s 743us/step - loss: 0.0565 - val_loss: 0.6919\n",
      "Epoch 9/10\n",
      "9375/9375 [==============================] - 7s 726us/step - loss: 0.0528 - val_loss: 0.6235\n",
      "Epoch 10/10\n",
      "9375/9375 [==============================] - 7s 744us/step - loss: 0.0498 - val_loss: 0.6417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ec542920>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model3.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43017064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172/1172 [==============================] - 0s 307us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.77      1499\n",
      "           1       0.83      0.94      0.88      1499\n",
      "           2       0.93      0.75      0.83      1499\n",
      "           3       0.93      0.87      0.90      1499\n",
      "           4       0.99      1.00      0.99      1499\n",
      "           5       0.95      0.87      0.91      1499\n",
      "           6       0.91      0.98      0.94      1499\n",
      "           7       0.92      0.92      0.92      1499\n",
      "           8       0.83      0.93      0.88      1499\n",
      "           9       0.88      0.78      0.83      1499\n",
      "          10       0.89      0.76      0.82      1499\n",
      "          11       0.96      0.94      0.95      1499\n",
      "          12       0.84      0.97      0.90      1499\n",
      "          13       0.78      0.89      0.83      1499\n",
      "          14       0.62      0.87      0.72      1499\n",
      "          15       0.99      0.99      0.99      1499\n",
      "          16       0.98      0.78      0.86      1499\n",
      "          17       0.99      0.89      0.94      1499\n",
      "          18       0.81      0.77      0.79      1499\n",
      "          19       0.80      0.55      0.65      1499\n",
      "          20       0.99      0.98      0.98      1499\n",
      "          21       0.92      0.96      0.94      1499\n",
      "          22       0.71      0.73      0.72      1499\n",
      "          23       0.99      0.99      0.99      1499\n",
      "          24       0.93      1.00      0.96      1499\n",
      "\n",
      "    accuracy                           0.88     37475\n",
      "   macro avg       0.88      0.88      0.88     37475\n",
      "weighted avg       0.88      0.88      0.88     37475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = np.argmax(tf.nn.softmax(model3.predict(xt)).numpy(),axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18afc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd77dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
